{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 4\n",
    "\n",
    "You'll now make the switch to hierarchical clustering. I wanted to let you implement it yourself, but that would probably take somewhat too long for a short practical. Still, average linkage clustering is implemented for you, but you' ll have to edit it to also allow complete and single linkage clustering. Finally, you make a small comparison between K-means clustering and hierarchical clustering. To get started, run the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dafdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEucliDist(vectorOne, vectorTwo):\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, axis = 1)\n",
    "\n",
    "def calcAbsDist(vectorOne, vectorTwo):\n",
    "    #using linalg.norm:\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, ord = 1, axis = 1)\n",
    "\n",
    "def makeKMeanClusters(X, k, funName = \"calcEucliDist\", maxIter = 50, nClusteringsToPerform = 20):\n",
    "    if k <= 0:\n",
    "        print(\"K must be greater than 0!\")\n",
    "        return None\n",
    "    if k > len(X):\n",
    "        print(\"K cannot be larger than the # of samples in your data!\")\n",
    "        return None\n",
    "    if maxIter <= 0:\n",
    "        print(\"Cannot have negative or 0 iterations!\")\n",
    "        return None\n",
    "    \n",
    "    resultToReturn = [None, None, None, None]\n",
    "    bestDistortion = np.Inf\n",
    "    \n",
    "    for clusteringIndex in range(0, nClusteringsToPerform):\n",
    "        initialCentroids   = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "        if len(initialCentroids) != k:\n",
    "            print(\"Centroids lost!\")\n",
    "        centroids          = initialCentroids\n",
    "        threeLastCentroids = []\n",
    "        #print(centroids)\n",
    "        for i in range(0, maxIter):\n",
    "\n",
    "            threeLastCentroids.append(np.round(centroids, 4))\n",
    "            distancesToCentroids = np.vstack([globals()[funName](centroids, datapoint) for datapoint in X])\n",
    "            closestCentroid      = np.where(distancesToCentroids == np.amin(distancesToCentroids,\n",
    "                                                                            axis = 1)[:, np.newaxis])[1]\n",
    "            centroids            = np.vstack([np.mean(X[np.where(closestCentroid == clusterNum)],\n",
    "                                                      axis = 0) for clusterNum in np.unique(closestCentroid)])\n",
    "\n",
    "            if i >2:\n",
    "                threeLastCentroids.pop(0)\n",
    "                if np.array_equal(threeLastCentroids[-1],threeLastCentroids[-2]) and np.array_equal(threeLastCentroids[-2], threeLastCentroids[-3]):\n",
    "                    print(\"No changes in cluster centroids detected in last 3 iterations. Finished at iteration \" + str(i+1) + \".\")\n",
    "                    break\n",
    "        \n",
    "        # new code\n",
    "        squareDistancesPerPoint = []\n",
    "        for index, centroid in enumerate(closestCentroid):\n",
    "            squareDistancesPerPoint.append(np.square(centroids[centroid, :] - X[index, :]))\n",
    "        distortion = 1/len(X) * np.sum(np.array(squareDistancesPerPoint))\n",
    "        \n",
    "        if distortion < bestDistortion:\n",
    "            bestDistortion = distortion\n",
    "            resultToReturn = [centroids, closestCentroid, initialCentroids, bestDistortion]\n",
    "                \n",
    "    return resultToReturn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7131a7e",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "\n",
    "The below again defines some clusters for you to work with. Run it, then  move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5897cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, n_features=2, centers=5, cluster_std=1.0,\n",
    "                  center_box=(- 10.0, 10.0), shuffle=True, random_state=42, return_centers=False)\n",
    "\n",
    "dataFrameForPlot = pd.DataFrame(np.c_[X, y])\n",
    "dataFrameForPlot.columns = [\"Feature 1\", \"Feature 2\", \"Class\"]\n",
    "sns.scatterplot(x = \"Feature 1\", y = \"Feature 2\", hue = \"Class\", palette = \"tab10\", data = dataFrameForPlot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069aae2",
   "metadata": {},
   "source": [
    "## Hierarchical clustering with average linking\n",
    "\n",
    "Let's start simple by doing hierarchical clustering using average linking. To do this, you need the following:\n",
    "1. Distances from all points to all other points\n",
    "2. A way to link the two closest clusters and save that as a new cluster.\n",
    "\n",
    "Below, I give you a basic function body that calculates the distance matrix for every point to every other point. Since this is a symmetric matrix (distance from point 1 to point 2 = distance from point 2 to point 1) and the distance from each point to itself is 0, I make sure not to calculate superfluous values. Then, it implements average linkage clustering. Read through the function body and see if you understand what it is doing. After that's done, go to the cell below it to test it out on our sample data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e725dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarCluster(X, distanceFunc = \"calcEucliDist\", linkageMethod = \"average\", displayDistMatrix = False):\n",
    "    \n",
    "    if linkageMethod not in [\"average\", \"complete\", \"single\"]:\n",
    "        print(\"Error, please input a valid linkage method!\")\n",
    "        return None\n",
    "    if distanceFunc  not in globals().keys():\n",
    "        print(\"Error, please input a valid distance function name!\")\n",
    "    \n",
    "    # make an empty distance matrix\n",
    "    distanceMatrix = np.zeros(shape = (len(X), len(X)))\n",
    "    distanceMatrix.fill(np.nan)\n",
    "    # make a list with the indices of every data point. This is the list of clusters, where you start\n",
    "    # with every point in a cluster and then start merging them.\n",
    "    initialList = [[index] for index, _ in enumerate(X)]\n",
    "    clusterList = initialList.copy()\n",
    "    clusteringOverIterations = []\n",
    "    clusteringOverIterations.append(initialList)\n",
    "    # also make an empty list that saves which cluster indices were merged for every iteration\n",
    "    clusterIndicesMergedList = []\n",
    "    for rowIndex, row in enumerate(distanceMatrix):\n",
    "        for colIndex, cellValue in enumerate(row):\n",
    "            # distance from yourself to yourself is 0, don't calculate!\n",
    "            if colIndex == rowIndex:\n",
    "                continue\n",
    "            # in the first loop, you calculate distance from 1 to 2.\n",
    "            # in the second loop, you don't want to calculate distance from 2 to 1 again. This safeguards against that.\n",
    "            if colIndex < rowIndex:\n",
    "                continue\n",
    "\n",
    "            distanceMatrix[rowIndex, colIndex] = globals()[distanceFunc](X[rowIndex,:][np.newaxis,  :],\n",
    "                                                                             X[colIndex, :][np.newaxis, :])\n",
    "    if displayDistMatrix:\n",
    "        display(pd.DataFrame(distanceMatrix))\n",
    "\n",
    "    # We continue clustering until everything is in one giant cluster. Thats len(X)-1 clustering steps.\n",
    "    for i in range(0, len(X)-1):\n",
    "        # we start with no idea of which two clusters we need to cluster\n",
    "        lowestDistDatapoints = None\n",
    "        # since we haven't calculated any distance, our current distance is infinite\n",
    "        distToCluster = np.Inf\n",
    "        # clusterList initially looks like [[0], [1], ... [99]].\n",
    "        # itertools.combinations makes that into [([0], [1]), ([0], [2]), ([0], [3]) ... ([1], [2]), ([1], [3])... (98, 99)]\n",
    "        # so you get all possible combinations of clusters that you could cluster together\n",
    "        for combo in itertools.combinations(clusterList, 2):\n",
    "\n",
    "            totalDistance = 0\n",
    "            # make all combinations of data points in the first cluster and data points in the second cluster\n",
    "            # so if the current combo = ([0, 12, 15], [3, 2]), this results in:\n",
    "            # [[0, 3], [0, 2], [12, 3], [12, 2], [15, 3], [15,2]]: these are all the points that we need to get\n",
    "            # the distances for (and average for average linkage)\n",
    "            toIterate = [j for i in [list(zip([elem] * len(combo[1]), combo[1] )) for elem in combo[0]] for j in i]\n",
    "            for indicesTwoDatapoints in toIterate:\n",
    "                #sort the indices. Our matrix has only the distance between 1 and 2, not between 2 and 1.\n",
    "                #this turns [12, 2] from above into [2, 12], etc.\n",
    "                indicesTwoDatapoints = sorted(indicesTwoDatapoints)\n",
    "\n",
    "                # keep a running total of all distances between the points in the two clusters\n",
    "                if linkageMethod == \"average\":\n",
    "                    totalDistance += distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "                # you'll fill these in later to make complete and single linkage work!\n",
    "                if linkageMethod == \"complete\":\n",
    "                    furthestDistance = None\n",
    "                if linkageMethod == \"single\":\n",
    "                    closestDistance  = None\n",
    "\n",
    "            if linkageMethod == \"average\":\n",
    "                totalAvgDistance = totalDistance/(len(combo[0]) * len(combo[1]))\n",
    "\n",
    "            # if distance between these clusters is less than the lowest distance we have seen so far,\n",
    "            #set these clusters as the ones to cluster. \n",
    "                if totalAvgDistance < distToCluster:\n",
    "                    distToCluster       = totalAvgDistance\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "            # again, you'll fill these in later.\n",
    "            if linkageMethod == \"complete\":\n",
    "                distToCluster = None\n",
    "                dataPointsToCluster = None\n",
    "\n",
    "            if linkageMethod == \"single\":\n",
    "                distToCluster = None\n",
    "                dataPointsToCluster = None\n",
    "\n",
    "        clusterIndicesMergedList.append(dataPointsToCluster)\n",
    "        #make a new list of clusters\n",
    "        clusterList = clusterList.copy()\n",
    "        for index, elem in enumerate(clusterList):\n",
    "            # merge the second cluster into the first cluster\n",
    "            if elem == dataPointsToCluster[0]:\n",
    "                clusterList[index] = clusterList[index] + dataPointsToCluster[1]\n",
    "                #clusterList2[index] = sorted(clusterList[index])\n",
    "            # remove the separate second cluster (it's now been merged to the first one)    \n",
    "            if elem == dataPointsToCluster[1]:\n",
    "                clusterList.pop(index)\n",
    "        # Finally, save all clusters, from the very beginning (all separate clusters) until the very end (all in one cluster)         \n",
    "        clusteringOverIterations.append(clusterList)\n",
    "    \n",
    "    return [clusteringOverIterations, pd.DataFrame(distanceMatrix), clusterIndicesMergedList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c8355",
   "metadata": {},
   "source": [
    "## Testing the hierarchical clustering function. \n",
    "\n",
    "As you've heard, hierarchical clustering doesn't result in a certain set of clusters like K-means does. Instead, it gives all intermediate amounts of clusters, starting with everything in a separate cluster and agglomerating until everything is in one cluster. The below sets up a variable with a list of all these clusterings. We know there are 5 clusters in our data, so it is interesting to see what things look like if we request 10 clusters. Do those resemble the 5 clusters in the data, with 5 outlier points not yet connected up? It is up to you to:\n",
    "\n",
    "* Select the element of that list that has 10 clusters in it\n",
    "* Make a list where you give each data point in X a number from 0 to 9, indicating to which of the 10 clusters it belongs\n",
    "* Plot the data with seaborn, using `hue = yourClusterMembershipVariableFrom0To9`.\n",
    "* Plot the plot with the original cluster colouring below it for comparison. You should see that only the one orange point that is right in the middle of the purple cluster in the original data is 'misclassified' (that is, there's of course ten clusters here, but it's obvious that they'll be merged into the 5 'correct' clusters, save for this one outlier datapoint that is orange officially but lies smack in the middle of the purple cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8150bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusteringOutcomeX = hierarCluster(X)\n",
    "clusterListX       = clusteringOutcomeX[0]\n",
    "\n",
    "#print the clusters where everything is separate and everything is combined\n",
    "print(clusterListX[0])\n",
    "print(len(clusterListX[0]))\n",
    "print(clusterListX[-1])\n",
    "print(len(clusterListX[1]))\n",
    "\n",
    "#your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d567a",
   "metadata": {},
   "source": [
    "## Plotting the clustering process\n",
    "\n",
    "Let's plot the process from start to finish, in steps of five clusters being added together. To avoid cluttering the legend with up to 100 different clusters, assign everything that has not been clustered with any other point the label 'not clustered'. Up to you to:\n",
    "\n",
    "*  Do what you did above, but do it for every entry in `toPlot`.\n",
    "\n",
    "Hints:\n",
    "* You can use `myPlot = sns.scatterplot()` to make it callable. Then, using `plt.legend(loc = 2, bbox_to_anchor = (1,1))` followed finally by `plt.show(myPlot)` you can put the legend outside of the plot and show it. Handy when you have loads of clusters.\n",
    "* To do this you just need another loop around your code above that goes over the indices you need to get out of the list of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bd3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "toPlot = list(range(0, len(clusterListX), 5))\n",
    "\n",
    "# answer\n",
    "\n",
    "for indexToPlot in toPlot:\n",
    "    \n",
    "    clusterMembership = [0] * len(X)\n",
    "    for clusterNr, cluster in enumerate(clusterListX[indexToPlot]):\n",
    "        if len(cluster) == 1:\n",
    "            clusterMembership[cluster[0]] = 'not clustered'\n",
    "        else:            \n",
    "            for dataIndex in cluster:\n",
    "                #print(dataIndex)\n",
    "                #print(X[dataIndex,:])\n",
    "                clusterMembership[dataIndex] = str(clusterNr)\n",
    "    entries = list(np.unique(clusterMembership))\n",
    "    entries.remove('not clustered')\n",
    "    hue_order = [\"not clustered\"] + entries\n",
    "    b = sns.scatterplot(x = \"Feature 1\", y = \"Feature 2\", hue = clusterMembership,\n",
    "                    palette = \"tab10\", data = dataFrameForPlot, hue_order = hue_order).set(title = \"Hierarchical clustering with average linkage. Showing \" + str(len(clusterListX[indexToPlot]))+ \" clusters\")\n",
    "    plt.legend(loc = 2, bbox_to_anchor = (1,1))\n",
    "    plt.show(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c837d",
   "metadata": {},
   "source": [
    "## Making complete and single linkage clusters\n",
    "\n",
    "Before we get to drawing a tree-like diagram of the clusters, let's make the function completely functional. It's up to you to make sure that when the argument is `\"single\"`, the shortest distance between clusters is chosen as their distance to see whether these need to be clustered together in the next step (rather than the average distance). Then, do the same for `\"complete\"` linkage. I've copied the function below, up to you to change it!\n",
    "\n",
    "Hints:\n",
    "* If you get stuck, remember that you can just take everything out of the function body while building the function, and only as a last stap do the `def hierarCluster` etc. In that way, you can investigate the intermediate steps easily.\n",
    "* For single linkage, you might not want the initial distance to start at 0. After all, to get the closest two points in a cluster, you'll continuously be asking whether the distance between to points is less than the current lowest distance you have for that cluster.\n",
    "* For both single linkage and complete linkage, you eventually cluster the clusters that have the lowest distance between them. It only differs how you define the lowest distance between clusters (the smallest distance between all points in them, or the largest distance between all points in them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarCluster(X, distanceFunc = \"calcEucliDist\", linkageMethod = \"average\", displayDistMatrix = False):\n",
    "    \n",
    "    if linkageMethod not in [\"average\", \"complete\", \"single\"]:\n",
    "        print(\"Error, please input a valid linkage method!\")\n",
    "        return None\n",
    "    if distanceFunc  not in globals().keys():\n",
    "        print(\"Error, please input a valid distance function name!\")\n",
    "    \n",
    "    # make an empty distance matrix\n",
    "    distanceMatrix = np.zeros(shape = (len(X), len(X)))\n",
    "    distanceMatrix.fill(np.nan)\n",
    "    # make a list with the indices of every data point. This is the list of clusters, where you start\n",
    "    # with every point in a cluster and then start merging them.\n",
    "    initialList = [[index] for index, _ in enumerate(X)]\n",
    "    clusterList = initialList.copy()\n",
    "    clusteringOverIterations = []\n",
    "    clusteringOverIterations.append(initialList)\n",
    "    # also make an empty list that saves which cluster indices were merged for every iteration\n",
    "    clusterIndicesMergedList = []\n",
    "    for rowIndex, row in enumerate(distanceMatrix):\n",
    "        for colIndex, cellValue in enumerate(row):\n",
    "            # distance from yourself to yourself is 0, don't calculate!\n",
    "            if colIndex == rowIndex:\n",
    "                continue\n",
    "            # in the first loop, you calculate distance from 1 to 2.\n",
    "            # in the second loop, you don't want to calculate distance from 2 to 1 again. This safeguards against that.\n",
    "            if colIndex < rowIndex:\n",
    "                continue\n",
    "\n",
    "            distanceMatrix[rowIndex, colIndex] = globals()[distanceFunc](X[rowIndex,:][np.newaxis,  :],\n",
    "                                                                             X[colIndex, :][np.newaxis, :])\n",
    "    if displayDistMatrix:\n",
    "        display(pd.DataFrame(distanceMatrix))\n",
    "\n",
    "    # We continue clustering until everything is in one giant cluster. Thats len(X)-1 clustering steps.\n",
    "    for i in range(0, len(X)-1):\n",
    "        # we start with no idea of which two clusters we need to cluster\n",
    "        lowestDistDatapoints = None\n",
    "        # since we haven't calculated any distance, our current distance is infinite\n",
    "        distToCluster = np.Inf\n",
    "        # clusterList initially looks like [[0], [1], ... [99]].\n",
    "        # itertools.combinations makes that into [([0], [1]), ([0], [2]), ([0], [3]) ... ([1], [2]), ([1], [3])... (98, 99)]\n",
    "        # so you get all possible combinations of clusters that you could cluster together\n",
    "        for combo in itertools.combinations(clusterList, 2):\n",
    "\n",
    "            totalDistance = 0\n",
    "            # make all combinations of data points in the first cluster and data points in the second cluster\n",
    "            # so if the current combo = ([0, 12, 15], [3, 2]), this results in:\n",
    "            # [[0, 3], [0, 2], [12, 3], [12, 2], [15, 3], [15,2]]: these are all the points that we need to get\n",
    "            # the distances for (and average for average linkage)\n",
    "            toIterate = [j for i in [list(zip([elem] * len(combo[1]), combo[1] )) for elem in combo[0]] for j in i]\n",
    "            for indicesTwoDatapoints in toIterate:\n",
    "                #sort the indices. Our matrix has only the distance between 1 and 2, not between 2 and 1.\n",
    "                #this turns [12, 2] from above into [2, 12], etc.\n",
    "                indicesTwoDatapoints = sorted(indicesTwoDatapoints)\n",
    "\n",
    "                # keep a running total of all distances between the points in the two clusters\n",
    "                if linkageMethod == \"average\":\n",
    "                    totalDistance += distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "                # you'll fill these in later to make complete and single linkage work!\n",
    "                if linkageMethod == \"complete\":\n",
    "                    furthestDistance = None\n",
    "                if linkageMethod == \"single\":\n",
    "                    closestDistance  = None\n",
    "\n",
    "            if linkageMethod == \"average\":\n",
    "                totalAvgDistance = totalDistance/(len(combo[0]) * len(combo[1]))\n",
    "\n",
    "            # if distance between these clusters is less than the lowest distance we have seen so far,\n",
    "            #set these clusters as the ones to cluster. \n",
    "                if totalAvgDistance < distToCluster:\n",
    "                    distToCluster       = totalAvgDistance\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "            # again, you'll fill these in later.\n",
    "            if linkageMethod == \"complete\":\n",
    "                distToCluster = None\n",
    "                dataPointsToCluster = None\n",
    "\n",
    "            if linkageMethod == \"single\":\n",
    "                distToCluster = None\n",
    "                dataPointsToCluster = None\n",
    "\n",
    "        clusterIndicesMergedList.append(dataPointsToCluster)\n",
    "        #make a new list of clusters\n",
    "        clusterList = clusterList.copy()\n",
    "        for index, elem in enumerate(clusterList):\n",
    "            # merge the second cluster into the first cluster\n",
    "            if elem == dataPointsToCluster[0]:\n",
    "                clusterList[index] = clusterList[index] + dataPointsToCluster[1]\n",
    "                #clusterList2[index] = sorted(clusterList[index])\n",
    "            # remove the separate second cluster (it's now been merged to the first one)    \n",
    "            if elem == dataPointsToCluster[1]:\n",
    "                clusterList.pop(index)\n",
    "        # Finally, save all clusters, from the very beginning (all separate clusters) until the very end (all in one cluster)         \n",
    "        clusteringOverIterations.append(clusterList)\n",
    "    \n",
    "    return [clusteringOverIterations, pd.DataFrame(distanceMatrix), clusterIndicesMergedList]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac2731",
   "metadata": {},
   "source": [
    "## Testing your new function\n",
    "\n",
    "It'll be interesting to see what single linkage and complete linkage result in, relative to average linkage. In the code cell below, I run hierarchical clustering on the sample data (X), and select the list entry that has 5 clusters. Up to you to plot the clusters. See whether they have the same result or are dissimilar at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646fa4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleLinkCluster   = hierarCluster(X, linkageMethod = \"single\")[0]\n",
    "averageLinkCluster  = hierarCluster(X, linkageMethod = \"average\")[0]\n",
    "completeLinkCluster = hierarCluster(X, linkageMethod = \"complete\")[0]\n",
    "\n",
    "fiveClusterData = [elem for elem in singleLinkCluster if len(elem) == 5 ] + [elem for elem in averageLinkCluster if len(elem) == 5 ] + [elem for elem in completeLinkCluster if len(elem) == 5]\n",
    "\n",
    "#answer\n",
    "clusterNameList = [\"single linkage\", \"average linkage\", \"complete linkage\"]\n",
    "for index, clustering in enumerate([\"single linkage\", \"average linkage\", \"complete linkage\"]):\n",
    "    clusterMembership = [0] * len(X)\n",
    "    for clusterIndex, cluster in enumerate(fiveClusterData[index]):\n",
    "        for dataIndex in cluster:\n",
    "            clusterMembership[dataIndex] = str(clusterIndex)\n",
    "    b = sns.scatterplot(x = \"Feature 1\", y = \"Feature 2\", hue = clusterMembership,\n",
    "                    palette = \"tab10\", data = dataFrameForPlot).set(title = \"Hierarchical clustering with \" + clustering)\n",
    "    plt.legend(loc = 2, bbox_to_anchor = (1,1))\n",
    "    plt.show(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133599d3",
   "metadata": {},
   "source": [
    "## Intermediate steps\n",
    "\n",
    "You probably found out that for these well-defined clusters nothing much is different for the final clustering into five clusters. Let's look at the case for when there's still 15 clusters. That will probably look drastically different. No use to let you program that, so just run the code cell below. Also it's again not very DRY (Don't Repeat Yourself) what I am doing here, but it's for demonstration purposes. Normally you would functionalise your plotting calls!\n",
    "\n",
    "We've so far been using Euclidean distance. It makes most sense intuitively here. What if we would use the Manhattan distance instead? You can see the results for that below as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleLinkManhattanCluster   = hierarCluster(X, distanceFunc = \"calcAbsDist\", linkageMethod = \"single\")[0]\n",
    "averageLinkManhattanCluster  = hierarCluster(X, distanceFunc = \"calcAbsDist\", linkageMethod = \"average\")[0]\n",
    "completeLinkManhattanCluster = hierarCluster(X, distanceFunc = \"calcAbsDist\", linkageMethod = \"complete\")[0]\n",
    "\n",
    "clusterLen = 15\n",
    "fifteenClusterData = [elem for elem in singleLinkCluster if len(elem) == clusterLen ] + [elem for elem in averageLinkCluster if len(elem) == clusterLen ] + [elem for elem in completeLinkCluster if len(elem) == clusterLen]\n",
    "fifteenClusterDataManhattan = [elem for elem in singleLinkManhattanCluster if len(elem) == clusterLen ] + [elem for elem in averageLinkManhattanCluster if len(elem) == clusterLen ] + [elem for elem in completeLinkManhattanCluster if len(elem) == clusterLen]\n",
    "\n",
    "clusterNameList = [\"single linkage and Euclidean distance\", \"average linkage and Euclidean distance\", \"complete linkage and Euclidean distance\"]\n",
    "for index, clustering in enumerate(clusterNameList):\n",
    "    clusterMembership = [0] * len(X)\n",
    "    for clusterIndex, cluster in enumerate(fifteenClusterData[index]):\n",
    "        for dataIndex in cluster:\n",
    "            clusterMembership[dataIndex] = str(clusterIndex)\n",
    "    b = sns.scatterplot(x = \"Feature 1\", y = \"Feature 2\", hue = clusterMembership,\n",
    "                    palette = \"tab10\", data = dataFrameForPlot).set(title = \"Hierarchical clustering with \" + clustering + \" for \" + str(clusterIndex + 1) + \" clusters\")\n",
    "    plt.legend(loc = 2, bbox_to_anchor = (1,1))\n",
    "    plt.show(b)\n",
    "    \n",
    "    \n",
    "#answer\n",
    "clusterNameList = [\"single linkage and Manhattan distance\", \"average linkage and Manhattan distance\", \"complete linkage and Manhattan distance\"]\n",
    "for index, clustering in enumerate(clusterNameList):\n",
    "    clusterMembership = [0] * len(X)\n",
    "    for clusterIndex, cluster in enumerate(fifteenClusterDataManhattan[index]):\n",
    "        for dataIndex in cluster:\n",
    "            clusterMembership[dataIndex] = str(clusterIndex)\n",
    "    b = sns.scatterplot(x = \"Feature 1\", y = \"Feature 2\", hue = clusterMembership,\n",
    "                    palette = \"tab10\", data = dataFrameForPlot).set(title = \"Hierarchical clustering with \" + clustering + \" for \" + str(clusterIndex + 1) + \" clusters\")\n",
    "    plt.legend(loc = 2, bbox_to_anchor = (1,1))\n",
    "    plt.show(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b51e6",
   "metadata": {},
   "source": [
    "## What you see\n",
    "\n",
    "What you should see is that points that are close to others 'diagonally' are indeed close in the Euclidean distance case, but not in the Manhattan distance case. \n",
    "\n",
    "## Drawing a dendogram\n",
    "\n",
    "You might want to plot a nice tree diagram of your clusters. How to do that? Well, you need to draw lines connecting all data points together. Let's try it. Run the code below to generate such a plot. Note that it is not _completely_ the same as a normal dendogram, because I didn't draw the connecting line in the middle of each horizontal line, but at the average position of all indices in a cluster. Still, close enough for homemade!\n",
    "\n",
    "My question to you: in the final clustering steps of this clustering procedure you see that there's two clusters of two subclusters each that get clustered together, which are then finally clustered together with the cluster on the left. Which of the 5 cluster signals we put into our data is each of these likely to be? What is the outlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusteringOutcomeX = hierarCluster(X)\n",
    "clusterListX       = clusteringOutcomeX[0]\n",
    "clusteredPerStepX  = clusteringOutcomeX[2]\n",
    "xLabels            = np.array(list(itertools.chain(*clusterListX[-1])))\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (25, 8))\n",
    "ax.set_xticks(range(0, len(X)))\n",
    "ax.set_xticklabels(xLabels)\n",
    "ax.margins(y=0)\n",
    "\n",
    "heightPerDataPointPreviousStep = np.array([0] * len(X))\n",
    "for i, clusterStep in enumerate(clusteredPerStepX):\n",
    "    pos1Positions = np.array([np.where(xLabels == elem)[0] for elem in clusterStep[0]])\n",
    "    pos1Avg       = np.mean(pos1Positions)\n",
    "    #pos1Start     = np.min(pos1Positions)\n",
    "    #pos1End       = np.max(pos1Positions)\n",
    "    pos1ClustSize = len(pos1Positions)\n",
    "    pos2Positions = np.array([np.where(xLabels == elem)[0] for elem in clusterStep[1]])\n",
    "    pos2Avg       = np.mean(pos2Positions)\n",
    "    #pos2Start     = np.min(pos2Positions)\n",
    "    #pos2End       = np.max(pos2Positions)\n",
    "    pos2ClustSize = len(pos2Positions)\n",
    "    \n",
    "\n",
    "    \n",
    "    heightEnd   = max(pos1ClustSize, pos2ClustSize)\n",
    "    ax.plot([pos1Avg, pos1Avg], [heightPerDataPointPreviousStep[pos1Positions[0][0]],heightEnd], color = \"black\")\n",
    "    ax.plot([pos2Avg, pos2Avg], [heightPerDataPointPreviousStep[pos2Positions[0][0]],heightEnd], color = \"black\")\n",
    "    ax.plot([pos1Avg, pos2Avg], [heightEnd,heightEnd], color = \"black\")\n",
    "    \n",
    "    heightPerDataPointPreviousStep[np.ravel(pos1Positions)] += heightEnd - heightPerDataPointPreviousStep[pos1Positions[0][0]]\n",
    "    heightPerDataPointPreviousStep[np.ravel(pos2Positions)] += heightEnd - heightPerDataPointPreviousStep[pos2Positions[0][0]]\n",
    "    \n",
    "ax.set_ylim(0, max(heightPerDataPointPreviousStep)+1)\n",
    " \n",
    "plt.show(fig)\n",
    "sns.scatterplot(x = \"Feature 1\", y = \"Feature 2\", hue = \"Class\", palette = \"tab10\", data = dataFrameForPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e7d949",
   "metadata": {},
   "source": [
    "## Cluster method comparison\n",
    "\n",
    "Finally, let's see how average linkage hierarchical clustering and K-means clustering compare. Run k-means clustering with 10 centroids on this data and visualise the results. Also run hierarchical clustering and select the clustering step where there's 10 clusters remaining (you did this as the first exercise, so you can just replot the plot from there). Is there much difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27176dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774e02c",
   "metadata": {},
   "source": [
    "## Differences\n",
    "\n",
    "If you look at the plots for a while, you see that there's some agreement, but they're clearly not the same. For this data, we _know_ that there's 5 underlying distributions of feature 1 and 2 that generated the data, i.e. the correct answer is that there are 5 clusters. In reality, you wouldn't know. So if we pretend there's 10, searching around as you would in a real situation, you see there's quite some differences in the clusters K-means would give you versus those made with hierarchical clustering.\n",
    "\n",
    "A clear example is the cluster in the top left: here, I would say hierarchical clustering has got a better handle on things in this case: K-means separates the clusters into top and bottom, which feels visually off. It's probably a result of its random initialisation never starting such that there's not one centroid more in the upper part of that cluster and another more to the bottom. Note also that K-means equally subdivides clusters that to us look like they should be whole, whereas hierarchical clustering has basically the 5 clusters we would expect, with 5 outlier points of those clusters not quite yet added to the main blob.\n",
    "\n",
    "In the end, there's no correct answer, it doesn't exist (especially in this case, given the signal we put in), but there's many different possibilities of clustering even these quite simple data clouds into 10 clusters. Remember that clustering is a dialogue with your data, guided by what you find important or useful, and/or what you need for downstream purposes, and can thus be an informed choice at most. There's no single objectively best clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bec745",
   "metadata": {},
   "source": [
    "## What I want you to remember here:\n",
    "* What hierarchical clustering is and how it works\n",
    "* How it differs from K-means clustering\n",
    "* How a dendrogram relates to a hierarchical clustering\n",
    "* The points below\n",
    "\n",
    "## The end\n",
    "\n",
    "That's it for this practical. Some takeaways:\n",
    "* Hierarchical clustering starts with every data point in its own cluster, and agglomerates, merging clusters 1 by 1, until everything is in one cluster.\n",
    "* This means it's very easy to get any number of clusters, you just request a different intermediate step that's done anyway. For K-means, you'd have to rerun for different K.\n",
    "* Note also that hierarchical clustering (at least with these linkage and distance methods) is fully deterministic: there's just one result for hierarchical clustering using average linkage on the sample data used here. For K-means, depending on how you initialise you get different results.\n",
    "* I hope that seeing these algorithms defined in code makes sure that you appreciate that you can understand exactly what they're doing and why, and could do it yourself if need be and you had the time. Try to always understand what clustering algorithms are doing exactly before using them in the wild, and more generally: accept that clustering is not some quick step but is an iterative process.\n",
    "\n",
    "## Survey\n",
    "[Hallelujah, praise the Forms!](https://docs.google.com/forms/d/e/1FAIpQLScx9hq233cl8wKw8ws6stN3dSzTKRtN8eP3gtHtuk8Ywg5fEQ/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
