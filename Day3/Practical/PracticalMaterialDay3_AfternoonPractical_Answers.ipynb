{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Afternoon practical day 3\n",
    "\n",
    "You've just learned about convolutional neural networks. There will be a sneak peek into them at the end. For now, however, we finally implement backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, Math\n",
    "from scipy.optimize import fmin_bfgs, fmin_cg, fmin\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important functions\n",
    "def mySigmoid(data):\n",
    "    data= np.array(data)\n",
    "    output = 1/(1+ np.exp(-data))\n",
    "    return output\n",
    "\n",
    "def mySigmoidGradient(x):\n",
    "    outcome = mySigmoid(x) * (1-mySigmoid(x))\n",
    "    return outcome\n",
    "\n",
    "def nnCostFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10):\n",
    "   \n",
    "    m = len(X)\n",
    "    \n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #calculating the forward pass\n",
    "    inputs        = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    weightedSumHL = inputs @ thetaOneMatrix.T\n",
    "    activationsHL  = mySigmoid(weightedSumHL)\n",
    "    \n",
    "    inputsOL      = np.c_[np.ones(shape = (len(activationsHL), 1)), activationsHL]\n",
    "    weightedSumOL = inputsOL @ thetaTwoMatrix.T\n",
    "    activationsOL = mySigmoid(weightedSumOL)\n",
    "    \n",
    "    #cost\n",
    "    J = 1/m * np.sum((- (y * np.log(activationsOL)) - ((1-y) * np.log(1-activationsOL))))\n",
    "    \n",
    "    #regularised cost\n",
    "    #remember: units in the rows, their parameters in the columns\n",
    "    #Hence, [:,1:] removes the columns with the bias term.\n",
    "    regThetaOne = np.sum(np.square(thetaOneMatrix[:,1:]))\n",
    "    regThetaTwo = np.sum(np.square(thetaTwoMatrix[:,1:]))\n",
    "    regCost     = J + (lambda_/(2*m)) * (regThetaOne + regThetaTwo)\n",
    "    \n",
    "    return regCost\n",
    "\n",
    "def numericalGradientApproximation(nnThetas, X, y, lambda_, e = 1e-4):\n",
    "    nnThetasMinusE = nnThetas-e\n",
    "    nnThetasPlusE  = nnThetas+e\n",
    "    listGradients = []\n",
    "    for index, value in enumerate(nnThetas):\n",
    "        if index % 500 == 0:\n",
    "            print(\"Parameter \" + str(index) + \"out of \" + str(len(nnThetas)))\n",
    "        minusEThetas = nnThetas; minusEThetas[index] = nnThetasMinusE[index]\n",
    "        minusECost   = nnCostFunction(minusEThetas, X, y, lambda_)\n",
    "        plusEThetas  = nnThetas; plusEThetas[index] = nnThetasPlusE[index]\n",
    "        plusECost    = nnCostFunction(plusEThetas, X, y, lambda_)\n",
    "        numericalGradApproxThisTheta = (plusECost - minusECost)/(2*e)\n",
    "        listGradients.append(numericalGradApproxThisTheta)\n",
    "    return(listGradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5ef70",
   "metadata": {},
   "source": [
    "## Implementing backpropagation part 2: the real deal\n",
    "\n",
    "Part of the equations you need to implement are in this figure, namely the calculation of the error per unit. Note that this figure has these calculations for **1 training example**. In reality we want to implement them with linear algebra such that we calculate for all training examples at the same time:\n",
    "![NNBackProp](NeuralNetworkBackprop.PNG)\n",
    "\n",
    "Concretely:\n",
    "* You can calculate the error for the output layer $\\delta^{(3)}$: that's the activations for this training sample minus its label vector. **WAAROM NIET * sigma'(z^(3))?**\n",
    "* You can use these to calculate $\\delta^{(2)}$: $(\\Theta^{(2)})^T \\cdot \\delta^{(3)} \\odot sigmoidGradient(z^{(2)}) $. Here, .* or $\\odot$ is elementwise multiplication, which is just * for numpy. This calculates the part of the error that is due to the Hidden Layer weighing its inputs wrong. \n",
    "* The remove $\\delta_0^{(2)}$ just means that you shouldn't calculate a gradient for the +1 neuron: it's not connected to the previous layer so there's nothing to propagate back!\n",
    "* There's no error for the input layer, so after this you're done propagating the error, but you do need to change the weights of the Hidden Layer!\n",
    "\n",
    "This might seem a little bit simple, but it performs all we need to do.\n",
    "$\\delta^{(3)}$ is a (10, 1) vector containing at each position how wrong a given neuron was for a prediction (or averaged over predictions of many samples). $(\\Theta^{(2)})^T$ is a (10, 26)$^T$ = (26, 10) matrix. So 26 rows: 25 units, 10 columns containing the weights of each unit to the 10 units in the output layer, and 1 row with the 10 biases, 1 for each output layer unit. When we multiply this with the error in the output layer (10, 1), we get for each output unit's weights and the biases how their _activations_ should be changed to decrease the error. We change their activations by changing the inputs to the activation function (_weights and biases_), so we should take the derivative of the activation function for these values to find the gradients to step down to change the weights and biases so as to reduce the error in the output layer. Just to be clear: $z^{(2)}$ is the weighted sum that layer 2 generates using matrix $\\Theta^{(1)}$.\n",
    "\n",
    "Up to you to:\n",
    "* Make a copy of `nnCostFunction`. Call it `nnGradientFunction`. It keeps the same arguments and does exactly the same to begin with, but it will return the gradients calculated by backpropagation rather than stop at calculating the cost function. First set it to return `None`. \n",
    "* Calculate $\\delta^{(3)}$ using the cost.\n",
    "* Calculate $\\delta^{(2)}$ using $\\delta^{(3)}$. For _one_ training example that would be the formula above. But remember, you have a matrix of shape (20,004 x 10). With the first row corresponding to the errors on the first training example,  etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f8fbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(42)\n",
    "thetaOneMatrix = rng.uniform(-0.12, 0.12, size = (25, 785))\n",
    "thetaTwoMatrix = rng.uniform(-0.12, 0.12, size = (10, 26))\n",
    "\n",
    "savedData = np.load(\"dataMNISTNeuralNetwork.npz\")\n",
    "X_train, X_test, y_train, y_test = savedData[\"XTrain\"], savedData[\"XTest\"], savedData[\"yTrain\"], savedData[\"yTest\"]\n",
    "\n",
    "\n",
    "nnThetas       = np.append(np.ravel(thetaOneMatrix), np.ravel(thetaTwoMatrix))\n",
    "inputLayerSize = 784\n",
    "hiddenLayerSize = 25\n",
    "classLabels = 10\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "\n",
    "# answer\n",
    "\n",
    "def nnGradientFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10):\n",
    "   \n",
    "    m = len(X)\n",
    "    \n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #calculating the forward pass\n",
    "    inputs        = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    weightedSumHL = inputs @ thetaOneMatrix.T\n",
    "    activationsHL  = mySigmoid(weightedSumHL)\n",
    "    \n",
    "    inputsOL      = np.c_[np.ones(shape = (len(activationsHL), 1)), activationsHL]\n",
    "    weightedSumOL = inputsOL @ thetaTwoMatrix.T\n",
    "    activationsOL = mySigmoid(weightedSumOL)\n",
    "    \n",
    "    print(\"Activations output layer shape: \" + str(activationsOL.shape))\n",
    "    print(\"Activations for one training example: \" + str(activationsOL[0,:]))\n",
    "    \n",
    "    #cost\n",
    "    J = 1/m * np.sum((- (y * np.log(activationsOL)) - ((1-y) * np.log(1-activationsOL))))\n",
    "    \n",
    "    #regularised cost\n",
    "    #remember: units in the rows, their parameters in the columns\n",
    "    #Hence, [:,1:] removes the columns with the bias term.\n",
    "    regThetaOne = np.sum(np.square(thetaOneMatrix[:,1:]))\n",
    "    regThetaTwo = np.sum(np.square(thetaTwoMatrix[:,1:]))\n",
    "    regCost     = J + (lambda_/(2*m)) * (regThetaOne + regThetaTwo)\n",
    "    \n",
    "    \n",
    "    #calculate error layer 3\n",
    "    smallDeltaThree = activationsOL - y\n",
    "    print(\"delta^(3): \" + str(smallDeltaThree))\n",
    "    print(\"delta^(3) shape: \" + str(smallDeltaThree.shape) + \"\\n\")\n",
    "    \n",
    "    #calculate the weighted sums that the HL generates that go into the activation function and then get sent to \n",
    "    #the output layer\n",
    "    weightedSumsLayerTwo = zTwo = weightedSumHL #= np.c_[np.ones(shape = (len(X), 1)), X] @ thetaOneMatrix.T\n",
    "    print(\"z^(2): \" + str(zTwo))\n",
    "    print(\"z^(2) shape: \" + str(zTwo.shape) + \"\\n\")\n",
    "    \n",
    "    sigmoidGradientOfZTwo = mySigmoidGradient(zTwo)\n",
    "    print(\"Sigmoid gradient: \" + str(sigmoidGradientOfZTwo))\n",
    "    print(\"Sigmoid gradient shape: \" + str(sigmoidGradientOfZTwo.shape))\n",
    "    smallDeltaTwo   = smallDeltaThree @ thetaTwoMatrix * np.c_[np.ones(shape = (len(X), 1)),\n",
    "                                                               mySigmoidGradient(zTwo)]\n",
    "    smallDeltaTwo   = smallDeltaTwo[:, 1:]\n",
    "    print(\"delta^(2): \" + str(smallDeltaTwo))\n",
    "    print(\"size of delta^(2): \" + str(smallDeltaTwo.shape)) \n",
    "    \n",
    "    \n",
    "    bigDeltaThree     = smallDeltaThree.T @ np.c_[np.ones(shape = len(activationsHL)),\n",
    "                                                 activationsHL]\n",
    "    print(\"Gradients output layer: \" + str(bigDeltaThree))\n",
    "    print(\"Gradients output layer shape: \" + str(bigDeltaThree))\n",
    "    \n",
    "    bigDeltaTwo       = smallDeltaTwo.T   @ np.c_[np.ones(shape = len(X)),\n",
    "                                                 X]\n",
    "    print(\"Gradients hidden layer: \" + str(bigDeltaTwo))\n",
    "    print(\"Gradients hidden layer shape: \" + str(bigDeltaTwo))\n",
    "    \n",
    "    #average values, we've now summed them over all training examples\n",
    "    bigDeltaTwo   = bigDeltaTwo   * 1/m\n",
    "    bigDeltaThree = bigDeltaThree * 1/m\n",
    "\n",
    "    finalGradients = np.append(np.ravel(bigDeltaTwo), np.ravel(bigDeltaThree))\n",
    "    \n",
    "    return finalGradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e53f89d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations output layer shape: (20004, 10)\n",
      "Activations for one training example: [0.50679905 0.47026724 0.48045209 0.45695882 0.50439269 0.42537011\n",
      " 0.45660324 0.55061342 0.58567024 0.48573213]\n",
      "delta^(3): [[ 0.50679905  0.47026724  0.48045209 ...  0.55061342  0.58567024\n",
      "  -0.51426787]\n",
      " [ 0.51597866  0.47184661  0.47270976 ...  0.57208504  0.58398438\n",
      "  -0.52566946]\n",
      " [ 0.50011956  0.45834984  0.47915344 ...  0.55698424  0.59916884\n",
      "  -0.51727237]\n",
      " ...\n",
      " [-0.46315771  0.47886494  0.45896251 ...  0.56575545  0.59230702\n",
      "   0.48211795]\n",
      " [-0.47937306  0.46951334  0.45806689 ...  0.54667469  0.58760281\n",
      "   0.47131704]\n",
      " [-0.48248302  0.47086103  0.45044764 ...  0.56747375  0.60631971\n",
      "   0.48869242]]\n",
      "delta^(3) shape: (20004, 10)\n",
      "\n",
      "z^(2): [[ 1.47211529  0.72742786 -0.61089524 ...  0.31187328 -0.9862839\n",
      "  -0.24400619]\n",
      " [ 1.97432473  1.22491872 -1.0079633  ...  0.44606513 -0.53983498\n",
      "   0.30598892]\n",
      " [ 1.79004393  0.16434479 -0.54288865 ...  1.06426415 -0.48233109\n",
      "  -0.21282825]\n",
      " ...\n",
      " [ 0.38756623  1.63036701 -1.15184267 ...  0.65412715 -0.47791407\n",
      "   0.10986511]\n",
      " [ 1.15106798  0.52755491  0.52837007 ...  0.2576143  -1.19924585\n",
      "  -0.60583446]\n",
      " [ 0.02406093  1.05307915 -0.10329358 ...  0.55975394 -0.12839311\n",
      "   0.24578842]]\n",
      "z^(2) shape: (20004, 25)\n",
      "\n",
      "Sigmoid gradient: [[0.1517938  0.21964022 0.22805306 ... 0.24401815 0.19785477 0.24631543]\n",
      " [0.10705939 0.17551031 0.1958873  ... 0.23796519 0.23263561 0.24423829]\n",
      " [0.12259907 0.24831949 0.23244795 ... 0.19070552 0.23600554 0.24719024]\n",
      " ...\n",
      " [0.24084215 0.13695617 0.18247937 ... 0.22505499 0.23625122 0.24924712]\n",
      " [0.18255283 0.23338153 0.23333245 ... 0.24589763 0.17796649 0.22839394]\n",
      " [0.24996382 0.19174258 0.24933434 ... 0.23139624 0.24897252 0.24626195]]\n",
      "Sigmoid gradient shape: (20004, 25)\n",
      "delta^(2): [[ 0.00559656  0.00756336 -0.01908792 ... -0.00396473  0.00714478\n",
      "   0.0120355 ]\n",
      " [ 0.00384219  0.00622899 -0.01671616 ... -0.00340885  0.00843743\n",
      "   0.01246619]\n",
      " [ 0.00455319  0.00829677 -0.01899492 ... -0.00289673  0.00926132\n",
      "   0.01231473]\n",
      " ...\n",
      " [-0.02873665 -0.00857492  0.00874672 ... -0.02454918  0.03492032\n",
      "   0.00739522]\n",
      " [-0.02147605 -0.01504532  0.01125369 ... -0.02647504  0.02629842\n",
      "   0.00626164]\n",
      " [-0.03021329 -0.0124812   0.01267393 ... -0.02542813  0.03782626\n",
      "   0.00759053]]\n",
      "size of delta^(2): (20004, 25)\n",
      "Gradients output layer: [[8172.97852635 5757.79638179 4640.51198969 3341.42512387 4567.29336633\n",
      "  4479.21467231 3684.07235729 3448.50762581 3040.55409447 5033.903715\n",
      "  3709.16273132 4368.31735499 4976.71319298 3022.80830847 4033.07529943\n",
      "  2649.20165895 3625.97001059 2620.89231734 5220.26671242 6074.22402993\n",
      "  3105.80858851 5553.84323038 5147.90017464 4101.07375535 3572.61557587\n",
      "  3972.61851466]\n",
      " [7062.30840141 5111.91323973 4391.06417086 2929.06938257 4312.62361704\n",
      "  4240.76477348 3186.16081289 2736.01296019 2827.20868369 4452.01240131\n",
      "  3222.58750427 4189.81997407 4473.34344794 2702.01250823 3978.32459885\n",
      "  1987.5377871  2980.1454298  1891.95566902 4424.14092168 5273.68967509\n",
      "  2641.56205311 4989.18883576 4867.46725401 3654.64534375 2902.41771834\n",
      "  3446.66297374]\n",
      " [7449.97522221 5217.70125991 4350.01368958 3122.8779238  4455.86429943\n",
      "  4498.5445377  3413.92191672 3032.68491175 2878.13444284 4611.52241437\n",
      "  3453.12784608 3951.12418366 4598.56331969 2807.82662491 3812.45748263\n",
      "  2259.20408051 3246.93243038 2391.91276782 4736.50300636 5452.34997083\n",
      "  2869.66177743 4931.09621904 4721.46736794 3845.19365761 3158.75205994\n",
      "  3611.52762818]\n",
      " [7049.70178442 4861.85573075 4076.00142299 2891.98786341 3926.23976839\n",
      "  4256.96372886 3139.97996311 2767.50401645 2821.49435022 4187.4900807\n",
      "  3214.87326413 3846.04595156 4333.14811617 2601.05292829 3579.54369724\n",
      "  2114.51879283 3035.63274442 2307.24909502 4454.74285988 5190.41373276\n",
      "  2832.76519996 4544.01022894 4500.23878673 3701.75268843 3107.05906674\n",
      "  3439.17105991]\n",
      " [8067.43023893 5590.93798157 4676.83847402 3300.33123235 4405.59153209\n",
      "  4601.78887345 3654.07239163 3174.60361769 3250.37518902 5004.80932447\n",
      "  3912.05269696 4459.76562203 5167.60989525 2922.06727768 3895.76900732\n",
      "  2398.87183013 3529.61723345 2331.55110412 5253.642347   5791.87050142\n",
      "  3201.45419268 5416.8805345  5011.73936364 4095.7583994  3389.112065\n",
      "  3848.89849244]\n",
      " [6863.48144505 4790.3477862  4007.60460413 2764.77707665 3786.21924873\n",
      "  3941.26452342 2954.35178039 2697.74092121 2703.39288847 4179.98684039\n",
      "  3135.6568932  3810.01533016 4272.98783807 2542.49185282 3421.15207793\n",
      "  2051.94449094 3005.657971   2117.3770209  4366.2179268  4926.05531812\n",
      "  2681.91005291 4653.09811436 4357.44506919 3581.91723142 2980.17477876\n",
      "  3297.04718533]\n",
      " [7322.4472575  5060.49475251 4332.92051627 2998.27922435 4146.32858367\n",
      "  4157.46800685 3216.13742103 3040.8448748  2879.64652351 4590.38974353\n",
      "  3318.37587007 3890.00222855 4607.54010821 2930.49207923 3479.53242855\n",
      "  2204.53123933 3324.63099199 2203.43417605 4650.01166436 5272.84631479\n",
      "  2704.52675741 4866.32414752 4542.88748983 3749.38047579 3047.1413054\n",
      "  3666.2267773 ]\n",
      " [9000.37449629 6329.43082313 5060.41549097 3898.71020571 4916.03057399\n",
      "  5235.17813995 4242.07528572 3506.20793336 3444.80032798 5668.97484972\n",
      "  4191.71926619 5090.33031801 5797.1503616  3086.6423017  4515.8886869\n",
      "  2820.43860177 4030.10376862 2529.51239477 5831.27296344 6769.44611001\n",
      "  3428.90925988 6054.57104872 5686.6326741  4552.6345067  3888.85149363\n",
      "  4487.30890722]\n",
      " [9857.68306534 6813.82510419 5728.76864004 4155.97433128 5648.24434512\n",
      "  5788.42318265 4395.5283398  3895.56459279 3884.18477165 5988.94958807\n",
      "  4674.10648551 5540.71526383 6132.87342172 3668.29513757 5183.14773047\n",
      "  3034.28938066 4218.0608434  2999.13412216 6286.73693652 6990.00032287\n",
      "  3908.80667123 6551.87666221 6332.10849768 5146.01759872 4375.85111848\n",
      "  4791.85705435]\n",
      " [7617.68039603 5253.33000353 4369.14522299 3276.8622789  4156.47372492\n",
      "  4340.2457728  3485.31265066 2926.95693323 2955.46925666 4694.66921327\n",
      "  3645.85862221 4282.69785648 4830.91512389 2739.36787253 3764.19763487\n",
      "  2282.40546052 3414.23530864 2116.69593949 4885.34904242 5460.24436876\n",
      "  2999.22563123 5029.90790568 4765.58292174 3898.40974795 3249.96268771\n",
      "  3746.49428785]]\n",
      "Gradients output layer shape: [[8172.97852635 5757.79638179 4640.51198969 3341.42512387 4567.29336633\n",
      "  4479.21467231 3684.07235729 3448.50762581 3040.55409447 5033.903715\n",
      "  3709.16273132 4368.31735499 4976.71319298 3022.80830847 4033.07529943\n",
      "  2649.20165895 3625.97001059 2620.89231734 5220.26671242 6074.22402993\n",
      "  3105.80858851 5553.84323038 5147.90017464 4101.07375535 3572.61557587\n",
      "  3972.61851466]\n",
      " [7062.30840141 5111.91323973 4391.06417086 2929.06938257 4312.62361704\n",
      "  4240.76477348 3186.16081289 2736.01296019 2827.20868369 4452.01240131\n",
      "  3222.58750427 4189.81997407 4473.34344794 2702.01250823 3978.32459885\n",
      "  1987.5377871  2980.1454298  1891.95566902 4424.14092168 5273.68967509\n",
      "  2641.56205311 4989.18883576 4867.46725401 3654.64534375 2902.41771834\n",
      "  3446.66297374]\n",
      " [7449.97522221 5217.70125991 4350.01368958 3122.8779238  4455.86429943\n",
      "  4498.5445377  3413.92191672 3032.68491175 2878.13444284 4611.52241437\n",
      "  3453.12784608 3951.12418366 4598.56331969 2807.82662491 3812.45748263\n",
      "  2259.20408051 3246.93243038 2391.91276782 4736.50300636 5452.34997083\n",
      "  2869.66177743 4931.09621904 4721.46736794 3845.19365761 3158.75205994\n",
      "  3611.52762818]\n",
      " [7049.70178442 4861.85573075 4076.00142299 2891.98786341 3926.23976839\n",
      "  4256.96372886 3139.97996311 2767.50401645 2821.49435022 4187.4900807\n",
      "  3214.87326413 3846.04595156 4333.14811617 2601.05292829 3579.54369724\n",
      "  2114.51879283 3035.63274442 2307.24909502 4454.74285988 5190.41373276\n",
      "  2832.76519996 4544.01022894 4500.23878673 3701.75268843 3107.05906674\n",
      "  3439.17105991]\n",
      " [8067.43023893 5590.93798157 4676.83847402 3300.33123235 4405.59153209\n",
      "  4601.78887345 3654.07239163 3174.60361769 3250.37518902 5004.80932447\n",
      "  3912.05269696 4459.76562203 5167.60989525 2922.06727768 3895.76900732\n",
      "  2398.87183013 3529.61723345 2331.55110412 5253.642347   5791.87050142\n",
      "  3201.45419268 5416.8805345  5011.73936364 4095.7583994  3389.112065\n",
      "  3848.89849244]\n",
      " [6863.48144505 4790.3477862  4007.60460413 2764.77707665 3786.21924873\n",
      "  3941.26452342 2954.35178039 2697.74092121 2703.39288847 4179.98684039\n",
      "  3135.6568932  3810.01533016 4272.98783807 2542.49185282 3421.15207793\n",
      "  2051.94449094 3005.657971   2117.3770209  4366.2179268  4926.05531812\n",
      "  2681.91005291 4653.09811436 4357.44506919 3581.91723142 2980.17477876\n",
      "  3297.04718533]\n",
      " [7322.4472575  5060.49475251 4332.92051627 2998.27922435 4146.32858367\n",
      "  4157.46800685 3216.13742103 3040.8448748  2879.64652351 4590.38974353\n",
      "  3318.37587007 3890.00222855 4607.54010821 2930.49207923 3479.53242855\n",
      "  2204.53123933 3324.63099199 2203.43417605 4650.01166436 5272.84631479\n",
      "  2704.52675741 4866.32414752 4542.88748983 3749.38047579 3047.1413054\n",
      "  3666.2267773 ]\n",
      " [9000.37449629 6329.43082313 5060.41549097 3898.71020571 4916.03057399\n",
      "  5235.17813995 4242.07528572 3506.20793336 3444.80032798 5668.97484972\n",
      "  4191.71926619 5090.33031801 5797.1503616  3086.6423017  4515.8886869\n",
      "  2820.43860177 4030.10376862 2529.51239477 5831.27296344 6769.44611001\n",
      "  3428.90925988 6054.57104872 5686.6326741  4552.6345067  3888.85149363\n",
      "  4487.30890722]\n",
      " [9857.68306534 6813.82510419 5728.76864004 4155.97433128 5648.24434512\n",
      "  5788.42318265 4395.5283398  3895.56459279 3884.18477165 5988.94958807\n",
      "  4674.10648551 5540.71526383 6132.87342172 3668.29513757 5183.14773047\n",
      "  3034.28938066 4218.0608434  2999.13412216 6286.73693652 6990.00032287\n",
      "  3908.80667123 6551.87666221 6332.10849768 5146.01759872 4375.85111848\n",
      "  4791.85705435]\n",
      " [7617.68039603 5253.33000353 4369.14522299 3276.8622789  4156.47372492\n",
      "  4340.2457728  3485.31265066 2926.95693323 2955.46925666 4694.66921327\n",
      "  3645.85862221 4282.69785648 4830.91512389 2739.36787253 3764.19763487\n",
      "  2282.40546052 3414.23530864 2116.69593949 4885.34904242 5460.24436876\n",
      "  2999.22563123 5029.90790568 4765.58292174 3898.40974795 3249.96268771\n",
      "  3746.49428785]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients hidden layer: [[-221.88941345    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [ 130.18579792    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [  33.42637502    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " ...\n",
      " [-487.67420661    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [ 140.56213249    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [  38.66465763    0.            0.         ...    0.\n",
      "     0.            0.        ]]\n",
      "Gradients hidden layer shape: [[-221.88941345    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [ 130.18579792    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [  33.42637502    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " ...\n",
      " [-487.67420661    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [ 140.56213249    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [  38.66465763    0.            0.         ...    0.\n",
      "     0.            0.        ]]\n",
      "(19885,)\n",
      "(19885,)\n"
     ]
    }
   ],
   "source": [
    "b = nnGradientFunction(nnThetas, X, y)\n",
    "print(b.shape)\n",
    "print(nnThetas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4aa55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c5947b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XTrain', 'XTest', 'yTrain', 'yTest']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45218cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b46bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
