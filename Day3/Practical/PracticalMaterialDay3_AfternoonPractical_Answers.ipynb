{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Afternoon practical day 3\n",
    "\n",
    "You've just learned about convolutional neural networks. There will be a sneak peek into them at the end. For now, however, we finally implement backpropagation.\n",
    "\n",
    "**Note: there's a lot of math in this notebook. This is to help you, not to make your life hell. So know this: the goal here is to implement backpropagation with linear algebra. As long as you understand what backpropagation aims to do and can succesfully implement it, that's mission completed. Skip whatever math you don't want to read, especially the recaps!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, Math\n",
    "import scipy\n",
    "from scipy.optimize import fmin_bfgs, fmin_cg, fmin, fmin_l_bfgs_b, fmin_tnc\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important functions\n",
    "def mySigmoid(data):\n",
    "    data= np.array(data)\n",
    "    output = 1/(1+ np.exp(-data))\n",
    "    return output\n",
    "\n",
    "def mySigmoidGradient(x):\n",
    "    outcome = mySigmoid(x) * (1-mySigmoid(x))\n",
    "    return outcome\n",
    "\n",
    "def nnCostFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10, smallValueToRemoveInfinity = 1e-12):\n",
    "   \n",
    "    m = len(X)\n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #calculating the forward pass\n",
    "    inputs        = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    weightedSumHL = inputs @ thetaOneMatrix.T\n",
    "    activationsHL  = mySigmoid(weightedSumHL)\n",
    "    \n",
    "    inputsOL      = np.c_[np.ones(shape = (len(activationsHL), 1)), activationsHL]\n",
    "    weightedSumOL = inputsOL @ thetaTwoMatrix.T\n",
    "    activationsOL = mySigmoid(weightedSumOL)\n",
    "    \n",
    "    #cost\n",
    "    J = 1/m * np.sum((- (y * np.log(activationsOL + smallValueToRemoveInfinity)) - ((1-y) * np.log(1-activationsOL + smallValueToRemoveInfinity))))\n",
    "    \n",
    "    #regularised cost\n",
    "    #remember: units in the rows, their parameters in the columns\n",
    "    #Hence, [:,1:] removes the columns with the bias term.\n",
    "    regThetaOne = np.sum(np.square(thetaOneMatrix[:,1:]))\n",
    "    regThetaTwo = np.sum(np.square(thetaTwoMatrix[:,1:]))\n",
    "    regCost     = J + (lambda_/(2*m)) * (regThetaOne + regThetaTwo)\n",
    "    \n",
    "    return regCost\n",
    "\n",
    "def numericalGradientApproximation(nnThetas, X, y, lambda_ = 0, e = 1e-4, indexToStop = 500):\n",
    "    nnThetasMinusE = nnThetas-e\n",
    "    nnThetasPlusE  = nnThetas+e\n",
    "    listGradients = []\n",
    "    for index, value in enumerate(nnThetas):\n",
    "        if index % 50 == 0:\n",
    "            print(\"Parameter \" + str(index) + \" out of \" + str(len(nnThetas)))\n",
    "        minusEThetas = nnThetas; minusEThetas[index] = nnThetasMinusE[index]\n",
    "        minusECost   = nnCostFunction(minusEThetas, X, y, lambda_)\n",
    "        plusEThetas  = nnThetas; plusEThetas[index] = nnThetasPlusE[index]\n",
    "        plusECost    = nnCostFunction(plusEThetas, X, y, lambda_)\n",
    "        numericalGradApproxThisTheta = (plusECost - minusECost)/(2*e)\n",
    "        #Only do this for the first ~150 thetas. Otherwise it takes entirely too long!\n",
    "        listGradients.append(numericalGradApproxThisTheta)\n",
    "        if index == indexToStop:\n",
    "            break\n",
    "    return np.array(listGradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4728783",
   "metadata": {},
   "source": [
    "## Backpropagation recap\n",
    "### Feel free to skip if you've heard enough about it already (specifically, skip to \"Backpropagation-linear-algebra\")\n",
    "\n",
    "Let's take a moment to step back and understand what you want to do here. We want to change the parameters of the network such that we can decrease the cost on our training examples. This involves calculating partial derivatives of the cost with respect to all the parameters in the network. The _way_ we do that is by chaining many partial derivatives together: the chain rule of derivation will be getting some heavy use. To recap what you've learned so far: the fundamental equations of backpropagation are:\n",
    "1. **The error of a neuron**:\n",
    "$$\\delta^{(l)}_j = \\frac{\\partial C}{\\partial z^{(l)}_j} = \\frac{\\partial C}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j}$$\n",
    "This says: the error of a certain neuron _j_ in layer _l_ is equal to the partial derivative of the cost with respect to the weighted sum it produces (remember, $z^{(l)}_j$ is just $\\sum \\limits _{i=1} ^{n_{weights}} w_i \\cdot a^{(l-1)}_i  + b$. In other words: in our case a neuron in the output network takes in 25 inputs ($a^{(l-1)}_1$ until $a^{(l-1)}_{25}$) and multiplies them with a separate weight for each, takes the sum, and adds the bias). And that $\\frac{\\partial C}{\\partial z^{(l)}_j}$ in turn is equal to the partial derivative of the cost with respect to the activation, times the partial derivative of the activation with respect to this weighted sum. This is because of the chain rule: ![ChainRule](ChainRule.PNG) We want the derivative of the cost relative to the weighted sum (because we can change those weights and the bias), but those are passed through the nonlinear sigmoid activation function. So in our case, $ f = \\sigma(g)$ and $g(inputs, weights, biases) = \\sum \\limits _{i=1} ^{n_{weights}} w_{i,j} \\cdot a^{(l-1)}_i  + b^{(l)}_j = z^{(l)}_j$. Luckily the derivative of the sigmoid function is easy: $\\sigma(x) \\cdot \\sigma(1-x)$. Note also that our function g depends on the inputs of the previous layer, the weights, and the bias, so we need partial derivatives with respect to each of these! <br> <br> If we fill that all in we get:\n",
    "$$ \\delta^{(l)}_j = \\frac{\\partial C}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j} = \\frac{\\partial C}{\\partial {sigmoid(\\sum \\limits _{i=1} ^{n_{weights}} w_{i,j} \\cdot a^{(l-1)}_i  + b^{(l)}_j})} \\cdot \\frac{{\\partial {sigmoid(\\sum \\limits _{i=1} ^{n_{weights}} w_{i,j} \\cdot a^{(l-1)}_i  + b^{(l)}_j})}}{{\\partial \\sum \\limits _{i=1} ^{n_{weights}} w_{i,j} \\cdot a^{(l-1)}_i  + b^{(l)}_j}}$$ Which equals:\n",
    "$$ \\delta^{(l)}_j = \\frac{\\partial C}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j} = \\frac{\\partial C}{\\partial {sigmoid(\\sum \\limits _{i=1} ^{n_{weights}} w_{i,j} \\cdot a^{(l-1)}_i  + b^{(l)}_j})} \\cdot {sigmoid(z^{(l)}_j) \\cdot (1 - sigmoid(z^{(l)}_j))}$$\n",
    "You can just think of delta as the partial derivative of the cost w.r.t. the weighted sum that a certain neuron produces. To be entirely sure that this is understood, look at the image below: ![image](ExampleNeuralNetwork5.PNG) The cost function in this case hinges on $$\\begin{bmatrix}a^{(l)}_1 = 0.33 \\\\ a^{(l)}_2 = 0.8 \\\\ a^{(l)}_3 = 0.189\\end{bmatrix}$$ So we have here $$\\begin{bmatrix}\\delta^{(l)}_1 = \\sigma'(0 -  0.330) = \\sigma'(-0.33) \\\\ \\delta^{(l)}_2 = \\sigma'(1 - 0.800) = \\sigma'(0.2) \\\\ \\delta^{(l)}_3 = \\sigma'(0 - 0.189) = \\sigma'(-0.189)\\end{bmatrix}$$ We can calculate those values, and they turn out to be  $$\\begin{bmatrix}\\delta^{(l)}_1 \\approx 0.24 \\\\ \\delta^{(l)}_2 \\approx 0.25 \\\\ \\delta^{(l)}_3 \\approx 0.25\\end{bmatrix}$$ If those values seem awfully similar to you, remember that between -1 and 1 the sigmoid curve pretty much [looks like a straight line](https://keisan.casio.com/exec/system/15157249643325#!) and hence it makes sense that its gradient there is pretty much one value. <br> **Now it turns out that for the derivative of cross entropy (the cost function we're using instead of MSE), $\\frac{\\partial C}{\\partial z^{(l)}_j}$ just simplifies to $\\delta^{(l)}_j$ because it cancels out the sigmoid derivative. See the derivative of cross entropy [here](https://stats.stackexchange.com/a/371282) and look at the Deep Learning Crash Course slides in the extra material if you want to learn more!**\n",
    "Okay, so we now have a partial derivative for the cost w.r.t. the neurons in the output layer. With that we can move on to part 2 and 3. <br> <br>\n",
    "\n",
    "2. **The partial derivative of the cost w.r.t. a bias**. This is just:  $$\\frac{\\partial C}{\\partial b^{(l)}_j} = \\frac{\\partial C}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j} \\cdot \\frac{\\partial z^{(l)}_j}{\\partial b^{(l)}_j} = \\delta^{(l)}_j \\cdot \\frac{\\partial z^{(l)}_j}{\\partial b^{(l)}_j}$$ Or in words: how the cost is affected by a certain bias is how the cost is changed by a certain activation times how that activation is changed by a weighted sum times how that weighted sum is changed by a change in its bias. It is easy to see that $$\\frac{\\partial z^{(l)}_j}{\\partial b^{(l)}_j} = \\frac{\\partial \\sum \\limits _{i=1} ^{n_{weights}} w_i \\cdot a^{(l-1)}_i  + b^{(l)}_j}{\\partial {b^{(l)}_j}} = 1 $$This is just like $$\\frac{\\partial (ax + b)}{\\partial {b}} = 1 $$ while $$\\frac{\\partial (ax + b)}{\\partial {a}} = x $$Taken together, the partial derivative of the cost relative to a certain bias is just $$\\delta^{(l)}_j$$\n",
    "\n",
    "3. **The partial derivative of the cost w.r.t. a weight**. This is just: $$\\frac{\\partial C}{\\partial w^{(l)}_{i,j}} = \\frac{\\partial C}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j} \\cdot \\frac{\\partial z^{(l)}_j}{\\partial w^{(l)}_{i,j}} = \\delta^{(l)}_j \\cdot \\frac{\\partial z^{(l)}_j}{\\partial w^{(l)}_{i,j}}$$ For our toy network above, i runs from 1 to 2: there are 2 inputs for each neuron in the output layer, 1 for each hidden unit, and they both have a weight. j runs from 1 to 3 for the 3 neurons in the output layer. If we expand the term on the right and calculate it, we get: $$\\frac{\\partial z^{(l)}_j}{\\partial w^{(l)}_{i,j}} = \\frac{\\partial \\sum \\limits _{i=1} ^{n_{weights}} w_{i,j} \\cdot a^{(l-1)}_i  + b^{(l)}_j}{\\partial {w^{(l)}_{i,j}}} =  a^{(l-1)}_i$$ Putting it together, we get: $$\\delta^{(l)}_j \\cdot a^{(l-1)}_i$$ <br> <br>\n",
    "\n",
    "4. **Computing the partial derivative of the cost w.r.t. the activations from the previous layer --> propagating the errors back one layer**. We've done the partial derivatives of the weights and the biases, but we also want to calculate the partial derivative of the cost function relative to the input activations the output layer got. Here goes: \n",
    "$$\\frac{\\partial C}{\\partial z^{(l-1)}_{j}} = \\frac{\\partial C}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j} \\cdot \\frac{\\partial z^{(l)}_j}{\\partial a^{(l-1)}_{j}} \\cdot \\frac{\\partial a^{(l-1)}_j}{\\partial z^{(l-1)}_{j}} = \\delta^{(l)}_j \\cdot \\frac{\\partial z^{(l)}_j}{\\partial a^{(l-1)}_{j}} \\cdot \\sigma'(z^{(l-1)}_j)$$\n",
    "In words: How the partial derivative of the cost hinges on the weighted sum $z^{(l-1)}_{j}$ in the previous layer is how the cost hinges on the activations in the output layer, times how the activations in the output layer hinge on the weighted sum that the output layer calculates, times how that weighted sum hinges on the activations it takes in from layer l-1, times how those activations hinge on their weighted sum. We already know $$\\frac{\\partial C}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j} = \\delta^{(l)}_j \\\\ \\mbox{and} \\frac{\\partial a^{(l-1)}_j}{\\partial z^{(l-1)}_{j}} \\mbox{is just the sigmoid derivative: } \\sigma(z^{(l-1)}_j) \\cdot (1-\\sigma(z^{(l-1)}_j))$$ So all we need to do is calculate: $$\\frac{\\partial z^{(l)}_j}{\\partial a^{(l-1)}_{j}}$$ which is just:\n",
    "$$\\frac{\\partial z^{(l)}_j}{\\partial a^{(l-1)}_{j}} = \\frac{\\partial \\sum \\limits _{j=1} ^{n_{weights}} w_{j} \\cdot a^{(l-1)}_j  + b^{(l)}_j}{\\partial {a^{(l-1)}_{j}}} =  \\sum \\limits _{j=1} ^{n_{weights}} w_{j}$$. Finally this results in: $$\\delta^{(l-1)}_j = \\sum \\limits _{j=1} ^{n_{weights}} w_{j} \\cdot \\delta^{(l)}_j \\cdot \\sigma'(z^{(l-1)}_j)$$\n",
    "\n",
    "## Phew, that's a lot. The executive summary:\n",
    "1. Calculate error in the final layer with $\\delta_j^{(L)} = \\frac{\\partial C}{\\partial a^{(L)}_j} \\cdot \\frac{\\partial a^{(L)}_j}{\\partial z^{(l)}_j} = \\frac{\\partial C}{\\partial a^{(L)}_j} \\cdot \\sigma'(z^{(L)}_j)$. **For the cross entropy cost function that we use, this is just $a^{(L)}_j - y_j$**\n",
    "2. The partial derivative w.r.t. to each bias is just $\\delta_j^{(l)}$.\n",
    "3. The partial derivative of the cost w.r.t. each weight is just $\\delta_j^{(l)} \\cdot a^{(l-1)}_j$\n",
    "4. You can propagate the initial error back by multiplying it with the weights and multiplying with the derivative of the sigmoid function to get $\\delta^{(l-1)}_j = \\sum \\limits _{j=1} ^{n_{weights}} w_{j} \\cdot \\delta^{(l)}_j \\sigma'(z^{(l-1)}_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9b43e0",
   "metadata": {},
   "source": [
    "## Implementing this with linear algebra\n",
    "\n",
    "Now the above has myriad indices (i,j,l) to show what weight of what neuron in what layer we are talking about, because we need to to these things for every weight in every neuron in every layer, and that for every training example as well. You can do this using loops for a single training example, but that is not the ~wae~ way.\n",
    "\n",
    "Instead, we'll do this using linear algebra. Here's a toy network again: ![image](ToyNetworkLinAlg2.PNG) \n",
    "What's changed is that I have already calculated an (average) $\\delta^{(L)}_j$ based on two training examples. These training examples have 2 features each. First a recap of the forward pass\n",
    "\n",
    "## Forward pass linear algebra recap (feel free to skip)\n",
    "The data looks like:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & feat_1val_1 & feat_2val_1 \\\\ 1 & feat_1val_2 & feat_2val_2 \\end{bmatrix}$$\n",
    "\n",
    "The first matrix with the weights and biases is $\\Theta^{(1)}$ and looks like this:\n",
    "\n",
    "$$\\begin{bmatrix} \\mbox{biasneuron_1} & \\mbox{weightinput_1neuron_1} & \\mbox{weightinput_2neuron_1} \\\\  \\mbox{biasneuron_2} & \\mbox{weightinput_1neuron_2} & \\mbox{weightinput_2neuron_2} \\end{bmatrix}$$ \n",
    "\n",
    "And its transpose looks like: $$\\begin{bmatrix} \\mbox{biasneuron_1} & \\mbox{biasneuron_2} \\\\ \\mbox{weightinput_1neuron_1} & \\mbox{weightinput_1neuron_2} \\\\ \\mbox{weightinput_2neuron_1} & \\mbox{weightinput_2neuron_2} \\end{bmatrix}$$ \n",
    "\n",
    "Multiplied together you get: $ F \\cdot {\\Theta^{(1)}}^T$. 2 by 3 times 3 by 2 will make a 2 by 2 matrix of the weighted sums like so, $Z^{(l)} =$\n",
    "\n",
    "$$ \\begin{bmatrix} 1 \\cdot \\mbox{biasneuron_1} + \\mbox{feat_1val_1} \\cdot \\mbox{weightinput_1neuron_1} + \\mbox{feat_2val_1} \\cdot \\mbox{weightinput_2neuron_1} & 1 \\cdot \\mbox{biasneuron_1} + \\mbox{feat_1val_2} \\cdot \\mbox{weightinput_1neuron_1} + \\mbox{feat_2val_2} \\cdot \\mbox{weightinput_2neuron_1} \\\\ 1 \\cdot \\mbox{biasneuron_2} + \\mbox{feat_1val_1} \\cdot \\mbox{weightinput_1neuron_2} + \\mbox{feat_2val_1} \\cdot \\mbox{weightinput_2neuron_2} & 1 \\cdot \\mbox{biasneuron_2} + \\mbox{feat_1val_2} \\cdot \\mbox{weightinput_1neuron_2} + \\mbox{feat_2val_2} \\cdot \\mbox{weightinput_2neuron_2} \\end{bmatrix}$$ \n",
    "\n",
    "In this matrix, each row contains the weighted sums, $z^{(l)}$ for this layer for a certain training example, with column 1 corresponding to neuron 1, and column 2 corresponding to neuron 2. We can now perform the sigmoid on this layer to get the activations. I'm shortening the terms to make things more legible.\n",
    "\n",
    "$$ A^{(l)} = \\begin{bmatrix} \\sigma( 1 \\cdot b_1 + f_{1}v_{1} \\cdot w_1n_1 + f_2v_1 \\cdot w_2n_1) &\n",
    "\\sigma( 1 \\cdot b_1 + f_1v_2 \\cdot w_1n_2 + f_2v_2 \\cdot w_2n_2) \\\\ \\sigma( 1 \\cdot b_2 + f_1v_1 \\cdot w_1n_1 + f_2v_1 \\cdot w_2n_1) & \\sigma( 1 \\cdot b_2 + f_1v_2 \\cdot w_1n_2 + f_2v_2 \\cdot w_2n_2) \\end{bmatrix} = \\begin{bmatrix} a^{(l)}_1 & a^{(l)}_2 \\\\ a^{(l)}_1 & a^{(l)}_2 \\end{bmatrix}$$ \n",
    "\n",
    "Now we take this, add again a row of 1 to be multiplied with the biases in ${\\Theta^{(2)}}^T$ and do that multiplication. Note that I've added the layer indices as superscript to distinguish them from the previous layer's biases and weights:\n",
    "\n",
    "$$Z^{(L)} = \\begin{bmatrix} 1 & a^{(l)}_1 & a^{(l)}_2 \\\\ 1 & a^{(l)}_1 & a^{(l)}_2 \\end{bmatrix} \\cdot \\begin{bmatrix} b^{L}_1 & b^{(L)}_2 & b^{(L)}_3 \\\\ w_1n_1^{(L)} & w_1n_2^{(L)} & w_1n_3^{(L)} \\\\ w_2n_1^{(L)} & w_2n_2^{(L)} & w_2n_3^{(L)}\\end{bmatrix} = \\begin{bmatrix} z^{(L)}_1 & z^{(L)}_2 & z^{(L)}_3 \\\\ z^{(L)}_1 & z^{(L)}_2 & z^{(L)}_3 \\end{bmatrix}$$\n",
    "\n",
    "Where again the first row are the weighted sums. Using the sigmoid activation function we get: \n",
    "$$\\sigma(Z^{(L)}) = A^{(L)} = \\begin{bmatrix} a^{(L)}_1 & a^{(L)}_2 & a^{(L)}_3 \\\\ a^{(L)}_1 & a^{(L)}_2 & a^{(L)}_3 \\end{bmatrix}$$\n",
    "\n",
    "It's these final activations that you can compare with labels, which look like this:\n",
    "$$Y = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "## Backpropagation linear algebra\n",
    "\n",
    "So finally, because of the cross entropy cost function (which allows us to skip multiplying with $\\sigma'()$), we get: \n",
    "$$\\delta^{(L)} = A^{(L)} - Y$$ This brings us back to this image, where you'd begin backpropagation with the _average error_ of the neurons over each training example if you weren't using linear algebra. With linear algebra, it's easier to do the averaging later, so we'll work with the 2 by 3 matrix of $\\delta$ values for the 2 training samples:\n",
    "![image](ToyNetworkLinAlg3.PNG)\n",
    "\n",
    "Now let's get us some partial derivatives!\n",
    "1. **Partial derivatives w.r.t. weights and biases output layer**: for the biases, this is just $\\delta^{(L)}$. Thinking in linear algebra, we thus need to multiply them with a column or row of ones in some matrix. For the weights, it's each element of $\\delta^{(L)}$ multiplied with the activations of the previous layer that led to it $A^{(l)}$. Note that I've added indices like $s_1$ to indicate sample 1 or sample 2. So we get: $$ {\\delta^{(L)}}^T \\cdot \\begin{bmatrix} 1 & a^{(l)}_1s_1 & a^{(l)}_2s_1 \\\\ 1 & a^{(l)}_1s_2 & a^{(l)}_2s_2 \\end{bmatrix} = \\begin{bmatrix} 0.6 & 0.8 \\\\ 0.4 & -0.3 \\\\ 0.03 & 0.54 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & a^{(l)}_1s_1 & a^{(l)}_2s_1 \\\\ 1 & a^{(l)}_1s_2 & a^{(l)}_2s_2 \\end{bmatrix} = \\begin{bmatrix} 0.6 \\cdot 1 + 0.8 \\cdot 1 & 0.6 \\cdot a^{(l)}_1s_1  + 0.8 \\cdot a^{(l)}_1s_2  & 0.6 \\cdot a^{(l)}_2s_1 + 0.8 \\cdot a^{(l)}_2s_2  \\\\ 0.4 \\cdot 1 + -0.3 \\cdot 1 & 0.4 \\cdot a^{(l)}_1s_1 + -0.3 \\cdot a^{(l)}_1s_2 & 0.4 \\cdot a^{(l)}_2s_1 + -0.3 \\cdot a^{(l)}_2s_2 \\\\ 0.03 \\cdot 1 + 0.54 \\cdot 1 & 0.03 \\cdot a^{(l)}_1s_1 + 0.54 \\cdot a^{(l)}_1s_2 & 0.03 \\cdot a^{(l)}_2s_1 + 0.54 \\cdot a^{(l)}_2s_2 \\end{bmatrix}$$ What you see is that this is a matrix which has in its rows the sums of the gradient for each neuron. Thus, in the first row, the first column just has the summed $\\delta^{(L)}$-values for that neuron: exactly the partial derivative of the cost w.r.t. the bias of neuron 1 in the output layer, summed over the training examples! The second column has the partial derivative of the cost w.r.t. the first weight of neuron 1 in the output layer, summed over the training examples. Finally, the third column has the partial derivative of the cost w.r.t. the second weight of neuron 1 in the output layer, summed over the training examples. The second and third row have this for the other two neurons in the output layer. To go from this sum to the gradient, all we have to do is multiply with $\\frac{1}{m}$, which in this case is $\\frac{1}{2}$: $$\\Delta^{(L)} = \\frac{1}{m}\\begin{bmatrix} 0.6 \\cdot 1 + 0.8 \\cdot 1 & 0.6 \\cdot a^{(l)}_1s_1  + 0.8 \\cdot a^{(l)}_1s_2  & 0.6 \\cdot a^{(l)}_2s_1 + 0.8 \\cdot a^{(l)}_2s_2  \\\\ 0.4 \\cdot 1 + -0.3 \\cdot 1 & 0.4 \\cdot a^{(l)}_1s_1 + -0.3 \\cdot a^{(l)}_1s_2 & 0.4 \\cdot a^{(l)}_2s_1 + -0.3 \\cdot a^{(l)}_2s_2 \\\\ 0.03 \\cdot 1 + 0.54 \\cdot 1 & 0.03 \\cdot a^{(l)}_1s_1 + 0.54 \\cdot a^{(l)}_1s_2 & 0.03 \\cdot a^{(l)}_2s_1 + 0.54 \\cdot a^{(l)}_2s_2 \\end{bmatrix}$$ This'll average the gradients over all training examples, giving you the gradient w.r.t. the bias and cost for each neuron in the output layer! <br> . Note that this matrix is exactly in the shape of $\\Theta^{(2)}$, which looks like: $$\\begin{bmatrix} b^{(L)}_1 & w_1n_1^{(L)} & w_2n_1^{(L)} \\\\ b^{(L)}_2  & w_1n_2^{(L)} & w_2n_2^{(L)}  \\\\ b^{(L)}_3  & w_1n_3^{(L)} & w_2n_3^{(L)}\\end{bmatrix}$$ So if you'd want to take a gradient step, you could just do $$\\Theta^{(2)}_{new} = \\Theta^{(2)} - \\frac{\\alpha}{m} \\cdot \\begin{bmatrix} 0.6 \\cdot 1 + 0.8 \\cdot 1 & 0.6 \\cdot a^{(l)}_1s_1  + 0.8 \\cdot a^{(l)}_1s_2  & 0.6 \\cdot a^{(l)}_2s_1 + 0.8 \\cdot a^{(l)}_2s_2  \\\\ 0.4 \\cdot 1 + -0.3 \\cdot 1 & 0.4 \\cdot a^{(l)}_1s_1 + -0.3 \\cdot a^{(l)}_1s_2 & 0.4 \\cdot a^{(l)}_2s_1 + -0.3 \\cdot a^{(l)}_2s_2 \\\\ 0.03 \\cdot 1 + 0.54 \\cdot 1 & 0.03 \\cdot a^{(l)}_1s_1 + 0.54 \\cdot a^{(l)}_1s_2 & 0.03 \\cdot a^{(l)}_2s_1 + 0.54 \\cdot a^{(l)}_2s_2 \\end{bmatrix} $$ <br> <br>\n",
    "\n",
    "2. **Partial derivatives w.r.t. weighted sums in the previous layer ($Z^{(l)}$), i.e. propagating back the error**: Remember, the steps we need to take are: multiplying $\\delta^{(L)}$ with the weights (which are in $\\Theta^{(2)}$) and then multiplying with the derivative of the sigmoid on the weighted sums that the Hidden Layer produces ($\\sigma(Z^{(l)})$. To do that first multiply the two together: $$\\begin{bmatrix} 0.6 & 0.4 & 0.03 \\\\ 0.8  & -0.3 & 0.54 \\end{bmatrix} \\cdot \\begin{bmatrix} b^{(L)}_1 & w_1n_1^{(L)} & w_2n_1^{(L)} \\\\ b^{(L)}_2  & w_1n_2^{(L)} & w_2n_2^{(L)}  \\\\ b^{(L)}_3  & w_1n_3^{(L)} & w_2n_3^{(L)}\\end{bmatrix}$$ You'll see that there's something wrong here: there's biases, but the partial derivative only includes the weight, not them. So only include the 2nd to last column of $\\Theta^{(2)}$ instead: $$\\begin{bmatrix} 0.6 & 0.4 & 0.03 \\\\ 0.8  & -0.3 & 0.54 \\end{bmatrix} \\cdot \\begin{bmatrix}  w_1n_1^{(L)} & w_2n_1^{(L)} \\\\ w_1n_2^{(L)} & w_2n_2^{(L)}  \\\\ w_1n_3^{(L)} & w_2n_3^{(L)}\\end{bmatrix} = \\begin{bmatrix} 0.6 \\cdot w_1n_1^{(L)} + 0.4 \\cdot w_1n_2^{(L)} + 0.03 \\cdot w_1n_3^{(L)} & 0.6 \\cdot w_2n_1^{(L)} + 0.4 \\cdot w_2n_2^{(L)} + 0.03 \\cdot w_2n_3^{(L)} \\\\ 0.8 \\cdot w_1n_1^{(L)} + -0.3 \\cdot w_1n_2^{(L)} + 0.54 \\cdot w_1n_3^{(L)} & 0.8 \\cdot w_2n_1^{(L)} + -0.3 \\cdot w_2n_2^{(L)} + 0.54 \\cdot w_2n_3^{(L)} \\end{bmatrix}$$ This gives us a matrix very similar to $\\delta^{(L)}$. Only now there's just 2 columns for the 2 neurons in the HL! The final thing that's left to do is to multiply each of these values with the corresponding sigmoid derivative: $$\\sigma'(Z^{(l)}) = \\sigma'(\\begin{bmatrix}  1 \\cdot b_1 + f_{1}v_{1} \\cdot w_1n_1 + f_2v_1 \\cdot w_2n_1 & 1 \\cdot b_1 + f_1v_2 \\cdot w_1n_1 + f_2v_2 \\cdot w_2n_1 \\\\  1 \\cdot b_2 + f_1v_1 \\cdot w_1n_2 + f_2v_1 \\cdot w_2n_2 &  1 \\cdot b_2 + f_1v_2 \\cdot w_1n_2 + f_2v_2 \\cdot w_2n_2 \\end{bmatrix})$$ Note that you already calculate $Z^{(l)}$ in the forward pass. In total, it looks like: $$\\begin{bmatrix} 0.6 & 0.4 & 0.03 \\\\ 0.8  & -0.3 & 0.54 \\end{bmatrix} \\cdot \\begin{bmatrix}  w_1n_1^{(L)} & w_2n_1^{(L)} \\\\ w_1n_2^{(L)} & w_2n_2^{(L)}  \\\\ w_1n_3^{(L)} & w_2n_3^{(L)}\\end{bmatrix} \\odot \\begin{bmatrix}  \\sigma'(1 \\cdot b_1 + f_{1}v_{1} \\cdot w_1n_1 + f_2v_1 \\cdot w_2n_1) & \\sigma'(1 \\cdot b_1 + f_1v_2 \\cdot w_1n_1 + f_2v_2 \\cdot w_2n_1) \\\\  \\sigma'(1 \\cdot b_2 + f_1v_1 \\cdot w_1n_2 + f_2v_1 \\cdot w_2n_2) &  \\sigma'(1 \\cdot b_2 + f_1v_2 \\cdot w_1n_2 + f_2v_2 \\cdot w_2n_2) \\end{bmatrix}$$ This'll give you $\\delta^{(l)}$, which looks like: <br> $$\\begin{bmatrix} \\delta_1^{(l)} & \\delta_2^{(l)} \\\\ \\delta_1^{(l)} & \\delta_2^{(l)} \\end{bmatrix}$$\n",
    "\n",
    "3. **Partial derivatives w.r.t. weights and biases Hidden Layer**: now you can do the exact same thing as under point 1. above. The only difference is that the 'activations' of the previous layer are now just the input values. Once this step is done, you have all the gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab9f9c",
   "metadata": {},
   "source": [
    "## Finally: the actual implementation, the real deal, the pi√®ce de resistance:\n",
    "\n",
    "It's now up to you to implement backpropagation as described. This might seem daunting, but it's not as hard as it might feel. Take the following steps:\n",
    "* Make a copy of `nnCostFunction`. Call it `nnGradientFunction`. It keeps the same arguments and does exactly the same to begin with, but it will return the gradients calculated by backpropagation rather than stop at calculating the cost function. First set it to return `None`. \n",
    "* Calculate $\\delta^{(L)}$ using the real labels. Remember, this is just $A^{(L)} - y$.\n",
    "* Calculate the gradient matrix $\\Delta^{(L)}$ by following step 1. above. Don't forget to multiply with 1 over m!\n",
    "* Propagate the error back to calculate $\\delta^{(l)}$. Follow step 2. above. Use `mySigmoidGradient()` and know that $\\odot$ simply corresponds to * in numpy.\n",
    "* Calculate $\\Delta^{(l)}$ by doing the same thing as for $\\Delta^{(L)}$, but using the input values `np.c_[np.ones(shape = (len(X), 1)), X]` as the 'activations' of the previous layer.\n",
    "* Make the function return a flat array of the gradients so we can use it with advanced optimisers like `fmin_bfg()`. To do that, use `np.ravel()` on both matrices of gradients, and then append them to each other using `np.append(ravelledThetaOneGradientMatrix, ravelledThetaTwoGradientMatrix)`. This array should have an equal amount of entries as the nnThetas that the function takes in!\n",
    "\n",
    "Hints:\n",
    "* If you get stuck, be sure to print the .shape attribute of the matrices you are working with.\n",
    "* Don't forget to add a column of ones to the activations of the previous layer to be multiplied with the $\\delta^{(L)}$ or $\\delta^{(l)}$ matrix: this results in the gradient of the bias term, which is just $\\delta^{(L)}_j$\n",
    "* If things seem hopeless, ask a fellow sufferer (student) for help, clamour for attention from one of the teachers, or, if all else fails and the nuclear option is required, look at the answers. If you've been trying for 20 minutes and are just stuck, that's exactly the type of situation the answers are there for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8fbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations output layer shape: (20004, 10)\n",
      "Activations for one training example: [0.50679905 0.47026724 0.48045209 0.45695882 0.50439269 0.42537011\n",
      " 0.45660324 0.55061342 0.58567024 0.48573213]\n",
      "delta^(3): [[ 0.50679905  0.47026724  0.48045209 ...  0.55061342  0.58567024\n",
      "  -0.51426787]\n",
      " [ 0.51597866  0.47184661  0.47270976 ...  0.57208504  0.58398438\n",
      "  -0.52566946]\n",
      " [ 0.50011956  0.45834984  0.47915344 ...  0.55698424  0.59916884\n",
      "  -0.51727237]\n",
      " ...\n",
      " [-0.46315771  0.47886494  0.45896251 ...  0.56575545  0.59230702\n",
      "   0.48211795]\n",
      " [-0.47937306  0.46951334  0.45806689 ...  0.54667469  0.58760281\n",
      "   0.47131704]\n",
      " [-0.48248302  0.47086103  0.45044764 ...  0.56747375  0.60631971\n",
      "   0.48869242]]\n",
      "delta^(3) shape: (20004, 10)\n",
      "\n",
      "z^(2): [[ 1.47211529  0.72742786 -0.61089524 ...  0.31187328 -0.9862839\n",
      "  -0.24400619]\n",
      " [ 1.97432473  1.22491872 -1.0079633  ...  0.44606513 -0.53983498\n",
      "   0.30598892]\n",
      " [ 1.79004393  0.16434479 -0.54288865 ...  1.06426415 -0.48233109\n",
      "  -0.21282825]\n",
      " ...\n",
      " [ 0.38756623  1.63036701 -1.15184267 ...  0.65412715 -0.47791407\n",
      "   0.10986511]\n",
      " [ 1.15106798  0.52755491  0.52837007 ...  0.2576143  -1.19924585\n",
      "  -0.60583446]\n",
      " [ 0.02406093  1.05307915 -0.10329358 ...  0.55975394 -0.12839311\n",
      "   0.24578842]]\n",
      "z^(2) shape: (20004, 25)\n",
      "\n",
      "Sigmoid gradient: [[0.1517938  0.21964022 0.22805306 ... 0.24401815 0.19785477 0.24631543]\n",
      " [0.10705939 0.17551031 0.1958873  ... 0.23796519 0.23263561 0.24423829]\n",
      " [0.12259907 0.24831949 0.23244795 ... 0.19070552 0.23600554 0.24719024]\n",
      " ...\n",
      " [0.24084215 0.13695617 0.18247937 ... 0.22505499 0.23625122 0.24924712]\n",
      " [0.18255283 0.23338153 0.23333245 ... 0.24589763 0.17796649 0.22839394]\n",
      " [0.24996382 0.19174258 0.24933434 ... 0.23139624 0.24897252 0.24626195]]\n",
      "Sigmoid gradient shape: (20004, 25)\n",
      "delta^(2): [[ 0.00559656  0.00756336 -0.01908792 ... -0.00396473  0.00714478\n",
      "   0.0120355 ]\n",
      " [ 0.00384219  0.00622899 -0.01671616 ... -0.00340885  0.00843743\n",
      "   0.01246619]\n",
      " [ 0.00455319  0.00829677 -0.01899492 ... -0.00289673  0.00926132\n",
      "   0.01231473]\n",
      " ...\n",
      " [-0.02873665 -0.00857492  0.00874672 ... -0.02454918  0.03492032\n",
      "   0.00739522]\n",
      " [-0.02147605 -0.01504532  0.01125369 ... -0.02647504  0.02629842\n",
      "   0.00626164]\n",
      " [-0.03021329 -0.0124812   0.01267393 ... -0.02542813  0.03782626\n",
      "   0.00759053]]\n",
      "size of delta^(2): (20004, 25)\n",
      "Summed gradients output layer: [[8172.97852635 5757.79638179 4640.51198969 3341.42512387 4567.29336633\n",
      "  4479.21467231 3684.07235729 3448.50762581 3040.55409447 5033.903715\n",
      "  3709.16273132 4368.31735499 4976.71319298 3022.80830847 4033.07529943\n",
      "  2649.20165895 3625.97001059 2620.89231734 5220.26671242 6074.22402993\n",
      "  3105.80858851 5553.84323038 5147.90017464 4101.07375535 3572.61557587\n",
      "  3972.61851466]\n",
      " [7062.30840141 5111.91323973 4391.06417086 2929.06938257 4312.62361704\n",
      "  4240.76477348 3186.16081289 2736.01296019 2827.20868369 4452.01240131\n",
      "  3222.58750427 4189.81997407 4473.34344794 2702.01250823 3978.32459885\n",
      "  1987.5377871  2980.1454298  1891.95566902 4424.14092168 5273.68967509\n",
      "  2641.56205311 4989.18883576 4867.46725401 3654.64534375 2902.41771834\n",
      "  3446.66297374]\n",
      " [7449.97522221 5217.70125991 4350.01368958 3122.8779238  4455.86429943\n",
      "  4498.5445377  3413.92191672 3032.68491175 2878.13444284 4611.52241437\n",
      "  3453.12784608 3951.12418366 4598.56331969 2807.82662491 3812.45748263\n",
      "  2259.20408051 3246.93243038 2391.91276782 4736.50300636 5452.34997083\n",
      "  2869.66177743 4931.09621904 4721.46736794 3845.19365761 3158.75205994\n",
      "  3611.52762818]\n",
      " [7049.70178442 4861.85573075 4076.00142299 2891.98786341 3926.23976839\n",
      "  4256.96372886 3139.97996311 2767.50401645 2821.49435022 4187.4900807\n",
      "  3214.87326413 3846.04595156 4333.14811617 2601.05292829 3579.54369724\n",
      "  2114.51879283 3035.63274442 2307.24909502 4454.74285988 5190.41373276\n",
      "  2832.76519996 4544.01022894 4500.23878673 3701.75268843 3107.05906674\n",
      "  3439.17105991]\n",
      " [8067.43023893 5590.93798157 4676.83847402 3300.33123235 4405.59153209\n",
      "  4601.78887345 3654.07239163 3174.60361769 3250.37518902 5004.80932447\n",
      "  3912.05269696 4459.76562203 5167.60989525 2922.06727768 3895.76900732\n",
      "  2398.87183013 3529.61723345 2331.55110412 5253.642347   5791.87050142\n",
      "  3201.45419268 5416.8805345  5011.73936364 4095.7583994  3389.112065\n",
      "  3848.89849244]\n",
      " [6863.48144505 4790.3477862  4007.60460413 2764.77707665 3786.21924873\n",
      "  3941.26452342 2954.35178039 2697.74092121 2703.39288847 4179.98684039\n",
      "  3135.6568932  3810.01533016 4272.98783807 2542.49185282 3421.15207793\n",
      "  2051.94449094 3005.657971   2117.3770209  4366.2179268  4926.05531812\n",
      "  2681.91005291 4653.09811436 4357.44506919 3581.91723142 2980.17477876\n",
      "  3297.04718533]\n",
      " [7322.4472575  5060.49475251 4332.92051627 2998.27922435 4146.32858367\n",
      "  4157.46800685 3216.13742103 3040.8448748  2879.64652351 4590.38974353\n",
      "  3318.37587007 3890.00222855 4607.54010821 2930.49207923 3479.53242855\n",
      "  2204.53123933 3324.63099199 2203.43417605 4650.01166436 5272.84631479\n",
      "  2704.52675741 4866.32414752 4542.88748983 3749.38047579 3047.1413054\n",
      "  3666.2267773 ]\n",
      " [9000.37449629 6329.43082313 5060.41549097 3898.71020571 4916.03057399\n",
      "  5235.17813995 4242.07528572 3506.20793336 3444.80032798 5668.97484972\n",
      "  4191.71926619 5090.33031801 5797.1503616  3086.6423017  4515.8886869\n",
      "  2820.43860177 4030.10376862 2529.51239477 5831.27296344 6769.44611001\n",
      "  3428.90925988 6054.57104872 5686.6326741  4552.6345067  3888.85149363\n",
      "  4487.30890722]\n",
      " [9857.68306534 6813.82510419 5728.76864004 4155.97433128 5648.24434512\n",
      "  5788.42318265 4395.5283398  3895.56459279 3884.18477165 5988.94958807\n",
      "  4674.10648551 5540.71526383 6132.87342172 3668.29513757 5183.14773047\n",
      "  3034.28938066 4218.0608434  2999.13412216 6286.73693652 6990.00032287\n",
      "  3908.80667123 6551.87666221 6332.10849768 5146.01759872 4375.85111848\n",
      "  4791.85705435]\n",
      " [7617.68039603 5253.33000353 4369.14522299 3276.8622789  4156.47372492\n",
      "  4340.2457728  3485.31265066 2926.95693323 2955.46925666 4694.66921327\n",
      "  3645.85862221 4282.69785648 4830.91512389 2739.36787253 3764.19763487\n",
      "  2282.40546052 3414.23530864 2116.69593949 4885.34904242 5460.24436876\n",
      "  2999.22563123 5029.90790568 4765.58292174 3898.40974795 3249.96268771\n",
      "  3746.49428785]]\n",
      "Summed gradients output layer shape: (10, 26)\n",
      "Summed gradients hidden layer: [[-221.88941345    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [ 130.18579792    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [  33.42637502    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " ...\n",
      " [-487.67420661    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [ 140.56213249    0.            0.         ...    0.\n",
      "     0.            0.        ]\n",
      " [  38.66465763    0.            0.         ...    0.\n",
      "     0.            0.        ]]\n",
      "Summed gradients hidden layer shape: (25, 785)\n",
      "Final gradients output layer: [[0.40856721 0.28783225 0.2319792  0.16703785 0.228319   0.22391595\n",
      "  0.18416678 0.1723909  0.15199731 0.25164486 0.18542105 0.21837219\n",
      "  0.2487859  0.15111019 0.20161344 0.1324336  0.18126225 0.13101841\n",
      "  0.26096114 0.30365047 0.15525938 0.27763663 0.25734354 0.20501269\n",
      "  0.17859506 0.19859121]\n",
      " [0.35304481 0.25554455 0.21950931 0.14642418 0.21558806 0.21199584\n",
      "  0.15927619 0.13677329 0.14133217 0.22255611 0.16109716 0.20944911\n",
      "  0.22362245 0.13507361 0.19887645 0.09935702 0.14897748 0.09457887\n",
      "  0.22116281 0.26363176 0.13205169 0.24940956 0.2433247  0.18269573\n",
      "  0.14509187 0.17229869]\n",
      " [0.37242428 0.2608329  0.21745719 0.15611267 0.22274867 0.22488225\n",
      "  0.17066196 0.15160392 0.14387795 0.23053001 0.17262187 0.19751671\n",
      "  0.22988219 0.14036326 0.19058476 0.11293762 0.16231416 0.11957172\n",
      "  0.23677779 0.27256299 0.1434544  0.24650551 0.23602616 0.19222124\n",
      "  0.15790602 0.18054027]\n",
      " [0.35241461 0.24304418 0.20375932 0.14457048 0.19627273 0.21280563\n",
      "  0.1569676  0.13834753 0.14104651 0.20933264 0.16071152 0.19226384\n",
      "  0.21661408 0.13002664 0.1789414  0.1057048  0.15175129 0.11533939\n",
      "  0.2226926  0.25946879 0.14160994 0.22715508 0.22496695 0.18505062\n",
      "  0.15532189 0.17192417]\n",
      " [0.40329085 0.279491   0.23379516 0.16498356 0.22023553 0.23004343\n",
      "  0.18266709 0.15869844 0.16248626 0.25019043 0.19556352 0.22294369\n",
      "  0.25832883 0.14607415 0.1947495  0.11991961 0.17644557 0.11655424\n",
      "  0.26262959 0.28953562 0.1600407  0.27078987 0.25053686 0.20474697\n",
      "  0.16942172 0.19240644]\n",
      " [0.34310545 0.2394695  0.20034016 0.13821121 0.18927311 0.19702382\n",
      "  0.14768805 0.13486007 0.13514262 0.20895755 0.15675149 0.19046267\n",
      "  0.21360667 0.12709917 0.1710234  0.10257671 0.15025285 0.10584768\n",
      "  0.21826724 0.24625352 0.13406869 0.23260838 0.21782869 0.17906005\n",
      "  0.14897894 0.1648194 ]\n",
      " [0.36604915 0.25297414 0.21660271 0.14988398 0.20727497 0.20783183\n",
      "  0.16077472 0.15201184 0.14395354 0.22947359 0.16588562 0.19446122\n",
      "  0.23033094 0.1464953  0.17394183 0.11020452 0.16619831 0.11014968\n",
      "  0.23245409 0.2635896  0.1351993  0.24326755 0.22709895 0.18743154\n",
      "  0.1523266  0.18327468]\n",
      " [0.44992874 0.31640826 0.25297018 0.19489653 0.24575238 0.26170657\n",
      "  0.21206135 0.17527534 0.17220558 0.28339206 0.20954405 0.25446562\n",
      "  0.28979956 0.15430125 0.22574928 0.14099373 0.2014649  0.12645033\n",
      "  0.29150535 0.33840462 0.17141118 0.30266802 0.28427478 0.22758621\n",
      "  0.19440369 0.22432058]\n",
      " [0.4927856  0.34062313 0.28638116 0.20775717 0.28235575 0.28936329\n",
      "  0.21973247 0.19473928 0.1941704  0.2993876  0.23365859 0.27698037\n",
      "  0.30658235 0.18337808 0.25910557 0.15168413 0.21086087 0.14992672\n",
      "  0.31427399 0.34943013 0.19540125 0.32752833 0.31654212 0.25724943\n",
      "  0.21874881 0.23954494]\n",
      " [0.38080786 0.26261398 0.21841358 0.16381035 0.20778213 0.21696889\n",
      "  0.17423079 0.14631858 0.14774391 0.23468652 0.18225648 0.21409207\n",
      "  0.24149746 0.13694101 0.18817225 0.11409745 0.17067763 0.10581363\n",
      "  0.24421861 0.27295763 0.1499313  0.25144511 0.2382315  0.19488151\n",
      "  0.16246564 0.18728726]]\n",
      "Summed gradients output layer shape: (10, 26)\n",
      "Final gradients hidden layer: [[-0.01109225  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.00650799  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.00167098  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.02437883  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.0070267   0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.00193285  0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Final gradients hidden layer shape: (25, 785)\n",
      "(19885,)\n",
      "(19885,)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(42)\n",
    "thetaOneMatrix = rng.uniform(-0.12, 0.12, size = (25, 785))\n",
    "thetaTwoMatrix = rng.uniform(-0.12, 0.12, size = (10, 26))\n",
    "\n",
    "savedData = np.load(\"dataMNISTNeuralNetwork.npz\")\n",
    "X_train, X_test, y_train, y_test = savedData[\"XTrain\"], savedData[\"XTest\"], savedData[\"yTrain\"], savedData[\"yTest\"]\n",
    "\n",
    "\n",
    "nnThetas       = np.append(np.ravel(thetaOneMatrix), np.ravel(thetaTwoMatrix))\n",
    "inputLayerSize = 784\n",
    "hiddenLayerSize = 25\n",
    "classLabels = 10\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "#Up to you! Copy the function from above and go ahead.\n",
    "\n",
    "# answer\n",
    "\n",
    "def nnGradientFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10):\n",
    "   \n",
    "    m = len(X)\n",
    "    nnThetas = np.array(nnThetas)\n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #calculating the forward pass\n",
    "    inputs        = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    weightedSumHL = inputs @ thetaOneMatrix.T\n",
    "    activationsHL  = mySigmoid(weightedSumHL)\n",
    "    \n",
    "    inputsOL      = np.c_[np.ones(shape = (len(activationsHL), 1)), activationsHL]\n",
    "    weightedSumOL = inputsOL @ thetaTwoMatrix.T\n",
    "    activationsOL = mySigmoid(weightedSumOL)\n",
    "    \n",
    "    print(\"Activations output layer shape: \" + str(activationsOL.shape))\n",
    "    print(\"Activations for one training example: \" + str(activationsOL[0,:]))\n",
    "    \n",
    "    #cost\n",
    "    J = 1/m * np.sum((- (y * np.log(activationsOL)) - ((1-y) * np.log(1-activationsOL))))\n",
    "    \n",
    "    #regularised cost\n",
    "    #remember: units in the rows, their parameters in the columns\n",
    "    #Hence, [:,1:] removes the columns with the bias term.\n",
    "    regThetaOne = np.sum(np.square(thetaOneMatrix[:,1:]))\n",
    "    regThetaTwo = np.sum(np.square(thetaTwoMatrix[:,1:]))\n",
    "    regCost     = J + (lambda_/(2*m)) * (regThetaOne + regThetaTwo)\n",
    "    \n",
    "    \n",
    "    #calculate error layer 3\n",
    "    smallDeltaThree = activationsOL - y\n",
    "    print(\"delta^(3): \" + str(smallDeltaThree))\n",
    "    print(\"delta^(3) shape: \" + str(smallDeltaThree.shape) + \"\\n\")\n",
    "    \n",
    "    #calculate the weighted sums that the HL generates that go into the activation function and then get sent to \n",
    "    #the output layer\n",
    "    #We already have it so can just set it equal to what it was above.\n",
    "    weightedSumsLayerTwo = zTwo = weightedSumHL #= np.c_[np.ones(shape = (len(X), 1)), X] @ thetaOneMatrix.T\n",
    "    print(\"z^(2): \" + str(zTwo))\n",
    "    print(\"z^(2) shape: \" + str(zTwo.shape) + \"\\n\")\n",
    "    \n",
    "    sigmoidGradientOfZTwo = mySigmoidGradient(zTwo)\n",
    "    print(\"Sigmoid gradient: \" + str(sigmoidGradientOfZTwo))\n",
    "    print(\"Sigmoid gradient shape: \" + str(sigmoidGradientOfZTwo.shape))\n",
    "\n",
    "    smallDeltaTwo   = (smallDeltaThree @ thetaTwoMatrix[:, 1:]) * sigmoidGradientOfZTwo\n",
    "\n",
    "    print(\"delta^(2): \" + str(smallDeltaTwo))\n",
    "    print(\"size of delta^(2): \" + str(smallDeltaTwo.shape)) \n",
    "    \n",
    "    \n",
    "    bigDeltaThree     = smallDeltaThree.T @ np.c_[np.ones(shape = len(activationsHL)),\n",
    "                                                 activationsHL]\n",
    "    print(\"Summed gradients output layer: \" + str(bigDeltaThree))\n",
    "    print(\"Summed gradients output layer shape: \" + str(bigDeltaThree.shape))\n",
    "    \n",
    "    bigDeltaTwo       = smallDeltaTwo.T   @ np.c_[np.ones(shape = len(X)),\n",
    "                                                 X]\n",
    "    print(\"Summed gradients hidden layer: \" + str(bigDeltaTwo))\n",
    "    print(\"Summed gradients hidden layer shape: \" + str(bigDeltaTwo.shape))\n",
    "    \n",
    "    #average values, we've now summed them over all training examples\n",
    "    bigDeltaTwo   = bigDeltaTwo   * 1/m\n",
    "    bigDeltaThree = bigDeltaThree * 1/m\n",
    "    print(\"Final gradients output layer: \" + str(bigDeltaThree))\n",
    "    print(\"Summed gradients output layer shape: \" + str(bigDeltaThree.shape))\n",
    "    print(\"Final gradients hidden layer: \" + str(bigDeltaTwo))\n",
    "    print(\"Final gradients hidden layer shape: \" + str(bigDeltaTwo.shape))\n",
    "    \n",
    "    finalGradients = np.append(np.ravel(bigDeltaTwo), np.ravel(bigDeltaThree))\n",
    "    \n",
    "    return finalGradients\n",
    "\n",
    "test = nnGradientFunction(nnThetas, X, y)\n",
    "print(np.array(test).shape)\n",
    "print(nnThetas.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a856ef",
   "metadata": {},
   "source": [
    "## Testing your calculations with numerical gradient computations\n",
    "\n",
    "Implementing backpropagation probably took entirely too long. That's not strange: pouring this idea correctly into the linear algebra mould when you've only just learned about it is entirely difficult. Hopefully you managed to make it through, with or without the answers. Now we're going to check that the gradients we've calculated are correct. For that, let's use the numerical gradient computation function defined earlier. The implementation we use below stops after the first _indexToStop_ (here 500) theta parameters (out of a whopping 19,885) because otherwise it would take _hours_. \n",
    "\n",
    "This is of course just an approximation, so the gradients will only be correct up to the first ~3 digits after the comma. Let's check that the rounded entries of the numerical approximation agree with those of the actual gradient computation via backpropagation. You can just run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4aa55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 0 out of 19885\n",
      "Parameter 50 out of 19885\n",
      "Parameter 100 out of 19885\n",
      "Parameter 150 out of 19885\n",
      "Parameter 200 out of 19885\n",
      "Parameter 250 out of 19885\n",
      "Parameter 300 out of 19885\n",
      "Parameter 350 out of 19885\n",
      "Parameter 400 out of 19885\n",
      "Parameter 450 out of 19885\n",
      "Parameter 500 out of 19885\n",
      "True\n",
      "-0.011092252221905383 ; -0.011092252201194697\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "-3.919940566926533e-08 ; -3.9195313661366527e-08\n",
      "-8.46707141603167e-07 ; -8.466649603633414e-07\n",
      "-8.46707141603167e-07 ; -8.466427559028489e-07\n",
      "-3.527946547471316e-08 ; -3.5282887722587475e-08\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "-1.5278791630039697e-07 ; -1.5277556997261854e-07\n",
      "-4.3569429062303334e-07 ; -4.3566927843130543e-07\n",
      "-8.994926549716445e-07 ; -8.994716083066123e-07\n",
      "-1.6263479868110614e-06 ; -1.6263168589603083e-06\n",
      "-2.9071208054869525e-06 ; -2.9070301721390024e-06\n",
      "-4.308929659586378e-06 ; -4.3087755585702325e-06\n",
      "-4.7799973600635916e-06 ; -4.779767692753012e-06\n",
      "-4.8321630083175075e-06 ; -4.831903765989409e-06\n",
      "-6.386483561660222e-06 ; -6.3861183008384614e-06\n",
      "-4.506885281403905e-06 ; -4.5066084197742384e-06\n",
      "-4.496524425392746e-06 ; -4.496376604379293e-06\n",
      "-4.217682716644299e-06 ; -4.217510785053946e-06\n",
      "-2.0626031785476308e-06 ; -2.0625146035513353e-06\n",
      "-1.3094558525728495e-06 ; -1.3093837125666141e-06\n",
      "-8.935142445995335e-07 ; -8.93454199513144e-07\n",
      "-5.139550478241451e-07 ; -5.13931119883182e-07\n",
      "-2.4632499093600825e-07 ; -2.4630519845914023e-07\n",
      "-1.9838335338824705e-07 ; -1.9837020914792447e-07\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "-1.1091152347760945e-07 ; -1.1090683926795464e-07\n",
      "-2.448806336937684e-08 ; -2.4487079031132453e-08\n",
      "-3.6570987895329883e-07 ; -3.6568970074313256e-07\n",
      "-3.4267710123400467e-06 ; -3.426618988555674e-06\n",
      "-5.887988200714177e-06 ; -5.887663689918554e-06\n",
      "-1.2418962450204165e-05 ; -1.2418288619642226e-05\n",
      "-2.2688733774540773e-05 ; -2.268747412159655e-05\n",
      "-3.502049182752783e-05 ; -3.5018663524510885e-05\n",
      "-5.170181028771577e-05 ; -5.169894734535774e-05\n",
      "-6.689566392278283e-05 ; -6.689184317565378e-05\n",
      "-8.489326138598972e-05 ; -8.488779901227872e-05\n",
      "-9.408124190837771e-05 ; -9.407416623474774e-05\n",
      "-8.748008291180604e-05 ; -8.74732242195364e-05\n",
      "-7.30652302253806e-05 ; -7.305954508041168e-05\n",
      "-5.123633558126271e-05 ; -5.1231840991761146e-05\n",
      "-3.2457081434730376e-05 ; -3.245378188410086e-05\n",
      "-1.834825324299932e-05 ; -1.8346186791973196e-05\n",
      "-1.0330024389220425e-05 ; -1.0328826682837189e-05\n",
      "-5.682169000722029e-06 ; -5.681446424432579e-06\n",
      "-1.932310721593034e-06 ; -1.932076720834175e-06\n",
      "-1.5476715277265315e-06 ; -1.5475665193775967e-06\n",
      "-8.18385352870834e-07 ; -8.183098643144149e-07\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "-6.856956045950496e-08 ; -6.856737400084967e-08\n",
      "-1.7487387726542597e-07 ; -1.7485124459426515e-07\n",
      "-4.3417183370663585e-06 ; -4.341527137796675e-06\n",
      "-1.0537826337690313e-05 ; -1.0537255512588217e-05\n",
      "-2.8701629543120696e-05 ; -2.8699980170188155e-05\n",
      "-6.350793576793925e-05 ; -6.350414416544936e-05\n",
      "-0.00011849377809261876 ; -0.00011848594549945801\n",
      "-0.00019071204378222668 ; -0.00019069773316005012\n",
      "-0.0002680997445477558 ; -0.0002680776667318696\n",
      "-0.0003579504520216665 ; -0.0003579188545543843\n",
      "-0.00045203775632475214 ; -0.00045199362475045746\n",
      "-0.0005011868629781745 ; -0.0005011308878977161\n",
      "-0.0005131403201539376 ; -0.0005130770963646114\n",
      "-0.0004972643641901352 ; -0.0004971990597013587\n",
      "-0.0004037302919350632 ; -0.0004036745515634266\n",
      "-0.00028303036679076457 ; -0.00028298742460464155\n",
      "-0.000185592065289418 ; -0.00018556325809271357\n",
      "-0.0001192365843097377 ; -0.00011921820419757978\n",
      "-7.272228761207837e-05 ; -7.271239166328769e-05\n",
      "-3.5086136669286324e-05 ; -3.508166646071231e-05\n",
      "-1.5411935187148775e-05 ; -1.5410162035323083e-05\n",
      "-5.989115284937587e-06 ; -5.988316509331071e-06\n",
      "-1.465652169256529e-06 ; -1.4654100155553351e-06\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "-2.0152359434935007e-08 ; -2.014832745089734e-08\n",
      "-5.076988682891449e-07 ; -5.076561393480006e-07\n",
      "-5.2332695414440535e-06 ; -5.232703159663288e-06\n",
      "-2.3954543765275467e-05 ; -2.3952289041062613e-05\n",
      "-5.3291647836233635e-05 ; -5.328588237318854e-05\n",
      "-0.00012924593818719322 ; -0.00012923158099198417\n",
      "-0.00026920907919301864 ; -0.0002691777689634023\n",
      "-0.00045833006996051753 ; -0.0004582711587985955\n",
      "-0.0007068110160522127 ; -0.0007067135809890601\n",
      "-0.001047122162195586 ; -0.0010469758882081237\n",
      "-0.00151199473976138 ; -0.0015117860918678616\n",
      "-0.0020268473110895196 ; -0.002026564196455638\n",
      "-0.00239311668270926 ; -0.002392767255443573\n",
      "-0.002541841532843721 ; -0.0025414440196769306\n",
      "-0.0024426905858289092 ; -0.002442274187330895\n",
      "-0.0021447947828224594 ; -0.002144413757676489\n",
      "-0.0016745084710965482 ; -0.0016741962172162062\n",
      "-0.0012413720202095239 ; -0.001241143636754316\n",
      "-0.0007968517209172212 ; -0.0007967091075755661\n",
      "-0.0004355241426199497 ; -0.00043544820638174997\n",
      "-0.00020309441746593342 ; -0.00020306095471767094\n",
      "-6.825788026402638e-05 ; -6.824758091994454e-05\n",
      "-2.027809983299035e-05 ; -2.027486178945992e-05\n",
      "-5.08329664847091e-06 ; -5.082618770302361e-06\n",
      "-1.4158225338377774e-06 ; -1.4156587013758326e-06\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "-3.5244842494925596e-07 ; -3.523670244476307e-07\n",
      "-1.9743936161320065e-06 ; -1.974167496143764e-06\n",
      "-1.693772266981659e-05 ; -1.6934111890520853e-05\n",
      "-6.62660941175722e-05 ; -6.625219928224624e-05\n",
      "-0.00018061588289916759 ; -0.00018058058159908796\n",
      "-0.0003765928326312079 ; -0.00037651082696044114\n",
      "-0.0006701301557452438 ; -0.000669971669253755\n",
      "-0.0010884901260803522 ; -0.001088213226196899\n",
      "-0.0016310521918729024 ; -0.0016306325800741206\n",
      "-0.0023875389083077656 ; -0.0023869405563203827\n",
      "-0.0032960238373656607 ; -0.0032952156914234365\n",
      "-0.004217268372477251 ; -0.004216243518762042\n",
      "-0.004946464695149963 ; -0.004945231695252517\n",
      "-0.00536292600289535 ; -0.0053615153383645975\n",
      "-0.005334654516350927 ; -0.005333162618192944\n",
      "-0.0048773444530737725 ; -0.004875916461699603\n",
      "-0.004047306658644216 ; -0.004046078925235008\n",
      "-0.003057156218506059 ; -0.0030562014030621754\n",
      "-0.002037615351766427 ; -0.0020369654896157385\n",
      "-0.0011801692977534393 ; -0.001179800919182128\n",
      "-0.0006011550614746496 ; -0.0006009947828289341\n",
      "-0.00025638804571901703 ; -0.0002563319512205453\n",
      "-8.630812228290708e-05 ; -8.629201353471672e-05\n",
      "-2.0937897022605165e-05 ; -2.0933530464617434e-05\n",
      "-4.283218688196262e-06 ; -4.282232346497494e-06\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "5.818697282451405e-08 ; 5.831335414541172e-08\n",
      "-7.506963306587085e-06 ; -7.505023269516187e-06\n",
      "-4.348729187992867e-05 ; -4.3468473265306784e-05\n",
      "-0.00013841575508175612 ; -0.0001383645997776739\n",
      "-0.0003449944617006763 ; -0.000344861663670315\n",
      "-0.0006779965052279729 ; -0.0006777090488796489\n",
      "-0.001132637474816462 ; -0.001132118483404554\n",
      "-0.0017921963982743363 ; -0.0017913663263868784\n",
      "-0.002639828819944761 ; -0.002638620868467001\n",
      "-0.003578486483306632 ; -0.003576864582299777\n",
      "-0.004502628661606217 ; -0.004500611976432367\n",
      "-0.005349578552845656 ; -0.005347212934658785\n",
      "-0.0060035054275927 ; -0.006000820134133278\n",
      "-0.006363643200266035 ; -0.006360663173943237\n",
      "-0.006394591427653034 ; -0.00639142681890803\n",
      "-0.00613033073423477 ; -0.006127200173899894\n",
      "-0.005476918846401138 ; -0.0054740623367521835\n",
      "-0.004402132304919174 ; -0.004399760085149751\n",
      "-0.003132766259107327 ; -0.003131000143596907\n",
      "-0.001972413165566041 ; -0.00197127395740182\n",
      "-0.0010756069786381798 ; -0.0010750363621525594\n",
      "-0.00046153240995208905 ; -0.000461328570899866\n",
      "-0.0001421986756038314 ; -0.00014214258658995504\n",
      "-2.700794616054111e-05 ; -2.6993793866836313e-05\n",
      "-6.205212576953233e-06 ; -6.201403834893426e-06\n",
      "-6.348401396771004e-07 ; -6.347899983438765e-07\n",
      "0.0 ; 0.0\n",
      "-2.405390946122114e-07 ; -2.404476617812179e-07\n",
      "1.220845201865012e-06 ; 1.2208722921513981e-06\n",
      "-8.756697244260633e-06 ; -8.750555835490559e-06\n",
      "-5.7931887229073967e-05 ; -5.788889811952913e-05\n",
      "-0.00019310597051799003 ; -0.00019297815345709068\n",
      "-0.00046136796041691106 ; -0.00046105181450428745\n",
      "-0.0009077810459102088 ; -0.0009071289674977834\n",
      "-0.0015329351513849545 ; -0.0015318112334128386\n",
      "-0.0023470714024755404 ; -0.002345343794729615\n",
      "-0.003245931628067885 ; -0.0032435397656627174\n",
      "-0.003988175312304236 ; -0.00398522403965984\n",
      "-0.0045887589555461015 ; -0.0045854037722392604\n",
      "-0.00507085549052222 ; -0.005067263173508252\n",
      "-0.0054210595959279274 ; -0.005417282298303405\n",
      "-0.005629446083898089 ; -0.005625433652234335\n",
      "-0.0056634812681604 ; -0.005659227610976814\n",
      "-0.005768655531584108 ; -0.005764226487237067\n",
      "-0.005562780720283096 ; -0.005558405082872753\n",
      "-0.004860702090571666 ; -0.004856712170209221\n",
      "-0.0036909507614325972 ; -0.0036877209597108163\n",
      "-0.002474576450364714 ; -0.002472320481672341\n",
      "-0.0014620021961087482 ; -0.0014607276854761153\n",
      "-0.0006516077815120866 ; -0.000651096061510259\n",
      "-0.0001767929936037039 ; -0.00017665223950302789\n",
      "-2.7220052397927726e-05 ; -2.7190152351863617e-05\n",
      "-4.921633709290906e-06 ; -4.914397777611157e-06\n",
      "-8.285076357916347e-08 ; -8.274714247136217e-08\n",
      "0.0 ; 0.0\n",
      "1.859081779929139e-07 ; 1.8615775587704775e-07\n",
      "4.889797427999625e-06 ; 4.890670091128868e-06\n",
      "-6.0355040741615065e-06 ; -6.0210192387444295e-06\n",
      "-6.836713053116895e-05 ; -6.829333987212749e-05\n",
      "-0.00021912636200957725 ; -0.00021891315427069458\n",
      "-0.000524540131944313 ; -0.0005240197653577638\n",
      "-0.0010376799420325838 ; -0.001036618946237411\n",
      "-0.00173740703636904 ; -0.0017356035808546721\n",
      "-0.0025665559624142393 ; -0.0025638553102780293\n",
      "-0.0033472990949593158 ; -0.003343776175945834\n",
      "-0.0038041505633672774 ; -0.0038001076996252436\n",
      "-0.0040928426617931465 ; -0.004088634009136172\n",
      "-0.0042827701725900065 ; -0.004278721754680248\n",
      "-0.004398183218312452 ; -0.00439426400156151\n",
      "-0.004485393551976626 ; -0.004481355713181756\n",
      "-0.0045833694926656066 ; -0.004578999814874862\n",
      "-0.004882454836172296 ; -0.00487760301925988\n",
      "-0.005048935762101401 ; -0.00504365005316032\n",
      "-0.004758285948706223 ; -0.004752994704482205\n",
      "-0.003867553401027924 ; -0.003862937325571636\n",
      "-0.0028020558816981823 ; -0.0027986089357057153\n",
      "-0.001733207330503673 ; -0.0017310952271643032\n",
      "-0.0008007525358606503 ; -0.0007998279150100984\n",
      "-0.00021954533096885055 ; -0.000219270606116595\n",
      "-2.7564005386172037e-05 ; -2.751025629521564e-05\n",
      "-4.140094189844697e-06 ; -4.1310777021408285e-06\n",
      "0.0 ; 0.0\n",
      "0.0 ; 0.0\n",
      "1.271008172887349e-06 ; 1.2714584940454188e-06\n",
      "8.505378871758652e-06 ; 8.506195747770562e-06\n",
      "1.6156691366687203e-06 ; 1.63419056065095e-06\n",
      "-5.88787906954532e-05 ; -5.8781579603817136e-05\n",
      "-0.0002193047378398444 ; -0.0002190321124473371\n",
      "-0.0005455779429615934 ; -0.0005448895290527389\n",
      "-0.0011174912043151097 ; -0.0011160786028696634\n",
      "-0.0018722185347623454 ; -0.0018697831105640716\n",
      "-0.0026872987967686227 ; -0.0026837530597845216\n",
      "-0.0033301580595770488 ; -0.003325679669430315\n",
      "-0.0035717702432152268 ; -0.0035669369946589313\n",
      "-0.003588160466293273 ; -0.00358360581209638\n",
      "-0.0035972531189899285 ; -0.0035932202058219787\n",
      "-0.003605167094417306 ; -0.003601512634787696\n",
      "-0.0037832143717940396 ; -0.00377940283513567\n",
      "-0.003988746619402627 ; -0.003984362568765221\n",
      "-0.004311953080009602 ; -0.004306734826542424\n",
      "-0.004588375731672004 ; -0.0045823542782486015\n",
      "-0.004504327156866229 ; -0.004497994683383411\n",
      "-0.003862307132935776 ; -0.0038565607729523776\n",
      "-0.002946241229769093 ; -0.0029418026770144934\n",
      "-0.0019101395540863852 ; -0.001907318205596198\n",
      "-0.0009103058929143128 ; -0.0009089860197875055\n",
      "-0.00025059442185789793 ; -0.00025020260530084215\n",
      "-3.57793902885541e-05 ; -3.570298279242934e-05\n",
      "-6.67113173401257e-06 ; -6.6584915359158e-06\n",
      "-2.935479185284407e-07 ; -2.9310776028523833e-07\n",
      "0.0 ; 0.0\n",
      "2.05832010592279e-06 ; 2.0595880556584234e-06\n",
      "9.309066417294543e-06 ; 9.31113852686849e-06\n",
      "4.155251590508899e-06 ; 4.174318668503929e-06\n",
      "-4.108962632501035e-05 ; -4.100391137740189e-05\n",
      "-0.00021358842186093615 ; -0.00021329118915502931\n",
      "-0.0006119080537575006 ; -0.000611048034393491\n",
      "-0.001259458676159698 ; -0.0012576330732372298\n",
      "-0.0020665873675520742 ; -0.0020634711361822156\n",
      "-0.002853819550783987 ; -0.002849369846735783\n",
      "-0.0033724185143371526 ; -0.003367001806253711\n",
      "-0.0034506826744965004 ; -0.0034451134656166005\n",
      "-0.003291966816307121 ; -0.0032870307631327478\n",
      "-0.0031843724458173093 ; -0.003180192340579424\n",
      "-0.003391331728463505 ; -0.003387477711669362\n",
      "-0.0037148706853745443 ; -0.0037106708106193764\n",
      "-0.003870014492417582 ; -0.003864959485788688\n",
      "-0.004114503128087988 ; -0.004108385760481781\n",
      "-0.004256092800527574 ; -0.004249140586054523\n",
      "-0.00413201238802289 ; -0.004124890060630548\n",
      "-0.0036340968551109853 ; -0.0036276832737769382\n",
      "-0.002849699990903363 ; -0.0028446962385686447\n",
      "-0.0019121195600906495 ; -0.0019088500646802231\n",
      "-0.0009473655055008189 ; -0.0009457945138180435\n",
      "-0.0002745022516675822 ; -0.0002740253801647441\n",
      "-3.933658004830469e-05 ; -3.924605973537609e-05\n",
      "-5.392821399689278e-06 ; -5.380891288098155e-06\n",
      "-1.0966215202511952e-06 ; -1.0947998063670639e-06\n",
      "1.1446355895558705e-07 ; 1.1457501614131615e-07\n",
      "7.703987818507063e-07 ; 7.718181649352118e-07\n",
      "6.6375157705414495e-06 ; 6.639795380181113e-06\n",
      "3.15592238006691e-06 ; 3.1719737947355497e-06\n",
      "-4.367429708634389e-05 ; -4.3587977671677436e-05\n",
      "-0.0002579580062540526 ; -0.0002576014468402832\n",
      "-0.0007502529262949338 ; -0.0007491317655095031\n",
      "-0.0015140494722408742 ; -0.0015116468521370052\n",
      "-0.002339454526236076 ; -0.0023354303690936717\n",
      "-0.003094386653146925 ; -0.003088794753303148\n",
      "-0.0034639405840322384 ; -0.0034574128093822765\n",
      "-0.00343477299910593 ; -0.003428257331350437\n",
      "-0.0031629818992452556 ; -0.0031573610481672176\n",
      "-0.003186192209816499 ; -0.0031813187106877194\n",
      "-0.0036911017628380454 ; -0.003686276150993706\n",
      "-0.004117660436009452 ; -0.004112139055933994\n",
      "-0.00417859867104497 ; -0.004171971133004604\n",
      "-0.004120885969487585 ; -0.0041133794415060265\n",
      "-0.00396842165902587 ; -0.003960605869934852\n",
      "-0.0036794338084874976 ; -0.003671891812828676\n",
      "-0.003215229087096511 ; -0.0032086269063924533\n",
      "-0.00258714178543488 ; -0.002581959419600821\n",
      "-0.0018164740539753893 ; -0.00181302716839582\n",
      "-0.000993025126304755 ; -0.0009912544474133256\n",
      "-0.00030207118109275424 ; -0.0003015250893412258\n",
      "-2.73831055611532e-05 ; -2.7326185758624888e-05\n",
      "-4.220286871061911e-06 ; -4.212372672895981e-06\n",
      "-4.798834062773101e-07 ; -4.792166663492026e-07\n",
      "1.2438373305988083e-07 ; 1.2453149622615456e-07\n",
      "8.545731390491002e-07 ; 8.54760706658908e-07\n",
      "3.4968686963994187e-06 ; 3.4975800033976157e-06\n",
      "-2.1230213003153834e-06 ; -2.106705920823515e-06\n",
      "-5.3054324833915385e-05 ; -5.2967683572546775e-05\n",
      "-0.00033948554444132226 ; -0.00033897875617583395\n",
      "-0.0009603220428290992 ; -0.0009587997951143734\n",
      "-0.0017877371913870882 ; -0.001784550907046878\n",
      "-0.002612536399429224 ; -0.0026073942605009393\n",
      "-0.0032544792710854694 ; -0.003247664470684697\n",
      "-0.003492840278745154 ; -0.0034852138375285335\n",
      "-0.0033674895067121974 ; -0.003360014639852693\n",
      "-0.0032169776175438977 ; -0.003210212717874583\n",
      "-0.0036336797472235898 ; -0.0036272624592825764\n",
      "-0.0044488843744484394 ; -0.004442066914300824\n",
      "-0.004831133965726205 ; -0.004823244883489508\n",
      "-0.004625406490223343 ; -0.0046164967626438624\n",
      "-0.004133067453410976 ; -0.004124020871465461\n",
      "-0.0036429190607199053 ; -0.003634463361379403\n",
      "-0.003169130178450865 ; -0.0031616197659900536\n",
      "-0.0027403615947857404 ; -0.00273387243954204\n",
      "-0.0022917408941204825 ; -0.0022865683613204624\n",
      "-0.0017250254363406386 ; -0.0017214884984184664\n",
      "-0.0010317992401333302 ; -0.0010298722008528216\n",
      "-0.00035174201100640515 ; -0.00035108204787803743\n",
      "-2.2234944608191753e-05 ; -2.2192550019894952e-05\n",
      "-5.636691380595011e-06 ; -5.626730192886953e-06\n",
      "-1.1510673241388257e-06 ; -1.1482148565278294e-06\n",
      "0.0 ; 0.0\n",
      "2.4122332986599123e-07 ; 2.41140440948584e-07\n",
      "2.0387307117327248e-06 ; 2.0378720932967553e-06\n",
      "-4.419004632393492e-06 ; -4.401927711228382e-06\n",
      "-7.906907593610537e-05 ; -7.895222697840154e-05\n",
      "-0.0004928523933139623 ; -0.0004920552632015074\n",
      "-0.0012521926995320213 ; -0.0012499739909799246\n",
      "-0.002135308301298911 ; -0.0021310096043336557\n",
      "-0.002871593950919527 ; -0.002865171926380583\n",
      "-0.003317403115586376 ; -0.003309477016877338\n",
      "-0.0034019550837871664 ; -0.0033934603660412677\n",
      "-0.0032879442361163215 ; -0.003279453579807523\n",
      "-0.0034880326524677477 ; -0.0034795979120261222\n",
      "-0.004553438438109907 ; -0.0045445261864784925\n",
      "-0.0054213300471591215 ; -0.005411591668114113\n",
      "-0.005551585292912172 ; -0.005540836007789096\n",
      "-0.004787744481157732 ; -0.0047767174526924805\n",
      "-0.003952175553723549 ; -0.003942032731352185\n",
      "-0.003287176394610258 ; -0.0032784905812377474\n",
      "-0.002774839744104474 ; -0.002767374178702653\n",
      "-0.0024555890106053527 ; -0.0024490157146672686\n",
      "-0.002183785484780373 ; -0.002178386200313298\n",
      "-0.0017115544278008964 ; -0.001707783257032247\n",
      "-0.0010725593622443544 ; -0.0010704416153473062\n",
      "-0.0003989884325774416 ; -0.000398175878935092\n",
      "-3.0364898504132044e-05 ; -3.0303715092827588e-05\n",
      "-5.49108331456788e-06 ; -5.4792437254036486e-06\n",
      "-4.0164968597056515e-07 ; -4.0039083160081645e-07\n",
      "9.058002670570325e-08 ; 9.046541293855626e-08\n",
      "0.0 ; 0.0\n",
      "7.202929577099946e-07 ; 7.196998552672085e-07\n",
      "-8.42896602644035e-06 ; -8.406901841340186e-06\n",
      "-0.00014411414057820759 ; -0.0001438946073406555\n",
      "-0.0007350177333183374 ; -0.0007337191965106626\n",
      "-0.001622049009311327 ; -0.0016187872953210558\n",
      "-0.0025112514408478824 ; -0.0025055109809102305\n",
      "-0.00308531043264574 ; -0.0030774569514946393\n",
      "-0.003316323573588302 ; -0.003307311988720585\n",
      "-0.003300683965698617 ; -0.003291270687100223\n",
      "-0.0033979462632349956 ; -0.0033881367400567797\n",
      "-0.004141306293665709 ; -0.004130545261382679\n",
      "-0.005682355790385873 ; -0.0056702648443263115\n",
      "-0.006373636336382484 ; -0.006360609599020961\n",
      "-0.0060246658546927135 ; -0.006011276827422307\n",
      "-0.0046451065626271675 ; -0.004632684897920569\n",
      "-0.0036372454895210013 ; -0.0036266666558759653\n",
      "-0.0030474725477955197 ; -0.0030383619886720226\n",
      "-0.0027180955905138056 ; -0.002709944832979261\n",
      "-0.002492512511019141 ; -0.0024851529278890894\n",
      "-0.002236965466515891 ; -0.002230934592617473\n",
      "-0.001740242568155425 ; -0.001736113390826688\n",
      "-0.0010869222005637888 ; -0.0010846417630716587\n",
      "-0.00041608250840705465 ; -0.00041515326731911273\n",
      "-3.6486644398440074e-05 ; -3.6413476678376355e-05\n",
      "-6.8674186705098264e-06 ; -6.8586514245794206e-06\n",
      "-1.3289653988263403e-07 ; -1.325561882481452e-07\n",
      "2.965894784867921e-08 ; 2.9611868512802175e-08\n",
      "0.0 ; 0.0\n",
      "-1.3587035645844642e-06 ; -1.3571899160069734e-06\n",
      "-1.9295864527838672e-05 ; -1.9254295935411392e-05\n",
      "-0.00022805699744346569 ; -0.00022766273755081556\n",
      "-0.0010380251863554106 ; -0.0010359379931657031\n",
      "-0.0020044793210248265 ; -0.001999858638690455\n",
      "-0.002839315483975014 ; -0.0028319388212239005\n",
      "-0.0032818973518585863 ; -0.003272490762817881\n",
      "-0.003391658306018993 ; -0.0033813122346515456\n",
      "-0.003411350227620961 ; -0.0034006638838235403\n",
      "-0.003834446932967267 ; -0.003822838707812082\n",
      "-0.0050253111867531505 ; -0.005011976709212718\n",
      "-0.006469305064063525 ; -0.006454489853702228\n",
      "-0.006861166878844014 ; -0.0068457032131874485\n",
      "-0.0059252544353282175 ; -0.005910508162898509\n",
      "-0.004291205041147293 ; -0.004278374885480218\n",
      "-0.0034016310099878972 ; -0.003390521925439316\n",
      "-0.0030186674250100437 ; -0.0030085089708364876\n",
      "-0.0028816782930598892 ; -0.0028720430123030383\n",
      "-0.0026872299602308898 ; -0.002678667905264831\n",
      "-0.0023230557976507645 ; -0.0023162488105299417\n",
      "-0.001746816818889221 ; -0.0017423099674118703\n",
      "-0.001039012821632841 ; -0.001036632246709246\n",
      "-0.0004061480103656602 ; -0.00040518751465157266\n",
      "-5.1916903132522024e-05 ; -5.180943674076843e-05\n",
      "-1.0671050816940489e-05 ; -1.0656235893691246e-05\n",
      "-1.6834337514472846e-06 ; -1.6820367321201957e-06\n",
      "0.0 ; 0.0\n",
      "-2.1998685286621127e-07 ; -2.1968205032862897e-07\n",
      "-2.911802011439973e-06 ; -2.9047342309240776e-06\n",
      "-2.7034886742641137e-05 ; -2.696560752468713e-05\n",
      "-0.0003287323337162692 ; -0.00032810297145857703\n",
      "-0.0013073114047708744 ; -0.001304297270188215\n",
      "-0.0022912401260911826 ; -0.002285265789936375\n",
      "-0.0030575762544605826 ; -0.0030486376978444696\n",
      "-0.0034087660564114048 ; -0.0033979242530790543\n",
      "-0.0035030917728721745 ; -0.003491495448315618\n",
      "-0.003710813432880024 ; -0.0036987332263649364\n",
      "-0.004355404146716217 ; -0.004342209241059436\n",
      "-0.005480558892212881 ; -0.005465837151774622\n",
      "-0.006450241163288344 ; -0.006434455781523241\n",
      "-0.006391192425591306 ; -0.006375387084922579\n",
      "-0.005237300066213074 ; -0.005222522352354986\n",
      "-0.003959701508915775 ; -0.003946619626660208\n",
      "-0.0033472009332052703 ; -0.0033352092154714796\n",
      "-0.0032133807382162313 ; -0.0032016117890165674\n",
      "-0.003119889153381985 ; -0.003108815258556774\n",
      "-0.0028343131314452997 ; -0.0028247493988331485\n",
      "-0.002319389904448896 ; -0.0023121484238330936\n",
      "-0.0016545000244055797 ; -0.0016498340604798045\n",
      "-0.0009460170697727739 ; -0.000943568152500518\n",
      "-0.0003791405294685328 ; -0.00037815990783229836\n",
      "-7.361883846875238e-05 ; -7.344786112639667e-05\n",
      "-1.4879774722385601e-05 ; -1.4856293972798085e-05\n",
      "-2.072786348533597e-06 ; -2.0698420755138613e-06\n",
      "0.0 ; 0.0\n",
      "-3.765780156112736e-07 ; -3.75734998669941e-07\n",
      "-4.361117066317724e-06 ; -4.350284577014918e-06\n",
      "-5.1591152465635685e-05 ; -5.146193249316866e-05\n",
      "-0.0004641824795135548 ; -0.00046316303237148304\n",
      "-0.0014854546106761236 ; -0.0014815921911548458\n",
      "-0.0024187303897737233 ; -0.0024116788122086064\n",
      "-0.0031385047344256733 ; -0.003128418137876565\n",
      "-0.003465011556342275 ; -0.0034530938952670454\n",
      "-0.003613852390383831 ; -0.003601265894381811\n",
      "-0.003914187115987872 ; -0.0039011758845575173\n",
      "-0.004526581213071541 ; -0.004512871929485129\n",
      "-0.005246337213889661 ; -0.005231901054614241\n",
      "-0.005702998393410225 ; -0.00568815609724993\n",
      "-0.0053974181237312095 ; -0.005382548344812221\n",
      "-0.004492446527497562 ; -0.0044780798313937\n",
      "-0.0038023533823273764 ; -0.003788604434618037\n",
      "-0.003549535146727065 ; -0.0035359994665640215\n",
      "-0.0035412673956963896 ; -0.003527831724525754\n",
      "-0.003385745893078075 ; -0.003373338439693896\n",
      "-0.0029167913058632308 ; -0.0029065638607050914\n",
      "-0.002224067829402695 ; -0.0022166745328178195\n",
      "-0.0014927102667586154 ; -0.001488005079153254\n",
      "-0.0008101175351115551 ; -0.0008076413626767476\n"
     ]
    }
   ],
   "source": [
    "#Don't run this cell multiple times! \n",
    "numericApproxGrad = numericalGradientApproximation(nnThetas, X, y, indexToStop = 500)\n",
    "roundedNumApproxGrad = np.round(numericApproxGrad, 3)\n",
    "\n",
    "#are these rounded entries all the same?\n",
    "print(np.all(np.isclose(test[0:len(numericApproxGrad)], numericApproxGrad, atol = 0.01)))\n",
    "# they should be!\n",
    "\n",
    "#print the matched entries (gradient descent value ; numerical value)\n",
    "k = zip(test, numericApproxGrad)\n",
    "for analytical, numerical in k:\n",
    "    print(str(analytical) + \" ; \" + str(numerical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651ebbc",
   "metadata": {},
   "source": [
    "## Moving on: adding regularisation and training the network\n",
    "\n",
    "If the above did not return the same values for you and you've been repeatedly smashing your face into your desk, laptop, or other assorted objects in the vicinity out of frustration, feel free to copy the correct implementation of `nnGradientFunction` from the answers. It's now time to add one last change: we need to add regularisation to the gradients (partial derivatives) of our theta's. \n",
    "\n",
    "Luckily, it turns out that, like before, you can do this after you've already computed the gradients: \n",
    "![RegularisationCostAddedToGradients](RegularisationCostAddedToGradients.PNG)\n",
    "\n",
    "This shows it in a loop. But we have the gradients of our two theta matrices. And so, you can simply add $$\\frac{\\lambda}{m} \\cdot \\Theta^{(1)}$$ to $\\Theta^{(1)}$ (excluding the biases in the first column!), and likewise, $$\\frac{\\lambda}{m} \\cdot \\Theta^{(2)}$$ to $\\Theta^{(2)}$ (again, do nothing to the bias terms!). <br> <br>\n",
    "\n",
    "Up to you to:\n",
    "* Copy the nnGradientFunction from above (or edit it in-place) and add the regularisation terms to the gradient!\n",
    "\n",
    "Hint:\n",
    "* The easiest implementation is to just set the first column of `thetaOneMatrix` and `thetaTwoMatrix` to 0, and to then add them $\\cdot \\frac{\\lambda}{m}$ to the matrices of gradients you calculated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45218cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19885,)\n",
      "(19885,)\n",
      "[-1.10922522e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -3.91994057e-08 -8.46707142e-07 -8.46707142e-07\n",
      " -3.52794655e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "[-1.10922522e-02 -3.66656030e-05  2.15115729e-04  1.18397138e-04\n",
      " -2.43444902e-04  2.85316348e-04  1.56652491e-04  1.71604262e-04\n",
      " -2.23087203e-04 -2.97624848e-05 -7.75056843e-05  2.56007792e-04\n",
      "  8.63018117e-05  1.93579045e-04 -3.47913989e-05 -1.64470749e-04\n",
      "  3.27090439e-05 -2.61657315e-04  1.96539395e-04  7.89828429e-05\n",
      "  1.54821680e-04 -8.72669657e-05  2.82362342e-04  2.35825508e-04\n",
      "  1.66996699e-04 -1.83180139e-04 -1.99634051e-05 -2.73663008e-04\n",
      " -2.07384828e-04  1.09807410e-04]\n"
     ]
    }
   ],
   "source": [
    "# answer\n",
    "\n",
    "def nnGradientFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10,\n",
    "                      printInternalProgress = False, smallValueToPreventLogErrors = 1e-12):\n",
    "   \n",
    "    m = len(X)\n",
    "    nnThetas = np.array(nnThetas)\n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #calculating the forward pass\n",
    "    inputs        = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    weightedSumHL = inputs @ thetaOneMatrix.T\n",
    "    activationsHL  = mySigmoid(weightedSumHL)\n",
    "    \n",
    "    inputsOL      = np.c_[np.ones(shape = (len(activationsHL), 1)), activationsHL]\n",
    "    weightedSumOL = inputsOL @ thetaTwoMatrix.T\n",
    "    activationsOL = mySigmoid(weightedSumOL)\n",
    "    \n",
    "    if printInternalProgress:\n",
    "        print(\"Activations output layer shape: \" + str(activationsOL.shape))\n",
    "        print(\"Activations for one training example: \" + str(activationsOL[0,:]))\n",
    "    \n",
    "    #cost\n",
    "    J = 1/m * np.sum((- (y * np.log(activationsOL + smallValueToPreventLogErrors)) - ((1-y) * np.log(1-activationsOL+smallValueToPreventLogErrors))))\n",
    "    \n",
    "    #regularised cost\n",
    "    #remember: units in the rows, their parameters in the columns\n",
    "    #Hence, [:,1:] removes the columns with the bias term.\n",
    "    regThetaOne = np.sum(np.square(thetaOneMatrix[:,1:]))\n",
    "    regThetaTwo = np.sum(np.square(thetaTwoMatrix[:,1:]))\n",
    "    regCost     = J + (lambda_/(2*m)) * (regThetaOne + regThetaTwo)\n",
    "    \n",
    "    if printInternalProgress:\n",
    "        print(\"Regularised cost: \" + str(regCost))\n",
    "        print(\"Regularised cost shape: \" + str(regCost.shape))\n",
    "    \n",
    "    #calculate error layer 3\n",
    "    smallDeltaThree = activationsOL - y\n",
    "    if printInternalProgress:\n",
    "        print(\"delta^(3): \" + str(smallDeltaThree))\n",
    "        print(\"delta^(3) shape: \" + str(smallDeltaThree.shape) + \"\\n\")\n",
    "    \n",
    "    #calculate the weighted sums that the HL generates that go into the activation function and then get sent to \n",
    "    #the output layer\n",
    "    weightedSumsLayerTwo = zTwo = weightedSumHL #= np.c_[np.ones(shape = (len(X), 1)), X] @ thetaOneMatrix.T\n",
    "    if printInternalProgress:\n",
    "        print(\"z^(2): \" + str(zTwo))\n",
    "        print(\"z^(2) shape: \" + str(zTwo.shape) + \"\\n\")\n",
    "    \n",
    "    sigmoidGradientOfZTwo = mySigmoidGradient(zTwo)\n",
    "    if printInternalProgress:\n",
    "        print(\"Sigmoid gradient: \" + str(sigmoidGradientOfZTwo))\n",
    "        print(\"Sigmoid gradient shape: \" + str(sigmoidGradientOfZTwo.shape))\n",
    "    smallDeltaTwo   = smallDeltaThree @ thetaTwoMatrix * np.c_[np.ones(shape = (len(X), 1)),\n",
    "                                                               mySigmoidGradient(zTwo)]\n",
    "    smallDeltaTwo   = smallDeltaTwo[:, 1:]\n",
    "    if printInternalProgress:\n",
    "        print(\"delta^(2): \" + str(smallDeltaTwo))\n",
    "        print(\"size of delta^(2): \" + str(smallDeltaTwo.shape)) \n",
    "    \n",
    "    \n",
    "    bigDeltaThree     = smallDeltaThree.T @ np.c_[np.ones(shape = len(activationsHL)),\n",
    "                                                 activationsHL]\n",
    "    if printInternalProgress:\n",
    "        print(\"Gradients output layer: \" + str(bigDeltaThree))\n",
    "        print(\"Gradients output layer shape: \" + str(bigDeltaThree.shape))\n",
    "    \n",
    "    bigDeltaTwo       = smallDeltaTwo.T   @ np.c_[np.ones(shape = len(X)),\n",
    "                                                 X]\n",
    "    if printInternalProgress:\n",
    "        print(\"Gradients hidden layer: \" + str(bigDeltaTwo))\n",
    "        print(\"Gradients hidden layer shape: \" + str(bigDeltaTwo.shape))\n",
    "    \n",
    "    #average values, we've now summed them over all training examples\n",
    "    bigDeltaTwo   = bigDeltaTwo   * 1/m\n",
    "    bigDeltaThree = bigDeltaThree * 1/m\n",
    "    if printInternalProgress:\n",
    "        print(\"Unregularised final gradients:\")\n",
    "        print(np.append(np.ravel(bigDeltaTwo), np.ravel(bigDeltaThree)))\n",
    "        print(\"Unregularised final gradients shape: \" + str(np.append(np.ravel(bigDeltaTwo), np.ravel(bigDeltaThree)).shape))\n",
    "    #add the regularisation cost into the gradients\n",
    "    thetaOneMatrix[:, 0] = 0\n",
    "    bigDeltaTwo   += lambda_/m * thetaOneMatrix\n",
    "    thetaTwoMatrix[:, 0] = 0\n",
    "    bigDeltaThree += lambda_/m * thetaTwoMatrix\n",
    "    \n",
    "    \n",
    "\n",
    "    finalGradients = np.append(np.ravel(bigDeltaTwo), np.ravel(bigDeltaThree))\n",
    "    if printInternalProgress:\n",
    "        print(\"Regularised final gradients:\")\n",
    "        print(finalGradients)\n",
    "        print(\"Regularised final gradients shape: \" + str(finalGradients.shape))\n",
    "        print(\"---\\n\")\n",
    "    \n",
    "    return finalGradients\n",
    "\n",
    "testRegularised = nnGradientFunction(nnThetas, X, y, lambda_ = 50)\n",
    "print(testRegularised.shape)\n",
    "print(nnThetas.shape)\n",
    "print(test[0:30])\n",
    "print(testRegularised[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b454d",
   "metadata": {},
   "source": [
    "## Training the neural network with your brand-spankin' new function\n",
    "\n",
    "Finally, the time has come to face your destiny: training a neural network to classify some digits in an afternoon practical. It's a simple destiny, as destinies go, but a worthwhile one nonetheless. Besides, the other choice is facing the fact that destinies don't exist and existence is meaningless, so have at it!\n",
    "\n",
    "* Use your favourite pal `fmin_cg` to minimise this function. See the documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html). Assign the result to `trainedNeuralNetwork`.Use a $\\lambda$ of 1, and be sure to set `maxiter = 60`.\n",
    "\n",
    "Hint:\n",
    "* This will take ~10 minutes to run. Feel free to grab something to drink or take a short walk. \n",
    "\n",
    "**Note**:\n",
    "* See how I start the two theta matrices with random uniform numbers? There's actually a very good reason for that. Remember, with logistic and linear regression, I regularly just set the start parameters to an array of zeros. That wouldn't work here. Why? Because then the partial derivative w.r.t. every weight and bias would be exactly the same, and nothing could be learned! Hence, it is necessary to break this symmetry by randomly initialising. See [part 1 here](https://www.deeplearning.ai/ai-notes/initialization/) to test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c6b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current theta values: [ 0.07244773 -0.01466873  0.0860609  ... -0.2300963  -0.10110768\n",
      " -0.1559709 ]\n",
      "Current theta values: [ 0.08238082 -0.014668    0.0860566  ... -0.24459242 -0.10934185\n",
      " -0.16882141]\n",
      "Current theta values: [ 0.0956764  -0.01466229  0.0860231  ... -0.24972213 -0.09982004\n",
      " -0.18524015]\n",
      "Current theta values: [ 0.09722214 -0.01465244  0.08596534 ... -0.23312141 -0.04903244\n",
      " -0.192877  ]\n",
      "Current theta values: [ 0.11710722 -0.01463004  0.08583387 ... -0.28327271  0.0425096\n",
      " -0.25932271]\n",
      "Current theta values: [ 0.23651744 -0.01455269  0.08538007 ... -0.58740685  0.26144015\n",
      " -0.55357255]\n",
      "Current theta values: [ 0.2700374  -0.01453715  0.08528892 ... -0.64876802  0.29733243\n",
      " -0.6077704 ]\n",
      "Current theta values: [ 0.27431329 -0.01453565  0.08528011 ... -0.65086077  0.30408301\n",
      " -0.6006793 ]\n",
      "Current theta values: [ 0.28487395 -0.01453026  0.08524848 ... -0.67868336  0.25649647\n",
      " -0.58262657]\n",
      "Current theta values: [ 0.28809902 -0.01452299  0.08520585 ... -0.71734385  0.19092582\n",
      " -0.56299654]\n",
      "Current theta values: [ 0.28837957 -0.01451751  0.08517369 ... -0.74988295  0.14752447\n",
      " -0.55628587]\n",
      "Current theta values: [ 0.28589781 -0.01451246  0.08514407 ... -0.78048887  0.1269518\n",
      " -0.55059831]\n",
      "Current theta values: [ 0.28380425 -0.01450002  0.08507108 ... -0.89918285 -0.02824124\n",
      " -0.59886121]\n",
      "Current theta values: [ 0.28207506 -0.01448888  0.0850057  ... -0.98938962 -0.1048984\n",
      " -0.62318853]\n",
      "Current theta values: [ 0.27978365 -0.01447494  0.08492391 ... -1.10165714 -0.18572864\n",
      " -0.69549808]\n",
      "Current theta values: [ 0.2713905  -0.01445208  0.08478983 ... -1.26287353 -0.20353115\n",
      " -0.78194958]\n",
      "Current theta values: [ 0.25752614 -0.01442697  0.0846425  ... -1.44841977 -0.26698929\n",
      " -0.94176517]\n",
      "Current theta values: [ 0.25211213 -0.01441744  0.08458661 ... -1.49717068 -0.25645926\n",
      " -0.99653264]\n",
      "Current theta values: [ 0.2451515  -0.01440435  0.08450978 ... -1.55071961 -0.2442589\n",
      " -1.08922322]\n",
      "Current theta values: [ 0.23404637 -0.01439437  0.08445121 ... -1.58348992 -0.22996879\n",
      " -1.16068814]\n",
      "Current theta values: [ 0.22718029 -0.01438544  0.08439884 ... -1.60337072 -0.19822917\n",
      " -1.21087535]\n",
      "Current theta values: [ 0.22343823 -0.01437384  0.08433077 ... -1.64056272 -0.19414323\n",
      " -1.29260421]\n",
      "Current theta values: [ 0.22438507 -0.01435767  0.08423592 ... -1.68507303 -0.16473506\n",
      " -1.38431924]\n",
      "Current theta values: [ 0.22720562 -0.01432816  0.08406276 ... -1.77736591 -0.13863345\n",
      " -1.54249891]\n",
      "Current theta values: [ 0.22990341 -0.01428559  0.08381303 ... -1.90862113 -0.0953125\n",
      " -1.75156124]\n",
      "Current theta values: [ 0.23085715 -0.01426697  0.08370378 ... -1.96410566 -0.07613517\n",
      " -1.82600635]\n",
      "Current theta values: [ 0.22816647 -0.01425173  0.08361436 ... -2.01517187 -0.08211732\n",
      " -1.87806934]\n",
      "Current theta values: [ 0.2231496  -0.01423172  0.08349696 ... -2.07735802 -0.08538542\n",
      " -1.92742962]\n",
      "Current theta values: [ 0.21765214 -0.0142131   0.08338772 ... -2.13522873 -0.10373327\n",
      " -1.96508782]\n",
      "Current theta values: [ 0.21390583 -0.01419898  0.08330488 ... -2.17309882 -0.12617057\n",
      " -1.97517053]\n",
      "Current theta values: [ 0.20884235 -0.01418036  0.08319567 ... -2.21824664 -0.14411936\n",
      " -1.98089967]\n",
      "Current theta values: [ 0.20146098 -0.01416     0.08307618 ... -2.26784149 -0.16746332\n",
      " -1.98988469]\n",
      "Current theta values: [ 0.19797159 -0.01415072  0.08302174 ... -2.28848503 -0.17809581\n",
      " -1.99655158]\n",
      "Current theta values: [ 0.19572689 -0.01413923  0.08295435 ... -2.30623784 -0.17306415\n",
      " -1.99849928]\n",
      "Current theta values: [ 0.19253372 -0.0141253   0.08287261 ... -2.33098534 -0.17863614\n",
      " -2.00702201]\n",
      "Current theta values: [ 0.18854319 -0.01410079  0.0827288  ... -2.37698623 -0.19692279\n",
      " -2.02674279]\n",
      "Current theta values: [ 0.18394273 -0.01405951  0.08248663 ... -2.44832686 -0.20786404\n",
      " -2.05214591]\n",
      "Current theta values: [ 0.18053295 -0.01403711  0.08235518 ... -2.4855903  -0.20513817\n",
      " -2.06649921]\n",
      "Current theta values: [ 0.17582689 -0.01401093  0.08220162 ... -2.53023925 -0.20032179\n",
      " -2.08703495]\n",
      "Current theta values: [ 0.17322949 -0.01399133  0.08208663 ... -2.56359075 -0.19701041\n",
      " -2.10584284]\n",
      "Current theta values: [ 0.17228167 -0.01397385  0.08198405 ... -2.59307536 -0.19771248\n",
      " -2.12600074]\n",
      "Current theta values: [ 0.17512364 -0.01393124  0.08173405 ... -2.66031295 -0.19281751\n",
      " -2.17291678]\n",
      "Current theta values: [ 0.17460952 -0.01389572  0.08152567 ... -2.71675219 -0.19232131\n",
      " -2.21260601]\n",
      "Current theta values: [ 0.16382882 -0.01382008  0.08108187 ... -2.8349455  -0.18883463\n",
      " -2.29355411]\n",
      "Current theta values: [ 0.15909694 -0.01378145  0.08085525 ... -2.89341333 -0.18014813\n",
      " -2.33262398]\n",
      "Current theta values: [ 0.154618   -0.01374233  0.08062576 ... -2.95450419 -0.18004191\n",
      " -2.37452211]\n",
      "Current theta values: [ 0.15360269 -0.01372861  0.08054524 ... -2.97271214 -0.17910899\n",
      " -2.38698595]\n",
      "Current theta values: [ 0.15202091 -0.01371284  0.0804527  ... -2.98948384 -0.16971681\n",
      " -2.39805565]\n",
      "Current theta values: [ 0.14805478 -0.01369855  0.08036886 ... -3.00545247 -0.16526333\n",
      " -2.40841256]\n",
      "Current theta values: [ 0.14501434 -0.01368246  0.0802745  ... -3.02200091 -0.1607982\n",
      " -2.41753353]\n",
      "Current theta values: [ 0.14552081 -0.01367193  0.08021272 ... -3.03114388 -0.15682379\n",
      " -2.42173466]\n",
      "Current theta values: [ 0.14523187 -0.01363339  0.07998657 ... -3.06843879 -0.148378\n",
      " -2.44004102]\n",
      "Current theta values: [ 0.14332746 -0.01357689  0.07965512 ... -3.12382323 -0.13614658\n",
      " -2.46738163]\n",
      "Current theta values: [ 0.13967254 -0.01354375  0.07946069 ... -3.15774932 -0.13399816\n",
      " -2.48631661]\n",
      "Current theta values: [ 0.13355486 -0.0134902   0.07914653 ... -3.20900561 -0.12309235\n",
      " -2.51684097]\n",
      "Current theta values: [ 0.12838897 -0.01344977  0.07890931 ... -3.24726289 -0.11534555\n",
      " -2.53905593]\n",
      "Current theta values: [ 0.12701205 -0.01342476  0.07876255 ... -3.27099875 -0.11028534\n",
      " -2.55141914]\n",
      "Current theta values: [ 0.12787125 -0.01340946  0.07867282 ... -3.28795251 -0.11246836\n",
      " -2.56186965]\n",
      "Current theta values: [ 0.13085643 -0.01339819  0.07860667 ... -3.29840327 -0.11079552\n",
      " -2.5681614 ]\n",
      "Current theta values: [ 0.13684433 -0.01336078  0.07838718 ... -3.33244389 -0.10884463\n",
      " -2.59059492]\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.368076\n",
      "         Iterations: 60\n",
      "         Function evaluations: 128\n",
      "         Gradient evaluations: 128\n"
     ]
    }
   ],
   "source": [
    "rng = default_rng(42)\n",
    "thetaOneMatrix = rng.uniform(-0.12, 0.12, size = (25, 785))\n",
    "thetaTwoMatrix = rng.uniform(-0.12, 0.12, size = (10, 26))\n",
    "initialThetas = np.append(np.ravel(thetaOneMatrix), np.ravel(thetaTwoMatrix))\n",
    "\n",
    "def mySigmoid(data):\n",
    "    data = np.array(data)\n",
    "    return scipy.special.expit(data)\n",
    "\n",
    "#answer\n",
    "def printProgress(xk):\n",
    "    print(\"Current theta values: \" + str(xk))\n",
    "    \n",
    "\n",
    "trainedNeuralNetwork = fmin_cg(f = nnCostFunction, x0 = initialThetas, fprime = nnGradientFunction,\n",
    "                                 args = (X_train, y_train, 1), full_output = 1, retall = 1, maxiter = 60,\n",
    "                               callback=printProgress)\n",
    "\n",
    "#I used this to generate the values for longer training, and with different lambdas\n",
    "# trainedNeuralNetworkLongerLambdaZero = fmin_cg(f = nnCostFunction, x0 = initialThetas, fprime = nnGradientFunction,\n",
    "#                                args = (X_train, y_train, 0), maxiter = 500)\n",
    "# trainedNeuralNetworkLongerLambdaOne = fmin_cg(f = nnCostFunction, x0 = initialThetas, fprime = nnGradientFunction,\n",
    "#                                args = (X_train, y_train, 1), maxiter = 500)\n",
    "# trainedNeuralNetworkLongerLambdaTen = fmin_cg(f = nnCostFunction, x0 = initialThetas, fprime = nnGradientFunction,\n",
    "#                                args = (X_train, y_train, 10), maxiter = 500)\n",
    "# trainedNeuralNetworkLongerLambdaHundred = fmin_cg(f = nnCostFunction, x0 = initialThetas, fprime = nnGradientFunction,\n",
    "#                                args = (X_train, y_train, 100), maxiter = 500)\n",
    "\n",
    "# np.savez_compressed(\"500IterationsTrainedThetasLambdas0_1_10_100\",\n",
    "#                    thetaListLambdaZero    = trainedNeuralNetworkLongerLambdaZero,\n",
    "#                    thetaListLambdaOne     = trainedNeuralNetworkLongerLambdaOne,\n",
    "#                    thetaListLambdaTen     = trainedNeuralNetworkLongerLambdaTen,\n",
    "#                    thetaListLambdaHundred = trainedNeuralNetworkLongerLambdaHundred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270c246",
   "metadata": {},
   "source": [
    "## Seeing your performance on the train and test set.\n",
    "\n",
    "Wow, you've made it. Let's see how well you do on the train and test sets.\n",
    "\n",
    "To do that: \n",
    "\n",
    "* Use the reconstituted $\\Theta^{(1)}$ and $\\Theta^{(2)}$ matrices, along with the `forwardPass()`-function below to perform forward passes for the train and test set.\n",
    "* Use `np.where` and `np.amax` (with `axis = 1`) to turn the raw numbers into class label vectors with 0 and 1. Or use `np.argmax` to do it in one step.\n",
    "* Use `np.all` to check whether the labels are the same as the true labels or not, and calculate a % of correctly classified images.\n",
    "* Print these percentages.\n",
    "* **Optional** If you like, this same neural network trained for 500 iterations with a $\\lambda$ of 0, 1, 10, or 100 is available as the file \"500IterationsTrainedThetasLambdas0_1_10_100.npz\", which you can load in with `np.load()`. This yields a dictionary with as keys the names of the array. You can see how things differ if we train for longer and try multiple values for $\\lambda$. Access it by using `loadedData[\"thetaListLambdaZero\"|\"thetaListLambdaOne\"|\"thetaListLambdaTen\"|\"thetaListLambdaHundred\"]`.\n",
    "\n",
    "Hint:\n",
    "* If you don't quite know how to do this: I did it for you in the afternoon practical yesterday when drawing the misclassified digits. Look there.\n",
    "* DuckDuckGo (or Google) is your friend: look at the documentation of these Numpy functions.\n",
    "* First experiment with one row to see how to use each function to do what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027312b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct train set: 95.49090181963608%\n",
      "Percentage correct test set: 93.78%\n"
     ]
    }
   ],
   "source": [
    "hiddenLayerSize = 25; inputLayerSize = 784; classLabels = 10\n",
    "hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "thetaOneMatrixTrained = np.reshape(trainedNeuralNetwork[0][0:hiddenLayerParamNr],\n",
    "                                   newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "outputLayerParamStart = hiddenLayerParamNr \n",
    "thetaTwoMatrixTrained = np.reshape(trainedNeuralNetwork[0][outputLayerParamStart:],\n",
    "                                   newshape = (classLabels, hiddenLayerSize+1))\n",
    "\n",
    "\n",
    "def forwardPass(X, y, thetaOne = thetaOneMatrixTrained, thetaTwo = thetaTwoMatrixTrained):\n",
    "    xInputs                = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    firstLayerWeightedSum  = xInputs @ thetaOne.T\n",
    "    firstLayerActivations  = mySigmoid(firstLayerWeightedSum)\n",
    "    secondLayerInputs      = np.c_[np.ones(shape = (len(firstLayerActivations), 1)), firstLayerActivations]\n",
    "    secondLayerWeightedSum = secondLayerInputs @ thetaTwo.T\n",
    "    secondLayerActivations = mySigmoid(secondLayerWeightedSum)\n",
    "    return secondLayerActivations\n",
    "\n",
    "#your answer here\n",
    "\n",
    "#answer\n",
    "trainPredictions           = forwardPass(X_train, y_train)\n",
    "# could also do the below in one step with np.argmax(axis=1) -> I only found out about that function later!\n",
    "maxEntryEachLabelTrain     = np.amax(trainPredictions[:, :], axis = 1)\n",
    "outcomeClassVectorsTrain   = np.where(maxEntryEachLabelTrain[:,np.newaxis] == trainPredictions[:, :],\n",
    "                                      1, 0)\n",
    "\n",
    "#compare train predictions with train truth\n",
    "classificationCorrectTrain = np.all(outcomeClassVectorsTrain == y_train, axis = 1)\n",
    "percentageCorrectTrain     = np.sum(classificationCorrectTrain)/len(y_train)*100\n",
    "\n",
    "testPredictions            = forwardPass(X_test, y_test)\n",
    "outcomeClassVectorsTest    = np.where(np.amax(testPredictions[:, :], axis = 1)[:,np.newaxis] == testPredictions[:, :],\n",
    "                                      1, 0)\n",
    "percentageCorrectTest      = np.sum(np.all(outcomeClassVectorsTest == y_test, axis = 1))/len(y_test)*100\n",
    "\n",
    "print(\"Percentage correct train set: \" + str(percentageCorrectTrain) + \"%\")\n",
    "print(\"Percentage correct test set: \"  + str(percentageCorrectTest)  + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137807a7",
   "metadata": {},
   "source": [
    "## The cool thing about working with images: visualising what your NN has learned!\n",
    "\n",
    "As I've told you in the lectures, each hidden unit learns some feature representation that allows the next layer of logistic regressors to better differentiate the (here 10) different classes. So neural networks will learn their own transformations of input features to get best classification performance: no manual feature encodings by humans needed.\n",
    "\n",
    "Now, these Hidden Layer units have 784 weights, 1 for each pixel. So, what if we create a 28\\*28 image of those weights? This will show us how strongly each weight reacts to each pixel input. In other words: it will show us what the HL units have learned to recognise/what strongly triggers their activation. The below shows you how that looks!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b57b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weightsOnly = thetaOneMatrixTrained[:, 1:]\n",
    "\n",
    "\n",
    "figWeights, axWeights = plt.subplots(5, 5, figsize = (8, 8))\n",
    "counter = 0\n",
    "for row in range(0, len(axWeights)):\n",
    "    for col in range(0, len(axWeights)):\n",
    "        axWeights[row, col].imshow(weightsOnly[counter,:].reshape(28,28),\n",
    "                                      cmap = \"gray\", interpolation = \"none\")\n",
    "        axWeights[row, col].set_title(\"Weight \" + str(counter + 1))\n",
    "        axWeights[row, col].set_axis_off()\n",
    "        counter += 1\n",
    "figWeights.suptitle(\"Plot of weights\")\n",
    "figWeights.tight_layout()        \n",
    "\n",
    "\n",
    "#plot a few digits for comparison:\n",
    "\n",
    "figDigits, axDigits = plt.subplots(3, 3, figsize = (5,5))\n",
    "counter = 0\n",
    "for row in range(0, len(axDigits)):\n",
    "    for col in range(0, len(axDigits)):\n",
    "        digit = np.reshape(X_train[counter + np.ceil(np.random.rand() * 10000).astype(int), :], newshape = (28,28))\n",
    "        axDigits[row, col].imshow(digit, cmap = \"gray\", interpolation = \"none\")\n",
    "        axDigits[row, col].set_axis_off()\n",
    "        counter += 1\n",
    "figDigits.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15c605",
   "metadata": {},
   "source": [
    "## Understanding the weights\n",
    "\n",
    "It's not too clear what _exactly_ the weights are capturing, but some weights really seem to be capturing specific strokes, just like in the 3Blue1Brown video. For example, weight 5 seems to capture the upward stroke of a 1 or 7, and weight 20 seems to be capturing something vaguely 8-like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc0ba9",
   "metadata": {},
   "source": [
    "## Convolving filters yourself\n",
    "\n",
    "The image below is the most exciting image ever to be classified in the history of neural networks. Gaze upon it and weep:\n",
    "![Square](Square.PNG)\n",
    "To aid in the monumental task of seeing where the square's edges are, you're going to use convolution. You'll use these two 3\\*3 filters for that:\n",
    "![Filter1](EdgeFilter1.PNG) ![Filter2](EdgeFilter2.PNG)\n",
    "\n",
    "Here, the values in the cells are the weights for that cell. Remember, to perform a convolution you position the filter over a certain focal pixel, multiply the weights in the filter with the correct underlying pixel values, sum it, add a bias (if using), put that into the activation function (if using) and that's the first entry of your output. Then you slide the filter 1 (or more pixels, if you want) to the right and do it again. If you reach the edge, you move one pixel down and go all the way back to the left, sliding it to the right once more: ![convolution](keras_conv2d_padding.GIF). \n",
    "\n",
    "You'll run into trouble at the edges of the image: there the kernel will overlap with non-existent pixels. For this, you use padding, which you already saw in the image above.\n",
    "\n",
    "Up to you to: \n",
    "* Add zero-padding to the square image. You just need 1 row of zeros along the bottom and top, and one column of zeros at the beginning and end of the array. You can do this easily with [np.pad()](https://numpy.org/doc/stable/reference/generated/numpy.pad.html)\n",
    "* Loop over the columns and rows (in that order) of the image, excepting the first and last column and row. Multiply the values in the first filter with the corresponding pixel intensities in the image, then sum them. Save these values in a new array.\n",
    "* Do the same for the second filter\n",
    "* Use `plt.imshow()` with `plt.show()` to visualise your edge detector outputs!\n",
    "* See if there's any edge detected that you wouldn't want to take into account, and also try to understand why it looks the way it looks!\n",
    "\n",
    "Hints:\n",
    "* Remember that Python's from-to function `range` excludes the final value: if you want to go from currentPixel-1 to currentPixel+1 (so 3 pixels), you need (currentPixel-1):(currentPixel+2)\n",
    "* You should not start at index 0 in your padded image (after all, then the padding has no use!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "800d1818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 86)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d1d79bac0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "squareImage = np.array(ImageOps.grayscale(Image.open(\"Square.PNG\"))) * 1/255\n",
    "filterOne   = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "filterTwo   = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
    "\n",
    "print(squareImage.shape)\n",
    "plt.imshow(squareImage, cmap = 'gray', vmin = 0, vmax = 1)\n",
    "plt.show()\n",
    "\n",
    "#answer\n",
    "squareImageZeroPadded = np.pad(squareImage, 1, \"constant\", constant_values = 0)\n",
    "print(squareImageZeroPadded.shape)\n",
    "\n",
    "filterOneOutput = np.zeros_like(squareImage)\n",
    "filterTwoOutput = np.zeros_like(squareImage)\n",
    "for row in range(1, squareImageZeroPadded.shape[0]-1):\n",
    "    for col in range(1, squareImageZeroPadded.shape[1]-1):\n",
    "        filterOneOutput[(row-1), (col-1)] = np.sum(squareImageZeroPadded[(row-1):(row+2),(col-1):(col+2)] * filterOne)\n",
    "        filterTwoOutput[(row-1), (col-1)] = np.sum(squareImageZeroPadded[(row-1):(row+2),(col-1):(col+2)] * filterTwo)\n",
    "\n",
    "figFilt, axFilt = plt.subplots(1, 2)\n",
    "axFilt[0].imshow(filterOneOutput, cmap = \"gray\", vmin = 0, vmax = 1)\n",
    "axFilt[1].imshow(filterTwoOutput, cmap = \"gray\", vmin = 0, vmax = 1)\n",
    "\n",
    "#Strange that one is shorter than the other. Let's invert the filters and see what happens then:\n",
    "#(Note: this code is not very DRY, but that's because it's extra)\n",
    "filterThree       = np.flip(filterOne, axis = 1)\n",
    "filterFour        = np.flip(filterTwo, axis = 0)\n",
    "filterThreeOutput = np.zeros_like(squareImage)\n",
    "filterFourOutput  = np.zeros_like(squareImage)\n",
    "for row in range(1, squareImageZeroPadded.shape[0]-1):\n",
    "    for col in range(1, squareImageZeroPadded.shape[1]-1):\n",
    "        filterThreeOutput[(row-1), (col-1)] = np.sum(squareImageZeroPadded[(row-1):(row+2),(col-1):(col+2)] * filterThree)\n",
    "        filterFourOutput[(row-1), (col-1)]  = np.sum(squareImageZeroPadded[(row-1):(row+2),(col-1):(col+2)] * filterFour)\n",
    "        \n",
    "figFiltFlip, axFiltFlip = plt.subplots(1, 2)\n",
    "axFiltFlip[0].imshow(filterThreeOutput, cmap = \"gray\", vmin = 0, vmax = 1)\n",
    "axFiltFlip[1].imshow(filterFourOutput , cmap = \"gray\", vmin = 0, vmax = 1)\n",
    "\n",
    "#It's really a function of the corner parts, which, depending on the orientation of the filter, either count or do not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da238b",
   "metadata": {},
   "source": [
    "## Keras convolutional neural network on MNIST data\n",
    "\n",
    "An accuracy of 93% on the test set using a dense neural network is nothing to scoff at, but you know that convolutional neural networks (along with other innovations like Dropout and other activation functions) are what made the real leaps in performance possible. Using the Keras library, you can easily train your own convolutional neural network. \n",
    "\n",
    "I will discuss Keras in a bit more detail next Monday. For now, you can do the following:\n",
    "\n",
    "* Go [here](https://keras.io/examples/vision/mnist_convnet/) and input the commands there into the code cell(s) below. You could copy all the code, but typing some of it yourself might give you more of an idea of what you're doing so it's probably preferable.\n",
    "* As you step through it be sure to search for things you don't know about and think about the dimensionality. What is the total dimensionality of the data after you've run 32 convolutional filters over it? `model.summary()` shows this, but see if you get it.\n",
    "* As an **optional** question: you could try to get the untrained and trained filter weights and visualise them to see what sort of features in the image each filter has become attuned to. See [this question on Stackoverflow](https://stackoverflow.com/questions/43305891/how-to-correctly-get-layer-weights-from-conv2d-in-keras?noredirect=1&lq=1) and [this link about getting weights from a Keras model](https://www.codespeedy.com/get_weights-and-set_weights-functions-in-keras-layers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbb10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483379d8",
   "metadata": {},
   "source": [
    "## What I want you to remember here:\n",
    "* How backpropagation works\n",
    "* A bit of detail of how the maths works: if I ask you to write down the series of partial derivatives you need to get the partial derivative w.r.t. the weights and biases of a certain neuron in a diagram, I expect you to at least _sort off_ be able to give it. Perhaps not with perfect math notation, but close.\n",
    "* How convolutions work and can help make powerful deep learning classifiers (specifically on images)\n",
    "\n",
    "## The end \n",
    "\n",
    "Wow, backpropagation is a real baguette in the donkey. Still, you're here. Well done! \n",
    "\n",
    "## Survey\n",
    "This wouldn't be a true Jupyter notebook for this course without a link. [Go ahead and fill this bad boy out!](https://docs.google.com/forms/d/e/1FAIpQLSeCfdD2N_6RswisczSV3y8kuun9dBCZGaR26JpYjZibGU-HrA/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
