{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 3\n",
    "\n",
    "Welcome to the second practical of today. Here, you'll start on the process of actually implementing backpropagation. First, preparation of some functions we will need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.optimize import fmin_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important functions\n",
    "def mySigmoid(data):\n",
    "    output = 1/(1+ np.exp(-data))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17132cfd",
   "metadata": {},
   "source": [
    "## Loading in the MNIST data\n",
    "\n",
    "We're going to be working with the MNIST data again, though now in a slighly different format of 28\\*28 = 784 pixel images, where each pixel has a brightness from 0 to 255. We change this brightness to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5e9af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#load-in taken from here: https://keras.io/examples/vision/mnist_convnet/\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03775e",
   "metadata": {},
   "source": [
    "## Subsampling the training data\n",
    "\n",
    "At this point, the data is in the format of 60.000 training samples, each consisting of a 28\\*28 matrix of pixel intensities, with 1 channel (i.e. there's just 1 colour here, grey. In a colour image you would have 3 channels: red, green and blue intensities).\n",
    "\n",
    "For a dense neural network that we'll be working with, we'll just convert these 28\\*28 matrices into vectors with 784 entries. We'll also subsample the data: this amount of data is fine for optimised routines, but for our own implementation 20.000 training samples is _plenty_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbbf87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), array([1983, 1951, 2089, 1973, 1807, 1948, 2044, 1986, 2248, 1975],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(42)\n",
    "\n",
    "print(np.unique(y_train, axis = 0, return_counts= True))\n",
    "#the training set is not quite balanced, but let's keep the proportions it has nonetheless.\n",
    "\n",
    "uniqueLabels = np.unique(y_train, axis = 0, return_counts= True)\n",
    "\n",
    "#if to make sure you don't subsample again if you rerun this cell\n",
    "if np.sum(uniqueLabels[1]) >=20010:\n",
    "    listChoiceIndices = []\n",
    "    for index, label in enumerate(uniqueLabels[0]):\n",
    "        print(\"Subsetting for label: \" + str(label))\n",
    "        nToPick = np.ceil(1/3*uniqueLabels[1][index]).astype(int)\n",
    "        indicesToPick = np.where(np.all(y_train == label, axis = 1))\n",
    "        choiceIndices = rng.choice(indicesToPick[0], size = nToPick, replace = False)\n",
    "        listChoiceIndices.append(choiceIndices)\n",
    "    choiceIndices = [val for sublist in listChoiceIndices for val in sublist]\n",
    "\n",
    "    x_train, y_train = x_train[choiceIndices], y_train[choiceIndices]\n",
    "    #convert matrices into vectors\n",
    "\n",
    "x_train_vectors = np.vstack([np.ravel(elem) for elem in x_train])\n",
    "x_test_vectors  = np.vstack([np.ravel(elem) for elem in x_test])\n",
    "\n",
    "#output for afternoon practical.\n",
    "# np.savetxt(\"XTrainMNIST.csv\", x_train_vectors, delimiter = \",\")\n",
    "# np.savetxt(\"XTestMNIST.csv\", x_test_vectors, delimiter = \",\")\n",
    "# np.savetxt(\"yTrainMNIST.csv\", y_train, delimiter=\",\")\n",
    "# np.savetxt(\"yTestMNIST.csv\", y_test, delimiter=\",\")\n",
    "np.savez_compressed(\"dataMNISTNeuralNetwork\", XTrain=x_train_vectors, XTest=x_test_vectors,\n",
    "                   yTrain = y_train, yTest = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b2d8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20004, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c8b0f59",
   "metadata": {},
   "source": [
    "## Implementing the cost function\n",
    "\n",
    "We'll be working with the same network architecture as yesterday (although slightly more inputs): a Hidden Layer (HL) with 25 nodes, and 10 output neurons (predicting each of the 10 possible digits).\n",
    "![NN](NeuralNetwork.png)\n",
    "\n",
    "\n",
    "First things first: we'll need to calculate the cost at the end of the network. We'll use the categorical cross-entropy, whose formula is: <br>\n",
    "![CategoricalCrossEntropyFormula](CategoricalCrossEntropy.png) <br> <br>\n",
    "\n",
    "Let's make a function that, ultimately, will return the gradients for every weight in our 2-layer neural network. We might want to use `fmin_bfgs` or another minimisation function on the weights (rather than bog-standard gradient descent). For this reason, we will make the function take in a list of all the thetas of the network. That's what these methods want: a single list/array of gradients. We internally turn this array back into the matrices $\\Theta_1$ and $\\Theta_2$. No stress: that's done for you. Your job: reimplement forward propagation in the function below, then calculate the cost with categorical cross-entropy. So:\n",
    "\n",
    "* Implement forward propagation in the function below. This is the same as yesterday.\n",
    "* After forward propagation, calculate the mean cost (1/m * total cost) for all the forward-propagated samples. There's no looping involved!\n",
    "\n",
    "Hints:\n",
    "* Don't forget to use `mySigmoid()`.\n",
    "* First experiment outside the function body before putting things into the function!\n",
    "* With the seed we've set for our rng, you should get a mean cost of 6.84 for these random thetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "154d2c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8477887682042775"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = default_rng(42)\n",
    "thetaOneMatrix = rng.uniform(-0.12, 0.12, size = (25, 785))\n",
    "thetaTwoMatrix = rng.uniform(-0.12, 0.12, size = (10, 26))\n",
    "\n",
    "nnThetas       = np.append(np.ravel(thetaOneMatrix), np.ravel(thetaTwoMatrix))\n",
    "inputLayerSize = 784\n",
    "hiddenLayerSize = 25\n",
    "classLabels = 10\n",
    "X = x_train_vectors\n",
    "y = y_train\n",
    "\n",
    "def nnCostFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10):\n",
    "    \n",
    "    m = len(X)\n",
    "    \n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #Your turn! Implement forward propagation below, then calculate the cost for the forward_propagated samples\n",
    "    #Feel free to remove the def nnCostfunction: -part and just play with the steps in this cell or a new cell below\n",
    "        #and to then later assemble it back into a function\n",
    "    \n",
    "    return None #change this to return the cost!\n",
    "\n",
    "#answer\n",
    "\n",
    "def nnCostFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10):\n",
    "   \n",
    "    m = len(X)\n",
    "    \n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #calculating the forward pass\n",
    "    inputs        = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    weightedSumHL = inputs @ thetaOneMatrix.T\n",
    "    activationHL  = mySigmoid(weightedSumHL)\n",
    "    \n",
    "    inputsOL      = np.c_[np.ones(shape = (len(activationHL), 1)), activationHL]\n",
    "    weightedSumOL = inputsOL @ thetaTwoMatrix.T\n",
    "    activationsOL = mySigmoid(weightedSumOL)\n",
    "    \n",
    "    #calculating the cost (skip the explanation to the 1 line of code if you want!)\n",
    "    #remember: log 1 = 0.\n",
    "    #log 0 = -Inf\n",
    "    #So: y is, for example [1,0,0,0,0,0,0,0,0,0]\n",
    "    #activations for this sample are, for instance: [0.8, 0.133, 0,0,0,0,0,0,0,0]\n",
    "    #np.log(activations) = [-0.223, -2.02, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf]\n",
    "    #[1,0,0,0,0,0,0,0,0,0] * [-0.223, -2.02, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf, -Inf] = [-0.223, 0, 0, 0, etc.]\n",
    "    # - [-0.223, 0, 0, 0, etc.] = [0.223, 0, 0, 0, etc.] --> so 0.223 cost due to not correctly predicting the 1 but instead 0.8\n",
    "\n",
    "    #second part:\n",
    "    #1-y = [0,1,1,1,1,1,1,1,1,1]\n",
    "    #1-activationsOL = [0.2, 0.867, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    #np.log(1-activationsOL) = [-1.61, -0.143, 0,0,0,0,0,0,0,0]\n",
    "    #1-y * np.log(1-activationsOL) = [0, -0.143, 0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    #finally:\n",
    "    # [0.223, 0, 0, 0, 0, 0, 0, 0, 0, 0 ] - [0, -0.143, 0,0,0,0,0,0,0,0] = [0.223, 0.143, 0,0,0,0,0,0,0,0]\n",
    "    # Hence, for this one sample, cost would be 0.366\n",
    "    \n",
    "    #Because everything here is an array, this is automatically done for every row (every sample) and the\n",
    "    #np.sum then sums all numbers up to get to the total cost, which is then averaged by multiplying with 1/m\n",
    "    J = 1/m * np.sum((- (y * np.log(activationsOL)) - ((1-y) * np.log(1-activationsOL))))\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "nnCostFunction(nnThetas, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2db8e6",
   "metadata": {},
   "source": [
    "## Adding regularisation\n",
    "\n",
    "You probably already reckoned from the inclusion of a lambda_ argument that there was going to be some regularisation. Indeed there is. We don't want to overfit our network to the training data, and it's still a good strategy to do so by adding a penalty for too large or too small weights.\n",
    "\n",
    "The regularised cost function looks as follows:\n",
    "![RegCostFunction](RegularisedCostFunctionNN.PNG) <br>\n",
    "\n",
    "It might seem a bit daunting, but all it's saying is 'loop over every parameter for every unit in the HL and the output layer, and add the square of that number to a running sum, then scale with $\\lambda$ and divide by the number of samples'. It's no different from before.\n",
    "\n",
    "Up to you to:\n",
    "* Copy your function from above in the cell below and edit it so that it can return a regularised cost.\n",
    "* Once done, test it with a $\\lambda$ of 100. The resultant cost should be ~7.09.\n",
    "\n",
    "Hint:\n",
    "* We don't regularise the bias!\n",
    "* `np.square()` is probably your friend.\n",
    "* Remember `lambda_/2*m != lambda_/(2*m)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "586e16c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.085673822927744"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#answer\n",
    "\n",
    "def nnCostFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10):\n",
    "   \n",
    "    m = len(X)\n",
    "    \n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #calculating the forward pass\n",
    "    inputs        = np.c_[np.ones(shape = (len(X), 1)), X]\n",
    "    weightedSumHL = inputs @ thetaOneMatrix.T\n",
    "    activationHL  = mySigmoid(weightedSumHL)\n",
    "    \n",
    "    inputsOL      = np.c_[np.ones(shape = (len(activationHL), 1)), activationHL]\n",
    "    weightedSumOL = inputsOL @ thetaTwoMatrix.T\n",
    "    activationsOL = mySigmoid(weightedSumOL)\n",
    "    \n",
    "    #cost\n",
    "    J = 1/m * np.sum((- (y * np.log(activationsOL)) - ((1-y) * np.log(1-activationsOL))))\n",
    "    \n",
    "    #regularised cost\n",
    "    #remember: units in the rows, their parameters in the columns\n",
    "    #Hence, [:,1:] removes the columns with the bias term.\n",
    "    regThetaOne = np.sum(np.square(thetaOneMatrix[:,1:]))\n",
    "    regThetaTwo = np.sum(np.square(thetaTwoMatrix[:,1:]))\n",
    "    regCost     = J + (lambda_/(2*m)) * (regThetaOne + regThetaTwo)\n",
    "    \n",
    "    return regCost\n",
    "\n",
    "nnCostFunction(nnThetas, X, y, lambda_ = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbeb299",
   "metadata": {},
   "source": [
    "## Numerical gradient checking\n",
    "\n",
    "Before we get to implementing backpropagation, there is one thing we need to do to be sure that we implement it correctly: make a function that can check this for us. In this case, we do that by numerically approximating the gradient. We do this by taking our current $\\theta$ value and adding some small value $e$, and also taking our $\\theta$ value and substracting this small value $e$. Then, we can calculate: <br> <br>\n",
    "$$f(\\theta) \\approx \\frac{J(\\theta + e) - J(\\theta - e)}{2e} $$ \n",
    "\n",
    "This corresponds to the following image: ![numericalApproximationGradient](NumericalGradientComputation.PNG) <br>\n",
    "\n",
    "If this explanation leaves you unsure, Welch labs has got you covered in [this great video](https://www.youtube.com/watch?v=pHMzNW8Agq4).\n",
    "Do note that the computational cost is **exorbitant**: you have to calculate the cost for all your training samples twice per parameter in the network, and our tiny network has some ~19.500 of them. \n",
    "\n",
    "Up to you to:\n",
    "* Define a function `numericalGradientApproximation()` that takes in nnThetas, X, y, and lambda_ as arguments, and runs `nnCostFunction()` for the current theta +$e$ and -$e$, and divides them by 2$e$. Set $e$ to $1\\cdot10^{-4}$ \n",
    "* **NOTE**: remember that you want to do this for each $\\theta$ separately: we are calculating the partial derivative w.r.t. each parameter in the network. In other words: you should change _only one_ theta in each calculation (one time subtracting $e$ from it, one time adding $e$ to it) of the cost function and keep the rest the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa468c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer\n",
    "def numericalGradientApproximation(nnThetas, X, y, lambda_, e = 1e-4):\n",
    "    nnThetasMinusE = nnThetas-e\n",
    "    nnThetasPlusE  = nnThetas+e\n",
    "    listGradients = []\n",
    "    for index, value in enumerate(nnThetas):\n",
    "        if index % 500 == 0:\n",
    "            print(\"Parameter \" + str(index) + \"out of \" + str(len(nnThetas)))\n",
    "        minusEThetas = nnThetas; minusEThetas[index] = nnThetasMinusE[index]\n",
    "        minusECost   = nnCostFunction(minusEThetas, X, y, lambda_)\n",
    "        plusEThetas  = nnThetas; plusEThetas[index] = nnThetasPlusE[index]\n",
    "        plusECost    = nnCostFunction(plusEThetas, X, y, lambda_)\n",
    "        numericalGradApproxThisTheta = (plusECost - minusECost)/(2*e)\n",
    "        listGradients.append(numericalGradApproxThisTheta)\n",
    "    return(listGradient)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3769c80",
   "metadata": {},
   "source": [
    "## Implementing backpropagation part 1: sigmoid gradient\n",
    "\n",
    "As a first step to actually do backpropagation, your task is to implement the sigmoid gradient function in the code cell below: $sigmoid(x)*(1-sigmoid(x))$. In keeping with the age-old traditions of our people, call it `mySigmoidGradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer\n",
    "def mySigmoidGradient(x):\n",
    "    outcome = mySigmoid(x) * (1-mySigmoid(x))\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e76d6",
   "metadata": {},
   "source": [
    "## Implementing backpropagation part 2: the real deal\n",
    "\n",
    "The equations you need to implement are in this figure:\n",
    "![NNBackProp](NeuralNetworkBackprop.PNG)\n",
    "\n",
    "Concretely:\n",
    "* You can calculate the error for the output layer $\\delta^{(3)}$: that's the activations minus the labels.\n",
    "* You can use these to calculate $\\delta^{(2)}$: $(\\Theta^{(2)})^T \\cdot \\delta^{(3)} \\odot sigmoidGradient(z^{(2)}) $. Here, .* or $\\odot$ is elementwise multiplication, which is just * for numpy.\n",
    "* The remove $\\delta_0^{(2)}$ just means that you shouldn't calculate a gradient for the +1 neuron.\n",
    "* There's no error for the input layer, but you do need to change the weights of the Hidden Layer!\n",
    "\n",
    "This might seem a little bit simple, but it performs all we need to do.\n",
    "$\\delta^{(3)}$ is a (10, 1) vector containing at each position how wrong a given neuron was for a prediction (or averaged over predictions of many samples). $(\\Theta^{(2)})^T$ is a (10, 26)$^T$ = (26, 10) matrix. So 26 rows: 25 units, 10 columns containing the weights of each unit to the 10 units in the output layer, and 1 row with the 10 biases, 1 for each output layer unit. When we multiply this with the error in the output layer (10, 1), we get for each output unit's weights and the biases how their _activations_ should be changed to decrease the error. We change their activations by changing the inputs to the activation function (_weights and biases_), so we should take the derivative of the activation function for these values to find the gradients to step down to change the weights and biases so as to reduce the error in the output layer. Just to be clear: $z^{(2)}$ is the weighted sum that layer 2 generates.\n",
    "\n",
    "Up to you to:\n",
    "* Make a copy of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f767d7ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
