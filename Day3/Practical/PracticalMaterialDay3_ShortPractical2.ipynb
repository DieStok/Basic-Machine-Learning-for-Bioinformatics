{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 3\n",
    "\n",
    "Welcome to the second practical of today. Here, you'll start on the process of actually implementing backpropagation. First, preparation of some functions we will need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.optimize import fmin_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important functions\n",
    "def mySigmoid(data):\n",
    "    output = 1/(1+ np.exp(-data))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17132cfd",
   "metadata": {},
   "source": [
    "## Loading in the MNIST data\n",
    "\n",
    "We're going to be working with the MNIST data again, though now in a slighly different format of 28\\*28 = 784 pixel images, where each pixel has a brightness from 0 to 255. We change this brightness to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#load-in taken from here: https://keras.io/examples/vision/mnist_convnet/\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03775e",
   "metadata": {},
   "source": [
    "## Subsampling the training data\n",
    "\n",
    "At this point, the data is in the format of 60.000 training samples, each consisting of a 28\\*28 matrix of pixel intensities, with 1 channel (i.e. there's just 1 colour here, grey. In a colour image you would have 3 channels: red, green and blue intensities).\n",
    "\n",
    "For a dense neural network that we'll be working with, we'll just convert these 28\\*28 matrices into vectors with 784 entries. We'll also subsample the data: this amount of data is fine for optimised routines, but for our own implementation 20.000 training samples is _plenty_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbbf87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(42)\n",
    "\n",
    "print(np.unique(y_train, axis = 0, return_counts= True))\n",
    "#the training set is not quite balanced, but let's keep the proportions it has nonetheless.\n",
    "\n",
    "uniqueLabels = np.unique(y_train, axis = 0, return_counts= True)\n",
    "\n",
    "#if to make sure you don't subsample again if you rerun this cell\n",
    "if np.sum(uniqueLabels[1]) >=20010:\n",
    "    listChoiceIndices = []\n",
    "    for index, label in enumerate(uniqueLabels[0]):\n",
    "        print(\"Subsetting for label: \" + str(label))\n",
    "        nToPick = np.ceil(1/3*uniqueLabels[1][index]).astype(int)\n",
    "        indicesToPick = np.where(np.all(y_train == label, axis = 1))\n",
    "        choiceIndices = rng.choice(indicesToPick[0], size = nToPick, replace = False)\n",
    "        listChoiceIndices.append(choiceIndices)\n",
    "    choiceIndices = [val for sublist in listChoiceIndices for val in sublist]\n",
    "\n",
    "    x_train, y_train = x_train[choiceIndices], y_train[choiceIndices]\n",
    "    #convert matrices into vectors\n",
    "\n",
    "x_train_vectors = np.vstack([np.ravel(elem) for elem in x_train])\n",
    "x_test_vectors  = np.vstack([np.ravel(elem) for elem in x_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b0f59",
   "metadata": {},
   "source": [
    "## Implementing the cost function\n",
    "\n",
    "We'll be working with the same network architecture as yesterday (although slightly more inputs): a Hidden Layer (HL) with 25 nodes, and 10 output neurons (predicting each of the 10 possible digits).\n",
    "![NN](NeuralNetwork.png)\n",
    "\n",
    "\n",
    "First things first: we'll need to calculate the cost at the end of the network. We'll use the categorical cross-entropy, whose formula is: <br>\n",
    "![CategoricalCrossEntropyFormula](CategoricalCrossEntropy.png) <br> <br>\n",
    "\n",
    "Let's make a function that, ultimately, will return the gradients for every weight in our 2-layer neural network. We might want to use `fmin_bfgs` or another minimisation function on the weights (rather than bog-standard gradient descent). For this reason, we will make the function take in a list of all the thetas of the network. That's what these methods want: a single list/array of gradients. We internally turn this array back into the matrices $\\Theta_1$ and $\\Theta_2$. No stress: that's done for you. Your job: reimplement forward propagation in the function below, then calculate the cost with categorical cross-entropy. So:\n",
    "\n",
    "* Implement forward propagation in the function below. This is the same as yesterday.\n",
    "* After forward propagation, calculate the mean cost (1/m * total cost) for all the forward-propagated samples. There's no looping involved!\n",
    "\n",
    "Hints:\n",
    "* Don't forget to use `mySigmoid()`.\n",
    "* First experiment outside the function body before putting things into the function!\n",
    "* With the seed we've set for our rng, you should get a mean cost of 6.84 for these random thetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(42)\n",
    "thetaOneMatrix = rng.uniform(-0.12, 0.12, size = (25, 785))\n",
    "thetaTwoMatrix = rng.uniform(-0.12, 0.12, size = (10, 26))\n",
    "\n",
    "nnThetas       = np.append(np.ravel(thetaOneMatrix), np.ravel(thetaTwoMatrix))\n",
    "inputLayerSize = 784\n",
    "hiddenLayerSize = 25\n",
    "classLabels = 10\n",
    "X = x_train_vectors\n",
    "y = y_train\n",
    "\n",
    "def nnCostFunction(nnThetas, X, y, lambda_ = 0, inputLayerSize = 784, hiddenLayerSize = 25, classLabels = 10):\n",
    "    \n",
    "    m = len(X)\n",
    "    \n",
    "    #reshaping the list of parameters to matrices\n",
    "    hiddenLayerParamNr    = (hiddenLayerSize * (inputLayerSize+1))\n",
    "    thetaOneMatrix        = np.reshape(nnThetas[0:hiddenLayerParamNr],\n",
    "                                       newshape = (hiddenLayerSize, inputLayerSize+1))\n",
    "    outputLayerParamStart = hiddenLayerParamNr \n",
    "    thetaTwoMatrix        = np.reshape(nnThetas[outputLayerParamStart:],\n",
    "                                       newshape = (classLabels, hiddenLayerSize+1))\n",
    "    \n",
    "    #Your turn! Implement forward propagation below, then calculate the cost for the forward_propagated samples\n",
    "    #Feel free to remove the def nnCostfunction: -part and just play with the steps in this cell or a new cell below\n",
    "        #and to then later assemble it back into a function\n",
    "    \n",
    "    return None #change this to return the cost!\n",
    "\n",
    "# your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2db8e6",
   "metadata": {},
   "source": [
    "## Adding regularisation\n",
    "\n",
    "You probably already reckoned from the inclusion of a lambda_ argument that there was going to be some regularisation. Indeed there is. We don't want to overfit our network to the training data, and it's still a good strategy to do so by adding a penalty for making too many weights too large.\n",
    "\n",
    "The regularised cost function looks as follows:\n",
    "![RegCostFunction](RegularisedCostFunctionNN.PNG) <br>\n",
    "\n",
    "It might seem a bit daunting, but all it's saying is 'loop over every parameter for every unit in the HL and the output layer, and add the square of that number to a running sum, then scale with $\\lambda$ and divide by the number of samples'. It's no different from before.\n",
    "\n",
    "Up to you to:\n",
    "* Copy your function from above in the cell below and edit it so that it can return a regularised cost.\n",
    "* Once done, test it with a $\\lambda$ of 100. The resultant cost should be ~7.09.\n",
    "\n",
    "Hint:\n",
    "* We don't regularise the bias!\n",
    "* `np.square()` is probably your friend.\n",
    "* Remember `lambda_/2*m != lambda_/(2*m)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbeb299",
   "metadata": {},
   "source": [
    "## Numerical gradient checking\n",
    "\n",
    "Before we get to implementing backpropagation, there is one thing we need to do to be sure that we implement it correctly: make a function that can check this for us. In this case, we do that by numerically approximating the gradient. We do this by taking our current $\\theta$ value and adding some small value $e$, and also taking our $\\theta$ value and substracting this small value $e$. Then, we can calculate: <br> <br>\n",
    "$$f(\\theta) \\approx \\frac{J(\\theta + e) - J(\\theta - e)}{2e} $$ \n",
    "\n",
    "This corresponds to the following image: ![numericalApproximationGradient](NumericalGradientComputation.PNG) <br>\n",
    "\n",
    "If this explanation leaves you unsure, Welch labs has got you covered in [this great video](https://www.youtube.com/watch?v=pHMzNW8Agq4).\n",
    "Do note that the computational cost is **exorbitant**: you have to calculate the cost for all your training samples twice per parameter in the network, and our tiny network has some ~19.500 of them. \n",
    "\n",
    "Up to you to:\n",
    "* Define a function `numericalGradientApproximation()` that takes in nnThetas, X, y, and lambda_ as arguments, and runs `nnCostFunction()` for the current theta +$e$ and -$e$, and divides them by 2$e$. Set $e$ to $1\\cdot10^{-4}$ \n",
    "* **NOTE**: remember that you want to do this for each $\\theta$ separately: we are calculating the partial derivative w.r.t. each parameter in the network. In other words: you should change _only one_ theta in each calculation (one time subtracting $e$ from it, one time adding $e$ to it) of the cost function and keep the rest the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa468c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3769c80",
   "metadata": {},
   "source": [
    "## Implementing backpropagation part 1: sigmoid gradient\n",
    "\n",
    "As a first step to actually do backpropagation, your task is to implement the sigmoid gradient function in the code cell below: $sigmoid(x)*(1-sigmoid(x))$. In keeping with the age-old traditions of our people, call it `mySigmoidGradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f520239",
   "metadata": {},
   "source": [
    "## What I want you to remember here:\n",
    "* What the (regularised) cost function for neural networks is and how to implement it.\n",
    "* That you can also approximate gradients numerically, but that this is _very_ computationally expensive. Prohibitively so, which is why we only use it to check ~ourselves before we wreck ourselves~.\n",
    "* That the derivative of the sigmoid function is what it is.\n",
    "* That [banana ketchup](https://en.wikipedia.org/wiki/Banana_ketchup) is a thing.\n",
    "\n",
    "## The end\n",
    "Well done, regularised cost function: check, sigmoid function: check, hotel: Trivago. Now it's time for convolutional neural networks and a guest talk before we move on to the afternoon practical where you'll actually implement backpropagation!\n",
    "\n",
    "## Survey\n",
    "[BAM](https://docs.google.com/forms/d/e/1FAIpQLScmJpZ3voXHZrTR4KjJbreJH5-IdIjyNBBfhf7bw0GigpmMdw/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
