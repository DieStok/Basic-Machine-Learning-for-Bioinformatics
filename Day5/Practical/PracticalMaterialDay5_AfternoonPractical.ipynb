{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Afternoon practical day 5\n",
    "\n",
    "Welcome to the final practical of today. We'll do a few things:\n",
    "* Use K-means clustering to reduce the dimensionality of images (compression)\n",
    "* Use PCA to get some insight into a high-dimensional dataset\n",
    "* Use PCA together with linear regression to get a more proper GWAS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.optimize import fmin_bfgs, fmin_cg, fmin\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "import itertools\n",
    "import xarray\n",
    "import scipy\n",
    "from pandas_plink import read_plink1_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important functions\n",
    "\n",
    "def calcEucliDist(vectorOne, vectorTwo):\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, axis = 1)\n",
    "\n",
    "def calcAbsDist(vectorOne, vectorTwo):\n",
    "    #using linalg.norm:\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, ord = 1, axis = 1)\n",
    "\n",
    "def makeKMeanClusters(X, k, funName = \"calcEucliDist\", maxIter = 50, nClusteringsToPerform = 20):\n",
    "    if k <= 0:\n",
    "        print(\"K must be greater than 0!\")\n",
    "        return None\n",
    "    if k > len(X):\n",
    "        print(\"K cannot be larger than the # of samples in your data!\")\n",
    "        return None\n",
    "    if maxIter <= 0:\n",
    "        print(\"Cannot have negative or 0 iterations!\")\n",
    "        return None\n",
    "    \n",
    "    resultToReturn = [None, None, None, None]\n",
    "    bestDistortion = np.Inf\n",
    "    \n",
    "    for clusteringIndex in range(0, nClusteringsToPerform):\n",
    "        print(\"Random initialisation: \" + str(clusteringIndex+1))\n",
    "        initialCentroids   = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "        if len(initialCentroids) != k:\n",
    "            print(\"Centroids lost!\")\n",
    "        centroids          = initialCentroids\n",
    "        threeLastCentroids = []\n",
    "        #print(centroids)\n",
    "        for i in range(0, maxIter):\n",
    "            print(\"iteration \" + str(i+1))\n",
    "\n",
    "            threeLastCentroids.append(np.round(centroids, 4))\n",
    "            distancesToCentroids = np.vstack([globals()[funName](centroids, datapoint) for datapoint in X])\n",
    "\n",
    "            #could be equidistant from two centroids. Then need to choose. So:\n",
    "            closestCentroidsPerDataPoint      = distancesToCentroids == np.amin(distancesToCentroids,\n",
    "                                                                            axis = 1)[:, np.newaxis]\n",
    "            # this checks for every row whether there's >1 cluster to which you are closest.\n",
    "            # if so, just assign to 1 of those randomly, and remove the other.\n",
    "            for index, row in enumerate(closestCentroidsPerDataPoint):\n",
    "                if np.sum(row) > 1:\n",
    "                    coords = np.where(row == True)[0]\n",
    "                    #print(coords)\n",
    "                    randomChoiceAssignCentroid = np.random.choice(coords, replace = False)\n",
    "                    closestCentroidsPerDataPoint[index,0:len(row)] = False\n",
    "                    closestCentroidsPerDataPoint[index,randomChoiceAssignCentroid] = True\n",
    "            closestCentroid = np.where(closestCentroidsPerDataPoint)[1]\n",
    "\n",
    "            centroids            = np.vstack([np.mean(X[np.where(closestCentroid == clusterNum)],\n",
    "                                                      axis = 0) for clusterNum in np.unique(closestCentroid)])\n",
    "\n",
    "            if i >2:\n",
    "                threeLastCentroids.pop(0)\n",
    "                if np.array_equal(threeLastCentroids[-1],threeLastCentroids[-2]) and np.array_equal(threeLastCentroids[-2], threeLastCentroids[-3]):\n",
    "                    print(\"No changes in cluster centroids detected in last 3 iterations. Finished at iteration \" + str(i+1) + \".\")\n",
    "                    break\n",
    "        \n",
    "        # new code\n",
    "        squareDistancesPerPoint = []\n",
    "        for index, centroid in enumerate(closestCentroid):\n",
    "            squareDistancesPerPoint.append(np.square(centroids[centroid, :] - X[index, :]))\n",
    "        distortion = 1/len(X) * np.sum(np.array(squareDistancesPerPoint))\n",
    "        \n",
    "        if distortion < bestDistortion:\n",
    "            bestDistortion = distortion\n",
    "            resultToReturn = [centroids, closestCentroid, initialCentroids, bestDistortion]\n",
    "                \n",
    "    return resultToReturn\n",
    "\n",
    "def hierarCluster(X, distanceFunc = \"calcEucliDist\", linkageMethod = \"average\", displayDistMatrix = False):\n",
    "    \n",
    "    if linkageMethod not in [\"average\", \"complete\", \"single\"]:\n",
    "        print(\"Error, please input a valid linkage method!\")\n",
    "        return None\n",
    "    if distanceFunc  not in globals().keys():\n",
    "        print(\"Error, please input a valid distance function name!\")\n",
    "    \n",
    "    # make an empty distance matrix\n",
    "    distanceMatrix = np.zeros(shape = (len(X), len(X)))\n",
    "    distanceMatrix.fill(np.nan)\n",
    "    # make a list with the indices of every data point. This is the list of clusters, where you start\n",
    "    # with every point in a cluster and then start merging them.\n",
    "    initialList = [[index] for index, _ in enumerate(X)]\n",
    "    clusterList = initialList.copy()\n",
    "    clusteringOverIterations = []\n",
    "    clusteringOverIterations.append(initialList)\n",
    "    # also make an empty list that saves which cluster indices were merged for every iteration\n",
    "    clusterIndicesMergedList = []\n",
    "    for rowIndex, row in enumerate(distanceMatrix):\n",
    "        for colIndex, cellValue in enumerate(row):\n",
    "            # distance from yourself to yourself is 0, don't calculate!\n",
    "            if colIndex == rowIndex:\n",
    "                continue\n",
    "            # in the first loop, you calculate distance from 1 to 2.\n",
    "            # in the second loop, you don't want to calculate distance from 2 to 1 again. This safeguards against that.\n",
    "            if colIndex < rowIndex:\n",
    "                continue\n",
    "\n",
    "            distanceMatrix[rowIndex, colIndex] = globals()[distanceFunc](X[rowIndex,:][np.newaxis,  :],\n",
    "                                                                             X[colIndex, :][np.newaxis, :])\n",
    "    if displayDistMatrix:\n",
    "        display(pd.DataFrame(distanceMatrix))\n",
    "\n",
    "    # We continue clustering until everything is in one giant cluster. Thats len(X)-1 clustering steps.\n",
    "    for i in range(0, len(X)-1):\n",
    "        # we start with no idea of which two clusters we need to cluster\n",
    "        lowestDistDatapoints = None\n",
    "        # since we haven't calculated any distance, our current distance is infinite\n",
    "        distToCluster = np.Inf\n",
    "        # clusterList initially looks like [[0], [1], ... [99]].\n",
    "        # itertools.combinations makes that into [([0], [1]), ([0], [2]), ([0], [3]) ... ([1], [2]), ([1], [3])... (98, 99)]\n",
    "        # so you get all possible combinations of clusters that you could cluster together\n",
    "        for combo in itertools.combinations(clusterList, 2):\n",
    "\n",
    "            distance = 0\n",
    "            distanceSingleLink = np.Inf # need this because for single linkage you want lowest distance to be selected\n",
    "                                        # so need to have the starting distance always be lower.\n",
    "            # make all combinations of data points in the first cluster and data points in the second cluster\n",
    "            # so if the current combo = ([0, 12, 15], [3, 2]), this results in:\n",
    "            # [[0, 3], [0, 2], [12, 3], [12, 2], [15, 3], [15,2]]: these are all the points that we need to get\n",
    "            # the distances for (and average for average linkage)\n",
    "            toIterate = [j for i in [list(zip([elem] * len(combo[1]), combo[1] )) for elem in combo[0]] for j in i]\n",
    "            for indicesTwoDatapoints in toIterate:\n",
    "                #sort the indices. Our matrix has only the distance between 1 and 2, not between 2 and 1.\n",
    "                #this turns [12, 2] from above into [2, 12], etc.\n",
    "                indicesTwoDatapoints = sorted(indicesTwoDatapoints)\n",
    "\n",
    "                # keep a running total of all distances between the points in the two clusters\n",
    "                if linkageMethod == \"average\":\n",
    "                    distance += distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "                if linkageMethod == \"complete\":\n",
    "                    # for a cluster, if the distance between two points is larger than the current largest distance\n",
    "                    # between points in a cluster, that is the new cluster distance.\n",
    "                    if distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]] > distance:\n",
    "                        distance = distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "                if linkageMethod == \"single\":\n",
    "                    if distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]] < distanceSingleLink:\n",
    "                        distanceSingleLink = distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "\n",
    "            if linkageMethod == \"average\":\n",
    "                totalAvgDistance = distance/(len(combo[0]) * len(combo[1]))\n",
    "\n",
    "            # if distance between these clusters is less than the lowest distance we have seen so far,\n",
    "            #set these clusters as the ones to cluster. \n",
    "                if totalAvgDistance < distToCluster:\n",
    "                    distToCluster       = totalAvgDistance\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "            if linkageMethod == \"complete\":\n",
    "                if distance < distToCluster:\n",
    "                    distToCluster       = distance\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "            if linkageMethod == \"single\":\n",
    "                if distanceSingleLink < distToCluster:\n",
    "                    distToCluster       = distanceSingleLink\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "        #make a new list of clusters\n",
    "        clusterIndicesMergedList.append(dataPointsToCluster)\n",
    "        clusterList = clusterList.copy()\n",
    "        for index, elem in enumerate(clusterList):\n",
    "            # merge the second cluster into the first cluster\n",
    "            if elem == dataPointsToCluster[0]:\n",
    "                clusterList[index] = clusterList[index] + dataPointsToCluster[1]\n",
    "                #clusterList2[index] = sorted(clusterList[index])\n",
    "            # remove the separate second cluster (it's now been merged to the first one)    \n",
    "            if elem == dataPointsToCluster[1]:\n",
    "                clusterList.pop(index)\n",
    "        # Finally, save all clusters, from the very beginning (all separate clusters) until the very end (all in one cluster) in one list by appending to that the current clusters      \n",
    "        clusteringOverIterations.append(clusterList)\n",
    "        \n",
    "        #addition to make a list of lists of everything:\n",
    "        \n",
    "    \n",
    "    return [clusteringOverIterations, pd.DataFrame(distanceMatrix), clusterIndicesMergedList]\n",
    "\n",
    "\n",
    "def drawHierarchicalClustering(hierarClusterOutcome, figsize = (25,8), title = \"Plot\", labels = None):\n",
    "    clusterListX       = hierarClusterOutcome[0]\n",
    "    clusteredPerStepX  = hierarClusterOutcome[2]\n",
    "    xLabels            = np.array(list(itertools.chain(*clusterListX[-1])))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.set_xticks(range(0, len(xLabels)))\n",
    "    if not labels is None:\n",
    "        labels = np.array(labels)\n",
    "        if len(labels) == len(xLabels):\n",
    "            labels = labels[xLabels]\n",
    "            ax.set_xticklabels(labels, rotation = 90)      \n",
    "        else:\n",
    "            print(\"Labels supplied should be of same length as the amount of data points!\")\n",
    "            return None\n",
    "    else:   \n",
    "        ax.set_xticklabels(xLabels)\n",
    "    ax.margins(y=0)\n",
    "\n",
    "    heightPerDataPointPreviousStep = np.array([0] * len(xLabels))\n",
    "    for i, clusterStep in enumerate(clusteredPerStepX):\n",
    "        pos1Positions = np.array([np.where(xLabels == elem)[0] for elem in clusterStep[0]])\n",
    "        pos1Avg       = np.mean(pos1Positions)\n",
    "        #pos1Start     = np.min(pos1Positions)\n",
    "        #pos1End       = np.max(pos1Positions)\n",
    "        pos1ClustSize = len(pos1Positions)\n",
    "        pos2Positions = np.array([np.where(xLabels == elem)[0] for elem in clusterStep[1]])\n",
    "        pos2Avg       = np.mean(pos2Positions)\n",
    "        #pos2Start     = np.min(pos2Positions)\n",
    "        #pos2End       = np.max(pos2Positions)\n",
    "        pos2ClustSize = len(pos2Positions)\n",
    "\n",
    "\n",
    "\n",
    "        heightEnd   = max(pos1ClustSize, pos2ClustSize)\n",
    "        ax.plot([pos1Avg, pos1Avg], [heightPerDataPointPreviousStep[pos1Positions[0][0]],heightEnd], color = \"black\")\n",
    "        ax.plot([pos2Avg, pos2Avg], [heightPerDataPointPreviousStep[pos2Positions[0][0]],heightEnd], color = \"black\")\n",
    "        ax.plot([pos1Avg, pos2Avg], [heightEnd,heightEnd], color = \"black\")\n",
    "\n",
    "        heightPerDataPointPreviousStep[np.ravel(pos1Positions)] += heightEnd - heightPerDataPointPreviousStep[pos1Positions[0][0]]\n",
    "        heightPerDataPointPreviousStep[np.ravel(pos2Positions)] += heightEnd - heightPerDataPointPreviousStep[pos2Positions[0][0]]\n",
    "\n",
    "    ax.set_ylim(0, max(heightPerDataPointPreviousStep)+1)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    plt.show(fig)\n",
    "    \n",
    "    \n",
    "def linAlgRegHypothesis(data, thetas):\n",
    "    #print(thetas)\n",
    "    data = np.array(data)\n",
    "    oneFeatToAdd = np.ones(len(data))\n",
    "    newFeatArray = np.c_[oneFeatToAdd, data]\n",
    "    predictions = newFeatArray @ thetas\n",
    "    return predictions\n",
    "\n",
    "def linAlgGradientDescent(x, y, thetas, alpha) :\n",
    "    m = len(x)\n",
    "    if thetas.ndim != 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    preds  = linAlgRegHypothesis(x, thetas)\n",
    "    if preds.shape != (m, 1):\n",
    "        preds  = preds[:, np.newaxis]\n",
    "    if y.ndim != 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation  = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    finalGradientSteps = alpha/m * gradientSummation\n",
    "    newThetas          = thetas - finalGradientSteps.T\n",
    "    \n",
    "    return newThetas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3843b",
   "metadata": {},
   "source": [
    "## K-means for image compression\n",
    "\n",
    "It is not so that dimension reduction and clustering are entirely unrelated. If you have 100 samples with 10 features each, and you believe (after the iterative clustering workflow) that you can really cluster the data clearly into, say, 3 clusters, then you can use the cluster centroids to represent the data and in principle can thus compress the differences you are interested in into these three clusters.\n",
    "\n",
    "A nice visual way of looking at this is image compression using K-means clustering. Pixels in images all have 3 features, their R, G and B values, which normally range between 0 and 255 inclusive. If we cluster all these pixels into, say, 32 clusters, we can then replace each pixel with the value of the centroid of the cluster it belongs to. This can really compress an image while still showing the most important features. \n",
    "\n",
    "Below, I load in an image of a cat, show it, and show a 3D plot of (some of the >3 million) pixel values. Go to the next cell to read what you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dce6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "catPic = plt.imread(\"CatImage.jpg\")\n",
    "plt.imshow(catPic)\n",
    "\n",
    "rValues = np.ravel(catPic[:,:,0])[:,np.newaxis]\n",
    "gValues = np.ravel(catPic[:,:,1])[:,np.newaxis]\n",
    "bValues = np.ravel(catPic[:,:,2])[:,np.newaxis]\n",
    "featMatCat = np.hstack([rValues, gValues, bValues])\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "random.seed(42)\n",
    "indicesToPlot = random.sample(range(0,len(featMatCat)), len(featMatCat))\n",
    "ax.scatter(featMatCat[indicesToPlot,0], featMatCat[indicesToPlot,1], featMatCat[indicesToPlot,2], alpha = 0.5)\n",
    "ax.set_xlabel(\"Red\"); ax.set_xlim((0,255))\n",
    "ax.set_ylabel(\"Green\"); ax.set_ylim((0,255))\n",
    "ax.set_zlabel(\"Blue\"); ax.set_zlim((0,255))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c84f9e",
   "metadata": {},
   "source": [
    "## Doing compression with K-means\n",
    "Okay, now it's up to you to:\n",
    "* Perform K-means clustering on the data, using K = 32, and using K = 16. I recommend you use sklearn's implementation: `KMeans`(see [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)). I did it with our own K-means implementation, but that really drove home the point of how K-means for large datasets requires 1. a _very_ efficient implementation and 2. a smart way of picking the initial centroids. Otherwise it takes a **_long_** time. Don't do what I did, use sklearn!\n",
    "* Make a new `featMatCat` (call it whatever you want). Here, replace each pixel value with the value of its closest centroid.\n",
    "* Now, [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html#) this new matrix in the shape of `catPic`. \n",
    "* Then, use `np.round(yourMatrix).astype(int)`. This makes sure you get integers between 0-255. The centroid coordinates are often floats, but a pixel intensity of 55.675 is not defined, for instance!\n",
    "* Finally, use `plt.imshow()` and `plt.show()` to show the compressed picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb4f8f",
   "metadata": {},
   "source": [
    "## What you see\n",
    "If all goes well, you'll see that the image becomes more gray, and the background colours become more blurry and chaotic, but that otherwise much of the detail is kept. If you were to save this to disk, you'd need a dictionary with the 16 or 32 centroid values, and then each pixel would just refer to one of those values. That could change the file size by a factor ~6 for the 16 colour case, which is pretty nice if you want to store thousands of images. You can probably imagine that, nevermind the compression, if we want to train a neural network to differentiate between cats and dogs, images compressed like this would still be plenty good enough to train it. But you'd have 6 times less disk space usage!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442dbe7",
   "metadata": {},
   "source": [
    "## Functionalising PCA and projecting back\n",
    "\n",
    "In the morning practical you got some familiarity with PCA. Now, let's implement it as a function ourselves, and also project data points back from the reduced PCA space to the original space. We won't be using svd, just `np.linalg.eig`.\n",
    "Up to you to:\n",
    "* Define a function called `doPCA` that takes in a **normalised** data matrix.\n",
    "* Have it calculate the covariance matrix using $X^T \\cdot X$\n",
    "* It should return a list of 1. ordered eigenvalues (high to low), 2. ordered eigenvectors (in the order of the eigenvalues) and 3. The PCA projection of that data matrix, without reducing the dimensions (so just the rotation of the full space, i.e. all the PCs). \n",
    "* Test it on X supplied below.\n",
    "* Plot a 3D scatterplot of the original data and the PCA data.\n",
    "\n",
    "Hints:\n",
    "* For 3D plotting, just copy the code from above and change it.\n",
    "* Use the `argsort` trick from the short practical to sort correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563db442",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "X, y = make_classification(n_samples = 100, n_features = 3, n_informative = 2,\n",
    "                           n_redundant = 0, n_classes = 2, hypercube = False, random_state=42)\n",
    "XStd  = np.std(X)\n",
    "XMean = np.mean(X)\n",
    "XNorm = (X-XMean)/XStd\n",
    "# your answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6de96e",
   "metadata": {},
   "source": [
    "## Projecting back and reducing the dimensionality\n",
    "\n",
    "What you might see in the 3D plots is...that you can't quite know whether this is the same data. It is, but you can't see it.\n",
    "\n",
    "We'd like to be able to go back from the PCA projection to the original data. If you keep all the PCs, then of course the data you get back is exactly the same: all you've done is rotated the data, making new axes that follow a certain recipe of linear combinations of the old axes. If you've reduced the dimensions, then there will be errors: you removed dimensions along which there can be variance. In the 3D case here, you map from 3D to a 2D plane, and if you try to map back to 3D then..well..there's no variation along the 3rd dimension in your reconstruction, while there might have been some in the actual data.\n",
    "\n",
    "The idea, or goal in reality of course is that, while yes, you may lose 5% of the variance, if that allows you to remove 50 of 100 dimensions, that's a _pretty darn good_ deal. So, let's get to it. You need to make two functions:\n",
    "\n",
    "1. `dimReducePCA()` which takes in `projectedData`, `eigenvectors`, and `nDims`, and returns only the nDims first dimensions from the projected data, and the first nDims eigenvectors (PC recipes) corresponding to it.\n",
    "\n",
    "2. `reconstructPCA()` which takes in `projectedData` and `eigenVectors`, and returns $(eigenVectors \\cdot projectedData^T)^T$. This gives you back your original coordinates. Note: this is linear algebra, so use @.\n",
    "\n",
    "* Finally, run `reconstructPCA` on the unaltered output features from your `doPCA` function. Check that the output is equal to `XNorm` using `numpy.allclose` [click](https://numpy.org/doc/stable/reference/generated/numpy.allclose.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ac4b2",
   "metadata": {},
   "source": [
    "## Reconstructing from 2D and 1D\n",
    "\n",
    "So how are we doing when we actually do what we want to use PCA for: reducing dimensions? Let's (visually) find out!\n",
    "* Use `dimReducePCA` with `nDims=2` on the output of your `doPCA` function.\n",
    "* Then, perform `reconstructPCA` on this reduced data.\n",
    "* Finally, plot a 3D plot with the original data in blue, and the reconstructed data in red. Visually, how are you doing?\n",
    "* Do the same for dimension reduction for just one dimension, and plot these points in orange.\n",
    "\n",
    "Hints:\n",
    "* If you are confused about this reconstruction magic, read [this](https://stats.stackexchange.com/a/229093).\n",
    "* Again, copy the 3D-plotting code from above, just add a second call to `ax.scatter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea668fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5964ba",
   "metadata": {},
   "source": [
    "## What you'll see\n",
    "\n",
    "You'll see that the red points you make all lie on a 2D hyperplane within the space. This is logical: we removed a PC from the data, so we can't get that third dimension back. If we look at this hyperplane from the side, it should look something like this: ![example](exampleDimRed.png).\n",
    "With 1D projection included, you get this, with a line through the 3D space: ![example2](exampleDimRed2.png)\n",
    "\n",
    "## What we want\n",
    "With this knowledge, we can think about what we want from our PCA. We want a compression into fewer dimensions that keeps as much of the variation as possible. For each of the reconstructed points, we can calculate how wrong it is: this is just the sum of square (Euclidean) distances. So the average error of all the reconstructed points is:\n",
    "$$\\frac{1}{m}\\cdot \\sum_{i=1}^m||x^{(i)}-x^{(i)}_{reconstructed}||^2$$\n",
    "\n",
    "The total variance in the data is just the average of the sum of the distance from each point to zero:\n",
    "$$\\frac{1}{m}\\cdot \\sum_{i=1}^m||x^{(i)}||^2$$\n",
    "\n",
    "Usually we say something like: 'we want to keep 90% of the variance in the data'. So what does that translate to? Well, the reconstruction error _is_ the part of the variance that you now no longer capture: it's the variance you have lost because you have reduced your dimensions. So if you want to keep 90% of the variance, that comes out to:\n",
    "$$\\frac{\\frac{1}{m}\\cdot \\sum_{i=1}^m||x^{(i)}-x^{(i)}_{reconstructed}||^2}{\\frac{1}{m}\\cdot \\sum_{i=1}^m||x^{(i)}||^2} <= 0.10$$ Or in words: I want to choose my # of dimensions to keep _k_, _such that_ I don't lose more than 10% of the variance.\n",
    "\n",
    "Written like this, it seems like you'd have to calculate this quantity for every k, until the criterion is satisfied. Luckily for us, and as you know, this is not the case. We can simply divide the eigenvalue for a principal component (eigenvector) by the sum of eigenvalues. Then we cumulatively sum the %variance explained and choose the #PCs k such that it's >= 90%.\n",
    "\n",
    "In the code cell below, calculate the cumulative % variance explained by the 3 PCs in this sample data. [Hint](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigVals, eigVecs, projData = doPCA(XNorm)\n",
    "# your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac86a9b",
   "metadata": {},
   "source": [
    "## Application of PCA to ecological data\n",
    "\n",
    "Now, let's apply PCA to some ecological data. Below, I load in data describing vegetation plots in an Alpine location. See [here](https://www.davidzeleny.net/anadat-r/doku.php/en:data:aravo) for more info, if you want. Now, in these vegetation plots, many different species of plant can be present. As well, certain characteristics of the plot are logged: day of snowfall and incline, for instance.\n",
    "\n",
    "It is up to you to:\n",
    "* Do PCA on the data (don't forget to normalise!)\n",
    "* Plot the 2D reconstruction of the data, complete with variance explained on each PC.\n",
    "* Deduce which factors explain most of the variance in this data. For this, look into the eigenvectors of PC1 and PC2: the absolute max element(s) of these eigenvectors are apparently the variables in the original data that most contribute to the separation of these datapoints along these axes! For our purposes here, take the 2 largest values (negative or positive) in each eigenvector and see what this tells you.\n",
    "\n",
    "Hints:\n",
    "* If you want np.arrays, use `np.array(df.to_numpy(), np.float64)`. To get columns: `df.columns`. To get the sample names, use `df.iloc[:,0]`\n",
    "* If you want to exclude the column with plot names, use something like `df.iloc[:,1:]`.\n",
    "* If you can't crack how to look in the eigenvectors for the maximum absolute elements, then look in the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoData = pd.read_csv('EcologicalData.csv')\n",
    "# I need to remove the many 0 entries, otherwise the matrix becomes noninvertible and you get complex eigenvectors\n",
    "# Don't concern yourself with this further! However, it goes to show that PCA doesn't like sparse matrices. For that\n",
    "# you need sparse PCA. Won't go into that here. See: https://medium.com/principal-component-analysis-on-a-high-dimensional/https-medium-com-pcasparsematrix-545d18efff0a \n",
    "ecoData.iloc[:,1:] = ecoData.iloc[:,1:] +1\n",
    "display(ecoData.head())\n",
    "\n",
    "#your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc321573",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "\n",
    "If all goes well, you should see that PC1 mainly concerns itself with the day of snow melting and a plant called Kobresia myosuroides (Vill.) Fiori, which looks like this: ![image](Kobresia.jpg).\n",
    "\n",
    "PC2 concerns itself with Geum montanum L. and Potentilla aurea L. The upshot here is that the eigenvectors are not just 'things you need to use to project the data to lower dimensions', but that these recipes to make the new linear combinations can tell you things. In a gene expression context, they could show you the genes whose expressions vary most among your experimental conditions (and which might thus be related), in behavioural contexts you could see in what (combinations) of behaviours chimpanzees differ most from each other, etc. The loadings (eigenvectors) are an important part of getting insight from PCA.\n",
    "\n",
    "Note that, for this data, the 2 axes certainly don't capture that much of the variance so you'd want a lot more PCs in actual data for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc712c3",
   "metadata": {},
   "source": [
    "## A close relative of PCA: MDS\n",
    "\n",
    "Before we move on to applying PCA in a GWAS, I want to introduce you to a close relative of PCA that we could now also easily implement. PCA works on the covariance matrix: it says how features/dimensions in your data vary by themselves (their spread) on the diagonal, and how they vary with others (all off-diagonal entries). Multi-dimensional scaling (MDS) works on a distance matrix. Other than that, it is **exactly the same**. So, just like PCA, you take the distance matrix, and calculate eigenvalues and eigenvectors, and project down to fewer dimensions. Thus, whereas PCA makes new linear combinations of the old dimensions, where every dimension has the maximum of variance and is not correlated with each other PCA axis, so MDS works on a _distance matrix_ and tries to preserve high-dimensional distances in lower-dimensional space. It works linearly, however, unlike the graph-based and highly impressive UMAP. It turns out that **for Euclidian distance, PCA and MDS are the same**. But for other distance measures, they are distinct.\n",
    "\n",
    "* Watch the end of [this video by StatQuest](https://youtu.be/GEn-_dAyYME?t=409)\n",
    "* Read the beginning of [this Wikipedia article](https://en.wikipedia.org/wiki/Multidimensional_scaling) (i.e. only the introduction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112ba29",
   "metadata": {},
   "source": [
    "## When to use what\n",
    "\n",
    "So why would you use PCA or MDS? Well, PCA gives you uncorrelated dimensions that capture most of the variance, and works on the covariance matrix. MDS works on the basis of pairwise distances, and tries to make a geometric embedding such that these distances are preserved. We'll do PCA and MDS on some spatial data, specifically, European cities and their coordinates. Below, I perform MDS. Your job is to perform PCA on the data and compare. Up to you to:\n",
    "\n",
    "* Take the 2 features (latitude and longitude) and perform PCA on them and plot the cities in these new dimensions.\n",
    "* As labels of the points, use the city names. Use `ax.annotate` [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.annotate.html?highlight=annotate#matplotlib.axes.Axes.annotate) instead of `ax.scatter`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "cityDataEurope = pd.read_csv(\"Cities.csv\")\n",
    "display(cityDataEurope.head())\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "latLongDataOnly = cityDataEurope.iloc[:,2:4].to_numpy()\n",
    "cityNames = cityDataEurope.loc[:,\"city\"].to_numpy()\n",
    "\n",
    "embedding = MDS(n_components = 2)\n",
    "embeddedCities = embedding.fit_transform(latLongDataOnly)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,15))\n",
    "ax.set_xlim((np.min(embeddedCities[:,0]), np.max(embeddedCities[:,0])))\n",
    "ax.set_ylim((np.min(embeddedCities[:,1]), np.max(embeddedCities[:,1])))\n",
    "for index, row in enumerate(embeddedCities):\n",
    "    ax.annotate(cityNames[index], (row[0], row[1]))\n",
    "fig.suptitle(\"MDS projection. You sort of see the geography of Europe, though flipped in a way\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (15,15))\n",
    "europeMap = plt.imread(\"map-of-europe-with-capitals.jpg\")\n",
    "fig.suptitle(\"Map of Europe with capitals\")\n",
    "ax.imshow(europeMap)\n",
    "fig.show()\n",
    "# your answer here\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b41012",
   "metadata": {},
   "source": [
    "## What you see\n",
    "\n",
    "It might not look it, but these are equivalent ways of showcasing the pairwise distances in two dimensions. A hint that they're the same is, for instance, that Graz is in the middle in both maps. Of course, it doesn't look like reality, but that's because we're doing this only based on distances or correlations, not on other properties, like how it relates to the rest of the world, or from which side we usually look at our map projections and find it logical to think about them.\n",
    "\n",
    "Now PCA stays PCA, but for MDS we could make a map that shows something else. For example, say that you're a purist who only wants to travel in straight lines (we're assuming that the Earth is flat for now, emphasis on _assuming_ and not _believing_) in the 4 cardinal directions. Then we could use the L1 norm or absolute distance. Let's do that and see how close different cities are then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distMatEuropeCities = np.zeros(shape = (len(cityDataEurope), len(cityDataEurope)))\n",
    "for rowIndex, colIndex in itertools.product(list(range(0,len(distMatEuropeCities))), repeat = 2):\n",
    "    #print(latLongDataOnly[rowIndex,:][:,np.newaxis])\n",
    "    distMatEuropeCities[rowIndex, colIndex] = calcAbsDist(latLongDataOnly[rowIndex,:][np.newaxis,:],\n",
    "                                                            latLongDataOnly[colIndex,:][np.newaxis,:])\n",
    "    \n",
    "embeddingLOne = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "manhattanMDS  = embeddingLOne.fit_transform(distMatEuropeCities)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,15))\n",
    "ax.set_xlim((np.min(manhattanMDS[:,0]), np.max(manhattanMDS[:,0])))\n",
    "ax.set_ylim((np.min(manhattanMDS[:,1]), np.max(manhattanMDS[:,1])))\n",
    "for index, row in enumerate(manhattanMDS):\n",
    "    ax.annotate(cityNames[index], (row[0], row[1]))\n",
    "fig.suptitle(\"MDS projection. Now with absolute distances\")\n",
    "fig.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983c03c",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "It looks somewhat different, but not too much. In the end, what you need to remember is that there are more (linear) methods than just PCA. PCA is or making uncorrelated dimensions that keep most variance, and so perfect for something like machine learning. MDS, on the other hand, is better when you want to make a projection to fewer dimensions that respects high-dimensional distances. The hot newcomer algorithm on the scene for that (which is non-linear) is [UMAP](https://pair-code.github.io/understanding-umap/), which we won't go into. Something else to look into might be [ivis](https://bering-ivis.readthedocs.io/en/latest/).\n",
    "\n",
    "For now, however, let's return to PCA. \n",
    "\n",
    "## PCA and GWAS\n",
    "\n",
    "In our mini-GWAS on Monday, I noted that there were many simplifications and liberties used. Not only did I add _very strong_ signal manually and trim the dataset drastically, I also had a dataset that included only people of European descent. For more power and/or generalisability, ideally you have samples from many ethnic groups. However, as you might imagine, this leads to issues: when we are looking at SNPs and their effect on BMI, if a certain subpopulation is overrepresented in _cases_ (i.e. people with high BMI), then we might get spurious hits: SNPs that are just more present in that subpopulation due to genetic ancestry, but have nothing to do with the condition. The text in box 4 from [this 2006 review paper](https://www.nature.com/articles/nrg1916) explains:\n",
    "\n",
    "![PS](PopStruct.PNG)\n",
    "\n",
    "What do we expect for these subgroups? They will have rather large variance in SNPs among them: it's logical that people from South America will, on average, differ quite a lot in alleles they have at SNP sites from people from North-East Russia or Japan. With PCA, we have in our hands a technique that can easily create the dimensions along which samples vary most. Do you see where I am going with this? If we perform PCA, and include the PCA in our linear regressions in addition to the value for the actual SNP at hand (something like $\\theta_0 \\cdot 1 + \\theta_1 \\cdot PC1 + \\theta_2 \\cdot PC2 + \\theta_3 \\cdot PC3 + \\theta_4 \\cdot PC4 + \\theta_5 \\cdot SNP_{count}$), then we can fit our linear regressions _while correcting for the dominant population structure in the data_. \n",
    "\n",
    "Of course, in that case we don't care about the magnitude of $\\theta_1$ until $\\theta_4$, we are still only interested in how the numerical trait (BMI) increases or decreases with the number of opposite alleles at a certain SNP location (0,1 or 2), which is $\\theta_5$. But by including the other parameters, we can know that there's an effect of this SNP even when correcting for population structure. Newer methods may exist, so in real research situations you should search those out and/or use Plink. [Here](https://www.nature.com/articles/ng1847) is the 2006 paper that proposed this method of using PCs.\n",
    "\n",
    "In fact, population stratification is a bit of a misnomer: this will happen also if you just sequence people from some village in the vicinity of Graz versus those in, say, Utrecht. Anytime there's this shared ancestry, that confounds the generality. So we should also use something to correct for that in a European population:\n",
    "\n",
    "![image](popStructComment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419fa96e",
   "metadata": {},
   "source": [
    "## Data provenance\n",
    "\n",
    "I took the same data as I used on Monday, but rather than putting synthetic signal into random SNPs, I put signal into 20 real SNPs known to influence BMI and obesity, chief of which is the FTO gene. I got these top 20 SNPs from [a 2020 paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7720118/) analysing the applicability of SNPs found to influence BMI in the general European population (specifically, the Hungarian general population) to the Roma subpopulation in Hungary.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Below, I generate data that has signal in ~20 SNPs, out of ~200. It is up to you to:\n",
    "* Eliminate NaNs using by setting them to have the reference allele (i.e. 0). \n",
    "* Perform PCA on the feature matrix of SNPs (`X`). Plot a PCA plot of the projection of all samples (people) into 2D. Label the axes with the %variance explained. Do there seem to be clumped subpopulations of people?\n",
    "* Keep the 2 first principal components. You will use these as extra variables in your multivariate linear regression.\n",
    "* Do the linear regression for each SNP and save the resulting thetas. Don't forget normalisation. You can use the function `linAlgGradientDescent(x, y, thetas, alpha)`. You should include the PCs in the feature matrix!\n",
    "* Calculate the standard error and the p-value using the function supplied, order by magnitude (small to large) and see whether the top 14 SNPs you get are indeed the signal put in. Hopefully you should do slightly better than Monday because we are now correcting for some covariates (though not, unfortunately, for age or ...). However, if the population is very homogeneous, then that affect won't be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fd434",
   "metadata": {},
   "outputs": [],
   "source": [
    "euroGWASData = read_plink1_bin(\"..\\..\\Day1\\Practical\\HapMap_3_r3_1.bed\",\n",
    "                               \"..\\..\\Day1\\Practical\\HapMap_3_r3_1.bim\",\n",
    "                               \"..\\..\\Day1\\Practical\\HapMap_3_r3_1.fam\", verbose=True)\n",
    "\n",
    "BMISNPs = pd.read_csv(\"TwentyTopBMIAssociatedSNPs.csv\")\n",
    "display(BMISNPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "#subsetting some columns\n",
    "downsizedData = euroGWASData[:, random.sample(range(0, euroGWASData.shape[1]), 200)]\n",
    "columnsWithoutVariation = np.all(downsizedData.values == downsizedData.values[0,:], axis = 0)\n",
    "downsizedData = downsizedData[:, np.invert(columnsWithoutVariation)]\n",
    "\n",
    "# add normally distributed BMI value with mean that is somewhat healthy \n",
    "newArray = np.zeros(len(downsizedData.trait))\n",
    "downsizedDataNew = downsizedData.drop_vars(\"trait\")\n",
    "newArray = np.random.default_rng(seed = 42).normal(22.5, 1, len(newArray))\n",
    "\n",
    "# SNPs to add signal to\n",
    "BMISNPsInEuroData = np.where(np.isin(euroGWASData.snp, BMISNPs.SNP.values))[0]\n",
    "subsetSNPsForSignal = euroGWASData[:, BMISNPsInEuroData]\n",
    "print(\"SNP information Euro cohort for SNPs in the top 20 BMI-associated SNPs:\")\n",
    "display(subsetSNPsForSignal)\n",
    "\n",
    "importantSNPsInSampleData = np.isin(BMISNPs.SNP.values, euroGWASData.snp)\n",
    "BMISNPsSubsetInEuroData   = BMISNPs.iloc[importantSNPsInSampleData,:]\n",
    "print(\"SNPs out of the top 20 that are in the data: \")\n",
    "display(BMISNPsSubsetInEuroData)\n",
    "\n",
    "#concatenate xarrays\n",
    "downsizedDataNew = xarray.concat([downsizedDataNew, subsetSNPsForSignal], dim = \"variant\")\n",
    "\n",
    "# add the signal\n",
    "print(\"Original random BMI values:\")\n",
    "print(newArray)\n",
    "for colIndex in range(0, subsetSNPsForSignal.shape[1]):\n",
    "    currentSNPName = subsetSNPsForSignal[:,colIndex].snp\n",
    "    SNPEffect   = BMISNPsSubsetInEuroData.loc[BMISNPsSubsetInEuroData.SNP == currentSNPName,[\"Beta_HG\"]].values\n",
    "    SNPEffect   = np.ravel(SNPEffect)\n",
    "    twoAlleles  = np.where(subsetSNPsForSignal[:, colIndex].values == 2.)\n",
    "    oneAllele   = np.where(subsetSNPsForSignal[:, colIndex].values == 1.)\n",
    "    noMuts      = np.where(subsetSNPsForSignal[:, colIndex].values == 0.)\n",
    "    newArray[twoAlleles] += np.random.default_rng().normal(SNPEffect*2, 0.1, len(twoAlleles))\n",
    "    newArray[oneAllele]  += np.random.default_rng().normal(SNPEffect, 0.1, len(oneAllele))\n",
    "downsizedDataNew[\"BMI\"] = (\"sample\", newArray)\n",
    "print(\"Final BMI values with SNP effects:\")\n",
    "print(downsizedDataNew[\"BMI\"])\n",
    "\n",
    "X = downsizedDataNew.values #the #SNPs each person has at each position, 165 people by ~192 SNPs\n",
    "y = downsizedDataNew[\"BMI\"].values # BMI values for each person, (165,) (1D-array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468ac63",
   "metadata": {},
   "source": [
    "## Below, write your code. Repeat of coding part of assignment:\n",
    "Above, I have generated BMI values (`y`) and SNP counts (`X`). It is up to you to:\n",
    "* Eliminate NaNs using by setting them to have the reference allele (i.e. 0). \n",
    "* Perform PCA on the feature matrix of SNPs (`X`). Plot a PCA plot of the projection of all samples (people) into 2D. Label the axes with the %variance explained. Do there seem to be clumped subpopulations of people?\n",
    "* Keep the 2 first principal components. You will use these as extra variables in your multivariate linear regression.\n",
    "* Do the linear regression for each SNP and save the resulting thetas. Don't forget normalisation. You can use the function `linAlgGradientDescent(x, y, thetas, alpha)`. You should include the PCs in the feature matrix!\n",
    "* Calculate the standard error and the p-value using the function supplied, order by magnitude (small to large) and see whether the top 14 SNPs you get are indeed the signal put in. Hopefully you should do slightly better than Monday because we are now correcting for some covariates (though not, unfortunately, for age or ...). However, if the population is very homogeneous, then that affect won't be there.\n",
    "\n",
    "Hints:\n",
    "* `linAlgGradientDescent` uses `linAlgRegHypothesis`, which automatically appends a column of 1s to the front. So you don't need to do anything for $\\theta_0$ to be correctly multiplied with 1.\n",
    "* Take it in parts. First simply do the PCA like you did before. Then, make the features you'd need for 1 SNP to do the linear regression. Then implement this in the gradient descent loop (of which you are given a part) to actually perform it.\n",
    "* When using the PCs as features, make sure to normalise them as well: they too should have unit variance and mean 0! (just use `(data - np.mean(data, axis = 0))/np.std(data, axis = 0)`).\n",
    "* Look what you did on the afternoon of day 1. This is very similar. As always, if you're stuck, check the answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: since you'll be doing multivariate linear regression, you need to specify where in the theta vector\n",
    "# the beta (slope) is. \n",
    "# For example, if you do your linear regression as:\n",
    "# 1 * theta0 + SNPcount * theta1 + PC1 * theta2 + PC3 * theta3, then the posBeta should be 1.\n",
    "# If you don't fill this in, you'll get p-values for the 2 PCs too. Also fine, just need to select the appropriate ones later.\n",
    "\n",
    "X        = downsizedDataNew.values #the #SNPs each person has at each position, 165 people by ~192 SNPs\n",
    "SNPNames = downsizedDataNew.snp.values # name for each column of X\n",
    "y        = downsizedDataNew[\"BMI\"].values # BMI values for each person, (165,) (1D-array)\n",
    "\n",
    "def calcPValuesRegSlope(X, y, thetas, posBeta = None):\n",
    "    \"\"\"Skips calculation for the significance of the intercept (i.e. it being sig. different from 0,\n",
    "    as it's the relation between BMI and the SNP we are interested in. Assumes theta as a vector (2D numpy array) of\n",
    "    n_thetas by 1 (so 2 rows, 1 column for univariate linear regression)\"\"\"\n",
    "    m = len(X)\n",
    "    if posBeta is None:\n",
    "        #calculate p-values for every theta except the intercept\n",
    "        thetasToCalc = thetas[1:,:]\n",
    "    else:\n",
    "        thetasToCalc = thetas[posBeta,:]\n",
    "    preds = linAlgRegHypothesis(X, thetas)\n",
    "    #print(preds)\n",
    "    #print(y)\n",
    "    if not y.ndim > 1:\n",
    "        y = y[:, np.newaxis]\n",
    "    #print(X)\n",
    "\n",
    "    sumSquareErrorsPred = np.sum(np.square(np.array(preds) - y))\n",
    "    stdErrors = []; tStats = []; pVals = []\n",
    "    for index, theta in enumerate(thetasToCalc):\n",
    "        #print(theta) ;print(index)\n",
    "        sumSquaresFeature = np.sum(np.square(X - np.mean(X)))\n",
    "        stdError = np.sqrt((1/(m-2)) * (sumSquareErrorsPred/sumSquaresFeature))\n",
    "        tStat    = (theta/stdError)[0]\n",
    "        pVal     = scipy.stats.t.sf(abs(tStat), m-1)\n",
    "        stdErrors.append(stdError); tStats.append(tStat); pVals.append(pVal)\n",
    "    #print(stdErrors)\n",
    "    #print(tStats)\n",
    "    #print(pVals)\n",
    "    df = pd.DataFrame(np.vstack([stdErrors, tStats, pVals]))\n",
    "    df.set_index(pd.Index([\"standard errors\", \"t statistics\", \"p-values\"]), inplace = True)\n",
    "    return[pVals, df]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# your answer here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start for fitting the data with linear regression\n",
    "startThetas = np.array([[0], [0], [0], [0]])\n",
    "nSteps      = 30\n",
    "#Normalise the outcome measure!\n",
    "\n",
    "alpha       = 0.3 \n",
    "listThetasForEachSNP = []\n",
    "\n",
    "for SNPIndex in range(0, X.shape[1]):\n",
    "    \n",
    "    valsThisSNP = X[:, SNPIndex]\n",
    "\n",
    "    currentThetas = startThetas\n",
    "    for step in range(0, nSteps):\n",
    "        #do something\n",
    "        break\n",
    "    break\n",
    "\n",
    "    \n",
    "    \n",
    "#      you can adapt this commented out code for calculating the p-values\n",
    "\n",
    "# get p-values\n",
    "# pValsPerSNP = []; listDFs = []\n",
    "# for SNPindex, thetas in enumerate(listThetasForEachSNP):\n",
    "#     pVal, dfInfo = calcPValuesRegSlope(np.c_[normSNPCounts[:, SNPindex], normPCs], normBMI, thetas)\n",
    "#     pValsPerSNP.append(pVal)\n",
    "#     listDFs.append(dfInfo)\n",
    "\n",
    "# # we're only really interested in the p-values for the slopes. For me that is theta 1, which is the first element of \n",
    "# # the pVal list you get as output from calcPValuesRegSlope\n",
    "\n",
    "# pValSlopesOnly = [pValList[0] for pValList in pValsPerSNP]\n",
    "# addPValsToDf   = pd.DataFrame(np.array(pValSlopesOnly)[np.newaxis,:]); addPValsToDf.columns = SNPNames; addPValsToDf.set_index(pd.Index([\"pValTheta1\"]), inplace = True)\n",
    "# print(\"fitted parameters for each SNP with p-values for theta 1 (slope):\")\n",
    "# display(dataFrameThetas.append(addPValsToDf))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aac81b",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "In the end, you don't perform _that_ much better, or perhaps not even better at all because in this small sample data the effect of population genotypic variation/clustering due to shared descent in different groups is not strong at all. However, what you _have_ done is combined your knowledge of gradient descent, linear regression, and unsupervised learning to make a more methodically sound (although, on the whole, still  _a good deal less sound_ than real GWAS studies, it needs be said) GWAS analysis of SNPs influencing BMI.\n",
    "\n",
    "Of course, PCA has many more uses in the ML context rather than only removing covariates in GWAS. Its chief use is decreasing dimensionality, as you have seen. This is a very important function, guarding against overfitting, and making visualisations possible. In fact, more advanced tools like t-SNE and UMAP can benefit from first reducing the dimensionality to, say, 100 dimensions using PCA, and _then_ using them. PCA is not outdated! For a nice slide deck on PCA, t-SNE and UMAP, see [here](https://www.bioinformatics.babraham.ac.uk/training/10XRNASeq/Dimension%20Reduction.pdf).\n",
    "\n",
    "## What I'd like you to remember here\n",
    "* That clustering can also be used for compression, particularly of images. However, you could also imagine data that is, for your purposes, pretty well described by just the values for 20 clusters of points, rather than the full feature matrix of 100 features for 1000 samples each, let's say. This is a relatively minor point though. The real star is:\n",
    "* How PCA works: normalising data, calculating the covariance matrix, rotating the full data to new axes which are linear combinations of the old dimensions. This minimises the projection error, and maximises the variance in each axis. Under the hood, this is done with eigenvectors and eigenvalues, or SVD.\n",
    "* That a user can then subselect any number of dimensions he or she wishes to keep, often using a cut-off of a certain % of variance kept in the data.\n",
    "* That the eigenvectors (also called loadings) inform you how much the values on each of the original dimensions contribute to the values on the new PCA axes. Features with high values here are worth looking into: apparently it's variance in them that mostly define certain PCs, so you might be able to get insights into your data!\n",
    "* That MDS is something that, you know, _exists_, and that its slight twist on PCA is that it works on a distance matrix, which allows you to use different distances that are then best preserved in lower dimensions (rather than just focussing on getting the linearly independent axes that hold most of the variance as in PCA). You could imagine that there's many different distance measures you could use between the gene expressions of 2 cells, and each would result in a different MDS plot (that can give you a different look at your data). \n",
    "* The nice possible use of PCA in GWAS (though there are other methods). GWAS focuses on many independent linear regressions (or logistic regressions for things like obese/not obese) on extremely large cohorts of people, with cases and controls. If some cases are more related, and have had similar _environmental_ exposure that caused increases in BMI (say unhealthy eating habits) that could cause spurious associations of SNPs they share with BMI. PCA, by catching the largest variations, can be used to correct for the largest variations, which are caused by changes in ancestry, removing these spurious hits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8270cc3",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Wow, you survived for a whole week. Given that you make about 2-3 million red blood cells per second, you went and generated between 865 and 1259 billion red blood cells during this time. Way to go! Compared to that, the achievement of learning quite a lot about ML seems somewhat minor. But you did that at _the same time_. Go you!\n",
    "\n",
    "As you all know, after preppyness comes the deathly serious task of filling out the\n",
    "\n",
    "## Survey\n",
    "That's right, [get it while it's hot!](https://docs.google.com/forms/d/e/1FAIpQLSfJpOWtftiPXv_Q6QpaLZ6BVt3aE5DD1MuxBCXM11mPyyxrSA/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
