{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 5\n",
    "\n",
    "Hey ~and welcome to the Shrek in the swamp karaoke dance party~, welcome to the short practical on PCA. Here you'll play around with performing PCA and projecting data yourself. You should know that in actual practice, fast algorithms for PCA use something called spectral value decomposition (SVD), so they don't actually explicitly calculate the eigenvectors and eigenvalues in the way explained here.\n",
    "\n",
    "You'll be given some sample data, and asked to calculate the covariance matrix yourself. Then, you can calculate eigenvectors of the covariance matrix (using built-in numpy tools, rather than solving the algebra manually), and transform some 3D data to 2D in this way. Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dafdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEucliDist(vectorOne, vectorTwo):\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, axis = 1)\n",
    "\n",
    "def calcAbsDist(vectorOne, vectorTwo):\n",
    "    #using linalg.norm:\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, ord = 1, axis = 1)\n",
    "\n",
    "def makeKMeanClusters(X, k, funName = \"calcEucliDist\", maxIter = 50, nClusteringsToPerform = 20):\n",
    "    if k <= 0:\n",
    "        print(\"K must be greater than 0!\")\n",
    "        return None\n",
    "    if k > len(X):\n",
    "        print(\"K cannot be larger than the # of samples in your data!\")\n",
    "        return None\n",
    "    if maxIter <= 0:\n",
    "        print(\"Cannot have negative or 0 iterations!\")\n",
    "        return None\n",
    "    \n",
    "    resultToReturn = [None, None, None, None]\n",
    "    bestDistortion = np.Inf\n",
    "    \n",
    "    for clusteringIndex in range(0, nClusteringsToPerform):\n",
    "        initialCentroids   = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "        if len(initialCentroids) != k:\n",
    "            print(\"Centroids lost!\")\n",
    "        centroids          = initialCentroids\n",
    "        threeLastCentroids = []\n",
    "        #print(centroids)\n",
    "        for i in range(0, maxIter):\n",
    "\n",
    "            threeLastCentroids.append(np.round(centroids, 4))\n",
    "            distancesToCentroids = np.vstack([globals()[funName](centroids, datapoint) for datapoint in X])\n",
    "            closestCentroid      = np.where(distancesToCentroids == np.amin(distancesToCentroids,\n",
    "                                                                            axis = 1)[:, np.newaxis])[1]\n",
    "            centroids            = np.vstack([np.mean(X[np.where(closestCentroid == clusterNum)],\n",
    "                                                      axis = 0) for clusterNum in np.unique(closestCentroid)])\n",
    "\n",
    "            if i >2:\n",
    "                threeLastCentroids.pop(0)\n",
    "                if np.array_equal(threeLastCentroids[-1],threeLastCentroids[-2]) and np.array_equal(threeLastCentroids[-2], threeLastCentroids[-3]):\n",
    "                    print(\"No changes in cluster centroids detected in last 3 iterations. Finished at iteration \" + str(i+1) + \".\")\n",
    "                    break\n",
    "        \n",
    "        # new code\n",
    "        squareDistancesPerPoint = []\n",
    "        for index, centroid in enumerate(closestCentroid):\n",
    "            squareDistancesPerPoint.append(np.square(centroids[centroid, :] - X[index, :]))\n",
    "        distortion = 1/len(X) * np.sum(np.array(squareDistancesPerPoint))\n",
    "        \n",
    "        if distortion < bestDistortion:\n",
    "            bestDistortion = distortion\n",
    "            resultToReturn = [centroids, closestCentroid, initialCentroids, bestDistortion]\n",
    "                \n",
    "    return resultToReturn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7131a7e",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "\n",
    "Let's keep things in the visualisable domain first and focus on a dataset in 3 dimensions that we want to try and reduce to two dimensions. Feel free to rotate and zoom the 3D visualisation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5897cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 100, n_features = 3, n_informative = 2,\n",
    "                           n_redundant = 0, n_classes = 2, hypercube = False, random_state=42)\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(X[:,0], X[:, 1], X[:,2] )\n",
    "ax.set_ylabel(\"Feat 2\")\n",
    "ax.set_xlabel(\"Feat 1\")\n",
    "ax.set_zlabel(\"Feat 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e2495",
   "metadata": {},
   "source": [
    "## Implementing the covariance function\n",
    "Of course, calculating covariance is done so often that standard functions are available. Still, if you implement it yourself you should know exactly what's going on. Implement the `calcCov()` function that takes in two vectors and calculates their covariance. Be sure to check that the vectors are the same length and, if not, return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7474d0e",
   "metadata": {},
   "source": [
    "## Thinking about covariance\n",
    "\n",
    "The covariance is how certain features vary with each other. To look at this visually, we can make 2D plots of every feature with every other feature and see whether we think they co-vary or not. Let's also annotate it with the actual covariance, calculated using your function. Below, I give you the code to get all the combinations of the 3 features (including with themselves, which is just the variance). Up to you to: \n",
    "* Plot a 2D scatterplot of each pair of features in X\n",
    "* Calculate the covariance of those features\n",
    "* Annotate the scatterplot with this information (say, using `ax.set_title(\"Covariance: \" + str(myCovarianceValue))`\n",
    "* To check that your calculated (co)variances are correct, use `np.cov(X.T)`. The output should correspond to what you have calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for combo in itertools.combinations_with_replacement([0, 1, 2], 2):\n",
    "    print(combo)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b6776",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "If all goes well, you should see that feature 1 and feature 2 (column 0 and 1) have quite some covariance, which is observable in the 2D scatterplot as some heavy-handed linear scatter points being thrown in (it almost seems like some dots were generated as a line, on the background of a random blob). The other 2 combinations of features have basically no covariance: knowing something about the value of feature 2 tells you precisely nothing about the value of feature 3. The straight lines are just scatterplots of a feature with itself, which is uninformative. There, you just calculate the variance: the average sum of square differences from the feature to its mean, i.e. the spread of this feature around its mean.\n",
    "\n",
    "If covariance is still unclear to you, feel free to watch [this excellent step-by-step explanation of it by the most honourable Lord StatQuest](https://www.youtube.com/watch?v=qtaqvPAeEJY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecdfb1",
   "metadata": {},
   "source": [
    "## Getting eigenvectors of the covariance matrix\n",
    "\n",
    "To do dimension reduction, we want to calculate the eigenvectors of the covariance matrix. These are vectors that, when multiplied with the covariance matrix, only shrink or elongate, but never change direction. It turns out that these vectors point exactly along the axis of most variation in the data, then the axis of second most variation in the data, etc., when ordered according to the size of their eigenvalues.\n",
    "\n",
    "It might be interesting to see for yourself what happens to a vector in the 3D space upon repeated multiplication with the covariance matrix. Let's start with the vector $\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$ and see where it goes. Up to you to:\n",
    "* Make a for loop from 1 to 10 inclusive\n",
    "* For each iteration, make a new figure (copying the code that's there), and change d,e,f to the correct locations.\n",
    "* In that subplot, multiply the covariance matrix with the vector 0, 1, 2,  3, 4, etc. times. This is linear algebra, so use @. Note: doing `covMatrix @ (covMatrix @ data)` = `np.power(covMatrix, 2) @ data`. Just use the latter formulation with your for-loop. \n",
    "\n",
    "Question:\n",
    "* What direction does the arrow point in? Does this match with your idea from the (co)variances you calculated above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start \n",
    "xVectorOrigin = np.array([[0],[0],[0]])\n",
    "xVectorEnd    = np.array([[1],[1],[1]])\n",
    "covarianceMatrix = np.cov(X.T)\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(X[:,0], X[:, 1], X[:,2] )\n",
    "ax.set_ylabel(\"Feat 2\")\n",
    "ax.set_xlabel(\"Feat 1\")\n",
    "ax.set_zlabel(\"Feat 3\")\n",
    "a, b, c = xVectorOrigin\n",
    "d, e, f = xVectorEnd\n",
    "ax.quiver(a,b,c,d,e,f, colors = \"black\", linewidths = 3)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#up to you to remake the figure when multiplying the vector with the covariance matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1b1c1",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "If all goes well, your result should look something like ![this](largestEigVecResult.PNG)That makes sense: PCA rotates the data, making linear combinations of your data axes (here: feat1, feat2, and feat3) that maximise the variance along each new axis. When calculating the (co)variances above, it was clear that most of the variance in the data is in feature 3. And whaddaya know, this eigenvector points mostly in that direction, and only a tiny bit in the direction of feature 2, and almost not in the direction of feature 1. Note that here we did not impose the constraint of unit length (that the square root of the square of vector elements should correspond to one). Actually, we weren't _quite_ doing the correct thing, but it gives you an idea. You can do it correctly below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d09524",
   "metadata": {},
   "source": [
    "## Doing PCA on our data.\n",
    "\n",
    "Let's now do PCA and reduce the dimensionality of the data to two dimensions. To do this:\n",
    "* Mean-center your data and make sure it has unit variance (use `np.mean(X, axis = 0)` and `np.std(X, axis = 0)`) \n",
    "* Calculate the covariance matrix (`np.cov(XMeanCentered.T)`)\n",
    "* Calculate the eigenvalues and eigenvectors of the covariance matrix (`np.linalg.eig()`). These eigenvectors are already unit length.\n",
    "* Draw these eigenvectors in the 3D visualisation of the data (you can copy the code from above and slightly edit quiver to make it work)\n",
    "* Multiply (@) each of the samples with the 2 eigenvectors with the highest eigenvalues. Remember, the eigenvector is a recipe, saying something like 'to make my new axis that has the highest/second-highest/third-highest etc. variance, combine 0.2 parts original axis 1, 0.5 parts original axis 2, and 0.3 parts original axis 3'. So if you multiply a sample's coordinates `[x,y,z]` with an eigenvector $\\begin{bmatrix} xPart \\\\ yPart \\\\ zPart \\end{bmatrix}$ you get its coordinates on that new axis!\n",
    "* Draw a new 2D plot with the coordinates on the first 2 PCs. Include the % of variance explained (remember, that's eigenvalue_of_PC/sum(eigenvalues)\\*100%)\n",
    "\n",
    "Hints:\n",
    "* The eigenvalues returned from this are not ordered according to magnitude! You need to pick the two eigenvectors corresponding to the two largest eigenvalues yourself! Probably easiest to use `np.argsort()` ([click](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)) to sort the eigenvalues and eigenvectors the same way. Note: to sort in descending order use `yourArray.argsort()[::-1]` (see [here](https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order))\n",
    "* When sorting your eigenvectors, make sure you ony sort the columns, i.e. use `eigenvectors[:,argSortIndices]`.\n",
    "* You can get the coordinates of PC One using `d,e,f = yourSortedEigenvectors[:,0]`\n",
    "* The answers have a labelled 3D plot of the PCs, so you can check whether they match there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd35ff5",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "If you're confused that the eigenvectors are so different than the first one you 'calculated' before, that's because I fooled you a bit. To get a feel for what happens with the covariance matrix we multiplied it with X, but actually the data should be mean-centered and have unit variance, and the vectors should have unit length! Before, feature 3 had a variance of ~1, whereas feature 1 had a variance of only something like 0.5, and feature 2 a variance of ~0.75. Therefore, if you put them on the same scale, it makes sense that now feature 1 and feature 2 are more important than before. This shows the importance of scaling and mean-centering correctly.\n",
    "\n",
    "Notice that you have about 79% of the variance in the data on these two axes. Not too shabby, seeing as you've sheared off a whole dimension. \n",
    "\n",
    "## Why should you care?\n",
    "\n",
    "This example is, of course, contrived. We have some nameless features, and only 3 of them to boot, and we're doing some magic and -huzzah- there's your 2D plot. Underwhelming, one might say. Now imagine, however, that you've just assayed gene expression of 20.000 genes in 5 conditions and you want to predict the experimental condition based on the genes. As you've seen in the morning practical, your ML algorithm will work great on training data, but crash and burn when presented with test data. But PCA can come to the rescue: PCA will combine all the 20.000 genes into new axes, axes that are mostly made up of the genes that vary most. It's quite probable that 10.000 genes are basically doing nothing in response to your experimental condition, so the eigenvector components of your top _n_ eigenvalues for those genes will be next to nothing. Your first PCs, however, by their nature, will combine the genes that vary most in your dataset. You might even find that these PCs correspond to known gene circuits, etc. The ability to rotate your data and keep a lot of the variance in _vastly_ fewer dimensions is a sort of ML superpower!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800233ad",
   "metadata": {},
   "source": [
    "## Covariance matrix with linear algebra and using SVD\n",
    "\n",
    "Before, I let you calculate the covariances yourself one by one. Note that, if you have unit variance and zero mean, you can calculate the covariance matrix with $\\frac{1}{m} \\cdot X^T \\cdot X$ Why? Well, if you have the following feature matrix with 2 samples in the rows and two features in the columns: $$\\begin{bmatrix} 3 & 4 \\\\ 8 & 2 \\end{bmatrix}$$ then if you do:\n",
    "$$\\frac{1}{m} \\cdot \\begin{bmatrix} 3 & 8 \\\\ 4 & 2 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 4 \\\\ 8 & 2 \\end{bmatrix}$$ you get 4 outcomes:\n",
    "$$\\begin{bmatrix} \\frac{1}{m} \\cdot (3 \\cdot 3 + 8 \\cdot 8) & \\frac{1}{m} \\cdot (3 \\cdot 4 + 8 \\cdot 2) \\\\ \\frac{1}{m} \\cdot (4 \\cdot 3 + 2 \\cdot 8) & \\frac{1}{m} \\cdot (4 \\cdot 4 + 2 \\cdot 2) \\end{bmatrix}$$ Which is exactly the covariance matrix:\n",
    "$$\\begin{bmatrix} \\sigma_{1,1} & \\sigma_{1,2} \\\\ \\sigma_{2,1} & \\sigma_{2,2} \\end{bmatrix}$$\n",
    "\n",
    "Most implementations actually use SVD to calculate the eigenvectors and eigenvalues, not direct calculation of eigenvectors using the covariance matrix. You don't need to know what it does, exactly, it's just a generalisation of calculating eigenvectors and eigenvalues that is faster and **doesn't require you outright calculating the covariances, which can be a real burden when the number of features is large!**. Below I write code for calculating the eigenvectors and eigenvalues using SVD. See [here](https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8) for more information if you're interested.\n",
    "\n",
    "Note that the axes are flipped relative to your PCA plot above. That's because it's arbitrary whether you put the vector along which you order the points from, say, 0 to -1 or from 0 to 1: it defines the same line through the high-dimensionals space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanX         = np.mean(X, axis = 0)\n",
    "stdX          = np.std(X, axis = 0)\n",
    "XMeanCentered = (X - meanX)/stdX\n",
    "#covMatrix = XMeanCentered.T @ XMeanCentered\n",
    "U, Sigma, V = np.linalg.svd(XMeanCentered, full_matrices = False)\n",
    "print(\"U:\")\n",
    "print(U)\n",
    "print(\"Sigma:\")\n",
    "print(Sigma)\n",
    "transformedData = U @ np.diag(Sigma)\n",
    "twoAxes = transformedData[:,0:2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "ax.scatter(twoAxes[:,0], twoAxes[:,1])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020efcf",
   "metadata": {},
   "source": [
    "## Finishing up the short practical\n",
    "\n",
    "As a final relaxed exercise to make sure that these concepts land, please (re)watch the StatQuest PCA video, which does a great job explaining terminology and the intuition behind PCA. [Here you go!](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
    "\n",
    "As an extra, I would also recommend reading this post: [click](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues), and looking over [this](https://setosa.io/ev/principal-component-analysis/).\n",
    "\n",
    "## What I'd like you to remember here\n",
    "* PCA is a thing. It uses the covariance matrix to rotate the space, whereby new axes become linear combinations of the original dimensions.\n",
    "* You need to center your data (and optionally scale the data to unit variance: if you _don't do this_, features with higher variance will influence the rotation more. By scaling you treat all features equally).\n",
    "* I'd like you to have a good intuitive understanding of what PCA is doing, which the StatQuest video and StackExchange thread should have helped with.\n",
    "\n",
    "## Survey\n",
    "[Yes](https://docs.google.com/forms/d/e/1FAIpQLScuEqvjDRIMq_dsqaK4fsYhF_i9NKbBg0wsqKmlASofaKFR1g/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
