{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 1 day 5\n",
    "\n",
    "Here you'll see the problems with high-dimensional data for yourself and experiment with feature selection. First, run the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import sklearn\n",
    "import itertools\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important functions\n",
    "\n",
    "def calcEucliDist(vectorOne, vectorTwo):\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, axis = 1)\n",
    "\n",
    "def calcAbsDist(vectorOne, vectorTwo):\n",
    "    #using linalg.norm:\n",
    "    return np.linalg.norm(vectorOne-vectorTwo, ord = 1, axis = 1)\n",
    "\n",
    "def makeKMeanClusters(X, k, funName = \"calcEucliDist\", maxIter = 50, nClusteringsToPerform = 20):\n",
    "    if k <= 0:\n",
    "        print(\"K must be greater than 0!\")\n",
    "        return None\n",
    "    if k > len(X):\n",
    "        print(\"K cannot be larger than the # of samples in your data!\")\n",
    "        return None\n",
    "    if maxIter <= 0:\n",
    "        print(\"Cannot have negative or 0 iterations!\")\n",
    "        return None\n",
    "    \n",
    "    resultToReturn = [None, None, None, None]\n",
    "    bestDistortion = np.Inf\n",
    "    \n",
    "    for clusteringIndex in range(0, nClusteringsToPerform):\n",
    "        initialCentroids   = X[np.random.choice(X.shape[0], k, replace=False), :]\n",
    "        if len(initialCentroids) != k:\n",
    "            print(\"Centroids lost!\")\n",
    "        centroids          = initialCentroids\n",
    "        threeLastCentroids = []\n",
    "        #print(centroids)\n",
    "        for i in range(0, maxIter):\n",
    "\n",
    "            threeLastCentroids.append(np.round(centroids, 4))\n",
    "            distancesToCentroids = np.vstack([globals()[funName](centroids, datapoint) for datapoint in X])\n",
    "            closestCentroid      = np.where(distancesToCentroids == np.amin(distancesToCentroids,\n",
    "                                                                            axis = 1)[:, np.newaxis])[1]\n",
    "            centroids            = np.vstack([np.mean(X[np.where(closestCentroid == clusterNum)],\n",
    "                                                      axis = 0) for clusterNum in np.unique(closestCentroid)])\n",
    "\n",
    "            if i >2:\n",
    "                threeLastCentroids.pop(0)\n",
    "                if np.array_equal(threeLastCentroids[-1],threeLastCentroids[-2]) and np.array_equal(threeLastCentroids[-2], threeLastCentroids[-3]):\n",
    "                    print(\"No changes in cluster centroids detected in last 3 iterations. Finished at iteration \" + str(i+1) + \".\")\n",
    "                    break\n",
    "        \n",
    "        # new code\n",
    "        squareDistancesPerPoint = []\n",
    "        for index, centroid in enumerate(closestCentroid):\n",
    "            squareDistancesPerPoint.append(np.square(centroids[centroid, :] - X[index, :]))\n",
    "        distortion = 1/len(X) * np.sum(np.array(squareDistancesPerPoint))\n",
    "        \n",
    "        if distortion < bestDistortion:\n",
    "            bestDistortion = distortion\n",
    "            resultToReturn = [centroids, closestCentroid, initialCentroids, bestDistortion]\n",
    "                \n",
    "    return resultToReturn\n",
    "\n",
    "def hierarCluster(X, distanceFunc = \"calcEucliDist\", linkageMethod = \"average\", displayDistMatrix = False):\n",
    "    \n",
    "    if linkageMethod not in [\"average\", \"complete\", \"single\"]:\n",
    "        print(\"Error, please input a valid linkage method!\")\n",
    "        return None\n",
    "    if distanceFunc  not in globals().keys():\n",
    "        print(\"Error, please input a valid distance function name!\")\n",
    "    \n",
    "    # make an empty distance matrix\n",
    "    distanceMatrix = np.zeros(shape = (len(X), len(X)))\n",
    "    distanceMatrix.fill(np.nan)\n",
    "    # make a list with the indices of every data point. This is the list of clusters, where you start\n",
    "    # with every point in a cluster and then start merging them.\n",
    "    initialList = [[index] for index, _ in enumerate(X)]\n",
    "    clusterList = initialList.copy()\n",
    "    clusteringOverIterations = []\n",
    "    clusteringOverIterations.append(initialList)\n",
    "    # also make an empty list that saves which cluster indices were merged for every iteration\n",
    "    clusterIndicesMergedList = []\n",
    "    for rowIndex, row in enumerate(distanceMatrix):\n",
    "        for colIndex, cellValue in enumerate(row):\n",
    "            # distance from yourself to yourself is 0, don't calculate!\n",
    "            if colIndex == rowIndex:\n",
    "                continue\n",
    "            # in the first loop, you calculate distance from 1 to 2.\n",
    "            # in the second loop, you don't want to calculate distance from 2 to 1 again. This safeguards against that.\n",
    "            if colIndex < rowIndex:\n",
    "                continue\n",
    "\n",
    "            distanceMatrix[rowIndex, colIndex] = globals()[distanceFunc](X[rowIndex,:][np.newaxis,  :],\n",
    "                                                                             X[colIndex, :][np.newaxis, :])\n",
    "    if displayDistMatrix:\n",
    "        display(pd.DataFrame(distanceMatrix))\n",
    "\n",
    "    # We continue clustering until everything is in one giant cluster. Thats len(X)-1 clustering steps.\n",
    "    for i in range(0, len(X)-1):\n",
    "        # we start with no idea of which two clusters we need to cluster\n",
    "        lowestDistDatapoints = None\n",
    "        # since we haven't calculated any distance, our current distance is infinite\n",
    "        distToCluster = np.Inf\n",
    "        # clusterList initially looks like [[0], [1], ... [99]].\n",
    "        # itertools.combinations makes that into [([0], [1]), ([0], [2]), ([0], [3]) ... ([1], [2]), ([1], [3])... (98, 99)]\n",
    "        # so you get all possible combinations of clusters that you could cluster together\n",
    "        for combo in itertools.combinations(clusterList, 2):\n",
    "\n",
    "            distance = 0\n",
    "            distanceSingleLink = np.Inf # need this because for single linkage you want lowest distance to be selected\n",
    "                                        # so need to have the starting distance always be lower.\n",
    "            # make all combinations of data points in the first cluster and data points in the second cluster\n",
    "            # so if the current combo = ([0, 12, 15], [3, 2]), this results in:\n",
    "            # [[0, 3], [0, 2], [12, 3], [12, 2], [15, 3], [15,2]]: these are all the points that we need to get\n",
    "            # the distances for (and average for average linkage)\n",
    "            toIterate = [j for i in [list(zip([elem] * len(combo[1]), combo[1] )) for elem in combo[0]] for j in i]\n",
    "            for indicesTwoDatapoints in toIterate:\n",
    "                #sort the indices. Our matrix has only the distance between 1 and 2, not between 2 and 1.\n",
    "                #this turns [12, 2] from above into [2, 12], etc.\n",
    "                indicesTwoDatapoints = sorted(indicesTwoDatapoints)\n",
    "\n",
    "                # keep a running total of all distances between the points in the two clusters\n",
    "                if linkageMethod == \"average\":\n",
    "                    distance += distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "                if linkageMethod == \"complete\":\n",
    "                    # for a cluster, if the distance between two points is larger than the current largest distance\n",
    "                    # between points in a cluster, that is the new cluster distance.\n",
    "                    if distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]] > distance:\n",
    "                        distance = distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "                if linkageMethod == \"single\":\n",
    "                    if distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]] < distanceSingleLink:\n",
    "                        distanceSingleLink = distanceMatrix[indicesTwoDatapoints[0], indicesTwoDatapoints[1]]\n",
    "\n",
    "            if linkageMethod == \"average\":\n",
    "                totalAvgDistance = distance/(len(combo[0]) * len(combo[1]))\n",
    "\n",
    "            # if distance between these clusters is less than the lowest distance we have seen so far,\n",
    "            #set these clusters as the ones to cluster. \n",
    "                if totalAvgDistance < distToCluster:\n",
    "                    distToCluster       = totalAvgDistance\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "            if linkageMethod == \"complete\":\n",
    "                if distance < distToCluster:\n",
    "                    distToCluster       = distance\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "            if linkageMethod == \"single\":\n",
    "                if distanceSingleLink < distToCluster:\n",
    "                    distToCluster       = distanceSingleLink\n",
    "                    dataPointsToCluster = combo\n",
    "\n",
    "        #make a new list of clusters\n",
    "        clusterIndicesMergedList.append(dataPointsToCluster)\n",
    "        clusterList = clusterList.copy()\n",
    "        for index, elem in enumerate(clusterList):\n",
    "            # merge the second cluster into the first cluster\n",
    "            if elem == dataPointsToCluster[0]:\n",
    "                clusterList[index] = clusterList[index] + dataPointsToCluster[1]\n",
    "                #clusterList2[index] = sorted(clusterList[index])\n",
    "            # remove the separate second cluster (it's now been merged to the first one)    \n",
    "            if elem == dataPointsToCluster[1]:\n",
    "                clusterList.pop(index)\n",
    "        # Finally, save all clusters, from the very beginning (all separate clusters) until the very end (all in one cluster) in one list by appending to that the current clusters      \n",
    "        clusteringOverIterations.append(clusterList)\n",
    "        \n",
    "        #addition to make a list of lists of everything:\n",
    "        \n",
    "    \n",
    "    return [clusteringOverIterations, pd.DataFrame(distanceMatrix), clusterIndicesMergedList]\n",
    "\n",
    "\n",
    "def drawHierarchicalClustering(hierarClusterOutcome, figsize = (25,8), title = \"Plot\", labels = None):\n",
    "    clusterListX       = hierarClusterOutcome[0]\n",
    "    clusteredPerStepX  = hierarClusterOutcome[2]\n",
    "    xLabels            = np.array(list(itertools.chain(*clusterListX[-1])))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    ax.set_xticks(range(0, len(xLabels)))\n",
    "    if not labels is None:\n",
    "        labels = np.array(labels)\n",
    "        if len(labels) == len(xLabels):\n",
    "            labels = labels[xLabels]\n",
    "            ax.set_xticklabels(labels, rotation = 90)      \n",
    "        else:\n",
    "            print(\"Labels supplied should be of same length as the amount of data points!\")\n",
    "            return None\n",
    "    else:   \n",
    "        ax.set_xticklabels(xLabels)\n",
    "    ax.margins(y=0)\n",
    "\n",
    "    heightPerDataPointPreviousStep = np.array([0] * len(xLabels))\n",
    "    for i, clusterStep in enumerate(clusteredPerStepX):\n",
    "        pos1Positions = np.array([np.where(xLabels == elem)[0] for elem in clusterStep[0]])\n",
    "        pos1Avg       = np.mean(pos1Positions)\n",
    "        #pos1Start     = np.min(pos1Positions)\n",
    "        #pos1End       = np.max(pos1Positions)\n",
    "        pos1ClustSize = len(pos1Positions)\n",
    "        pos2Positions = np.array([np.where(xLabels == elem)[0] for elem in clusterStep[1]])\n",
    "        pos2Avg       = np.mean(pos2Positions)\n",
    "        #pos2Start     = np.min(pos2Positions)\n",
    "        #pos2End       = np.max(pos2Positions)\n",
    "        pos2ClustSize = len(pos2Positions)\n",
    "\n",
    "\n",
    "\n",
    "        heightEnd   = max(pos1ClustSize, pos2ClustSize)\n",
    "        ax.plot([pos1Avg, pos1Avg], [heightPerDataPointPreviousStep[pos1Positions[0][0]],heightEnd], color = \"black\")\n",
    "        ax.plot([pos2Avg, pos2Avg], [heightPerDataPointPreviousStep[pos2Positions[0][0]],heightEnd], color = \"black\")\n",
    "        ax.plot([pos1Avg, pos2Avg], [heightEnd,heightEnd], color = \"black\")\n",
    "\n",
    "        heightPerDataPointPreviousStep[np.ravel(pos1Positions)] += heightEnd - heightPerDataPointPreviousStep[pos1Positions[0][0]]\n",
    "        heightPerDataPointPreviousStep[np.ravel(pos2Positions)] += heightEnd - heightPerDataPointPreviousStep[pos2Positions[0][0]]\n",
    "\n",
    "    ax.set_ylim(0, max(heightPerDataPointPreviousStep)+1)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7131a7e",
   "metadata": {},
   "source": [
    "## Seeing how distances become meaningless\n",
    "\n",
    "One of the strange characteristics of high-dimensional data is that data points become more and more extreme as the number of dimensions rises, yet at the same time the difference between the maximum and minimum distances between points keeps on shrinking so distance metrics become progressively less meaningful. \n",
    "\n",
    "Let's make datasets with 2, 5, 10, 100, 1.000, 10.000, 100.000, and 1.000.000 features (dimensions). We'll make three clusters of points in each dataset. It's up to you to calculate the distances between all points in these datasets, and plot the difference between the maximum and minimum distance in each case. To do this:\n",
    "\n",
    "* For every one of the five datasets in the `dataSetList`:\n",
    "    * Make a distance matrix (there's 100 data points). Fill it with `np.nan`.\n",
    "    * Use `itertools.combinations` to get all pairs of data points\n",
    "    * Use `calcEucliDist` to calculate Euclidean distance between points\n",
    "    * Save the distance matrices in a list\n",
    "* When done, get the maximum and minimum entry in each distance matrix (use `np.nanmin`), and calculate the difference between them. Divide this by the minimum distance. Put the outcome in a list.\n",
    "* Finally, plot the values in this list.\n",
    "\n",
    "Hints:\n",
    "* If you get an AxisError saying 'axis 1 is out of bounds for array of dimension 1', try replacing your equivalent of `dataSet[index, :] with dataSet[index, :][np.newaxis, :]`. It's because of shenanigans with the difference between 1D and 2D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "featCountList    = [2, 5, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "dataSetList      = []\n",
    "clusterLabelList = []\n",
    "\n",
    "for featCount in featCountList:\n",
    "    X, y = sklearn.datasets.make_blobs(n_samples = 100, n_features = featCount, centers = 3, random_state = featCount)\n",
    "    dataSetList.append(X)\n",
    "    clusterLabelList.append(y)\n",
    "\n",
    "# your answer here. You don't need to use the labelList.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8dfd1",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d255a",
   "metadata": {},
   "source": [
    "## Problems with overfitting in high dimensionality\n",
    "\n",
    "The intuition would be that more features is better: the more features, the easier to separate classes, say, or predict housing problems. The issue is that for more features you need exponentially more data to cover all your bases, to make sure you have a good coverage of the possible values for each feature. If you don't do that, you are very prone to overfitting. The simplest case can be illustrated with 2 dimensions: 1 dimension which is informative for class membership, and one which we add and is just completely random noise. The addition of the random noise suddenly allows us to get perfect separation, but we know that this is just a result of not having enough samples to see that the 2nd dimension is useless: ![overfitting](overfittingDueToDims.PNG)\n",
    "\n",
    "Let's experience these problems for ourselves. We'll make 11 datasets for classification, where 2 features are informative and the rest are just random. We'll keep the number of samples at 1.000. We'll train logistic regression (using the sklearn implementation, unregularised) on a train set and test on the test set, and see how well we do on train and test in each case. I make the ten datasets below, up to you to:\n",
    "* Split into train and test set for each case (80/20). Use a random state of 42. (Normally you'd do cross-validation or even nested cross-validation, but for this example we don't need to go so far).\n",
    "* Train a logistic regression classifier (without regularisation!) in each case.\n",
    "* Get the performance on the training data and the test data.\n",
    "* Plot performance on the training data and on the test data in the same plot (with _n\\_dimensions_ on the x-axis, and accuracy on the y-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "featuresToMake    = [2, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "classDataSetsList = []\n",
    "classLabelList    = []\n",
    "for nFeats in featuresToMake:\n",
    "    X, y = sklearn.datasets.make_classification(n_samples = 1000, n_features = nFeats, n_informative = 2,\n",
    "                                               n_redundant = 0, n_classes = 2, random_state=nFeats)\n",
    "    classDataSetsList.append(X)\n",
    "    classLabelList.append(y)\n",
    "    \n",
    "    \n",
    "# your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d9f0f",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "As you'll have seen, you're doing an absolutely stellar job on the training data from 998 noise features onwards, but performing abominably on the test set. How do we combat this? One option is to use dimensionality reduction methods, of which we'll discuss the most well-known one in the following lecture. Another option is to use feature selection. \n",
    "\n",
    "## Forward selection using filtering\n",
    "\n",
    "First, let's use the filter method, where you use some other criterion outside of the ML training and testing workflow to pre-determine which features to include. Below, I give you the case where you have 1000 features to work with. Your job: filter the data using t-tests between the mean of class 1 and class 2, and select the top 2 features. Then, train a classifier on those and see the results. **Note that you should only perform feature selection on the training set to prevent overfitting**.\n",
    "\n",
    "Hints:\n",
    "* You need to do a t-test for each feature (column), and between the rows corresponding to class 1 (y=0) and class 2 (y=1)\n",
    "* The highest difference implies the largest absolute t-statistic\n",
    "* You'll probably need to sort the absolute t-statistics, pick the two largest ones, and then pick the features that had that t-statistic.\n",
    "* This is one of few times I didn't give an itemised to-do list. If you feel lost, though, feel free to ask or look in the answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind # t-test function\n",
    "X, y = classDataSetsList[6], classLabelList[6]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# your answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4de7c9",
   "metadata": {},
   "source": [
    "## Conclusion filtering\n",
    "\n",
    "If all went well, you should see that there's a pretty sizeable increase in performance on the test set: for me, it went from 0.75% accuracy to 0.925% accuracy. Of course, using a t-test is pretty basic, and in reality features won't be either _very_ informative or random. Still, the procedure is the same. Note also that there are (many) other approaches, such as using mutual information (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) and [here](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)).\n",
    "\n",
    "## Wrappers for feature selection\n",
    "\n",
    "In the end, the proof is in the pudding: we don't care about t-test statistics or mutual information, they are a _proxy_ for how well the feature will serve us in our classification or prediction task. Another option is to just brute-force things: if we care about how our classifier performs, then surely the best course of action is simply to see how our classifier performs for certain features. To do this, we train a separate classifier for each feature, pick the one that performs best in cross-validation, then train a classifier using that best feature plus every other feature, again pick the feature that performs best in cross-validation, etc. We stop when we note that when adding more features our performance actually goes down.\n",
    "\n",
    "Note that this involves a _lot_ of training and testing: for 1000 features you're training 1000 classifiers (k times because you are doing cross-validation), then 999 classifiers (k times), then 998 classifiers (k times), etc. Let's do feature selection using such a wrapper right now. We'll use a dataset with fewer features: only a 100, of which 2 are informative again. Up to you to:\n",
    "\n",
    "* Train a logistic regression classifier (without regularisation) for each feature separately (using 5-fold cross-validation and sklearn (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)))\n",
    "* Record for each feature the average ROC AUC (score) over the 5 test folds. \n",
    "* Pick the feature with the highest average score for inclusion.\n",
    "* Now train all combinations of that feature with one other feature, again record the average ROC AUC over folds.\n",
    "* If the average ROC AUC on the test set is higher, add the new feature as well. If not (adding more features only brings down performance): stop.\n",
    "\n",
    "Hints:\n",
    "* Do your thang with the X_train and y_train data, we're keeping one final test set separate to know our final performance.\n",
    "* If you get problems when subsetting, use something like `X_train[:,colIndex][:, np.newaxis]`. Otherwise it's a 1D-array and sklearn doesn't like that. **Do this only in case the shape is wrong, don't always add dimensions like this!**\n",
    "* Cross-validation gives you back a dictionary, you'll want the `[\"test_score\"]` part of it.\n",
    "* Doing this is not entirely trivial, you'll need two loops, and to check whether the current result is better than the previous one, etc. If you get stuck, you know what to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = classDataSetsList[4], classLabelList[4]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "# your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff7d2f",
   "metadata": {},
   "source": [
    "## Conclusion wrapper\n",
    "\n",
    "It depends on exactly what you did, what the result is (you don't say?). If you allow the same feature to be selected multiple times, then that does indeed happen. I got feature 18 in there a whopping 18 times, for instance. So you shouldn't allow this: apparently having the same feature multiple times somehow can improve performance, but you know that any performance gain is fake: it's just the same feature repeated, after all!\n",
    "\n",
    "When you don't allow duplicate features, things are still somewhat strange: I get 13 features selected, while there should be only 2 features informative for class membership. This shows you how there's still the problem of not having enough training data even if we do feature selection: by chance, some of the completely random features might _seem_ to have some relation to class membership, because some observations with class 1 got a few slighly higher values for a certain feature than those in class 0. If you'd have 1.000.000.000 training examples, this effect would be less likely to exist. If you look at the output in the answers, you do see that 1 feature gets you up to 0.945 ROC AUC, adding the second gets you to 0.951 ROC AUC, and all the others only bring it to a ROC AUC of 0.958. So we could solve this by only accepting extra features that give, say, > 0.005 increase in AUC ROC. \n",
    "\n",
    "What you must have noticed is that this is computationally much more expensive. In reality, you could mix and match these strategies: first remove features that have almost no variance (they contain basically no information), then filter on some criterion but be quite lenient in how many features you still allow, and then use wrapping for final selection.\n",
    "\n",
    "## Conclusion forward feature selection\n",
    "\n",
    "Of course, methods like these are all _greedy_: you just select the _n_ features that, on their own, have the most difference between classes in some metric (filtering) or that, on their own, contribute most to increasing classification performance in combination with previously selected features (wrapper). However, it's not difficult to see that 10 individually not that informative features could, when combined, uniquely identify each class really well. You saw a hypothetical example of this in the lecture. Hence, these methods are not the end-all be-all, and more sophisticated methods exist. We won't go into those here. What we will go into is dimensionality reduction, which aims at compressing the variance/information in many dimensions into fewer ones as best as possible, to avert the dimensionality problems in that way.\n",
    "\n",
    "## Note on embedded methods\n",
    "\n",
    "There's a third class of methods: the ones like L1 regularisation, where the weights for certain features shrink to 0 during training, so that there's automatic feature selection. This, too, will prevent overfitting, and is the reason why I expressly dictated that there should be no regularisation in your logistic regressors. Another example of this would be feature selection in Random Forests based on the feature importance (which RFs automatically calculate for each \n",
    "feature).\n",
    "\n",
    "## What I'd like you to remember here:\n",
    "* High-dimensional data has wacky properties: distances become meaningless/dominated by noise, and data becomes _incredibly_ sparse very quickly. The amount of data you need to cover the feature space well rises exponentially with the number of features.\n",
    "* High-dimensional data makes it easy to overfit. This is directly related to the previous point: only if you have huge swathes of data (unrealistically, unobtainably huge) can you hope to cover the feature space well. Otherwise, you will suddenly find perfect hyperplanes or other separations of the data, that are really just based on noise and don't hold up in test sets.\n",
    "* Besides dimensionality reduction, which we'll tackle next, you can also perform feature selection. This (or at least the basic greedy forward feature selection methods we discuss here) is not guaranteed to give you the best _combination of features_ for prediction. Rather, you use some external numerical property (filter) or classifiers trained with individual best features added each time (wrapper), and keep adding features until performance goes down. Regularisation, especially with the L1 norm, gives an automatic way of performing feature selection.\n",
    "\n",
    "## The end\n",
    "No Ender dragon here. Only the prospect of another lecture. Congrats on being done with this practical! Oh and don't forget to shoot me your thoughts (be they positive or unholy):\n",
    "\n",
    "## Survey\n",
    "[You know the drill](https://docs.google.com/forms/d/e/1FAIpQLSdhDoOGetpgKehesdRMDp4y_KlSVhy8oQIcKMAaOEya3d6LwQ/viewform?usp=sf_link)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
