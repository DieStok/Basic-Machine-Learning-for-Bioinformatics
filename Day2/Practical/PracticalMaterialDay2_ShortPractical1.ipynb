{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 1 day 2\n",
    "\n",
    "Hi there. Here, you're going to implement the sigmoid function and the cost function for logistic regression, before running the algorithm on a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7131a7e",
   "metadata": {},
   "source": [
    "## Making and plotting the sigmoid function\n",
    "\n",
    "As a first exercise, do the following:\n",
    "* implement the sigmoid function. Call it `mySigmoid()`. Note that $e^x$ is `np.exp(x)` in Numpy. The output of your function on the `testArray` should be `array([1.92874985e-22, 2.68941421e-01, 5.00000000e-01, 7.31058579e-01, 1.00000000e+00])` which, if you print it, will look like <br> `[1.92874985e-22, 2.68941421e-01, 5.00000000e-01, 7.31058579e-01, 1.00000000e+00]`.\n",
    "* get 100 numbers from a normal distribution `rng = default_rng(seed = 42) ; vals = rng.standard_normal(100)`\n",
    "* apply your sigmoid function to these 100 values\n",
    "* plot the sigmoid outputs on the y axis and the original values on the x-axis\n",
    "\n",
    "Plotting hints:\n",
    "* start a new plot with `fig, ax = plt.subplots()`\n",
    "* you can plot points with `ax.scatter(xData, yData, colour= \"red\", label = \"WowCoolMan\")`\n",
    "* you can label the x- and y-axis with `ax.set_xlabel(\"Banana\"); ax.set_ylabel(\"Happiness level\")`\n",
    "* add a legend with `ax.legend()` (wait for it ....dary)\n",
    "* once you are done plotting, you need to run `fig.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600538de",
   "metadata": {},
   "outputs": [],
   "source": [
    "testArray = np.array([-50, -1, 0, 1, 50])\n",
    "\n",
    "\n",
    "# your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f75dcd",
   "metadata": {},
   "source": [
    "## Assigning a class\n",
    "\n",
    "Let's have these random numbers stand in for real output from logistic regression. Say we want to classify samples as 1 if they have >= 70% chance of being positive, whereas we want to classify all other samples as negative. Up to you to make it happen!\n",
    "\n",
    "* Get an array of predicted class membership based on your Sigmoid function output. Call it `predictedClass`. For example, you could make a vector of zeros with `np.zeros`, and then set the elements that are >= 0.7 in your sigmoid output to 1. Or you could use `np.where` with x and y arguments (see [here](https://numpy.org/doc/stable/reference/generated/numpy.where.html)). \n",
    "* Change your plotting code to:\n",
    "    * colour the points by their assigned class (use two `ax.scatter` calls with different labels)\n",
    "    * add a vertical dashed line where we've placed our threshold (you'll probably need [this](https://stackoverflow.com/a/68087129), and `ax.vlines` [(click)](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.vlines.html)). Save the x location of your dotted line as `thresholdXLocation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a03b26",
   "metadata": {},
   "source": [
    "## Running logistic regression\n",
    "\n",
    "To run logistic regression, we'll need to do a few things:\n",
    "* Make a function called `linAlgLogRegHypothesis(data, thetas)` that internally runs `linAlgRegHypothesis` and then uses the sigmoid function on those outputs and returns this.\n",
    "* Define a new cost function `costFuncLogReg(x, y, thetas)`.\n",
    "* Change gradient descent to allow it to select the hypothesis function it uses.\n",
    "\n",
    "First make `linAlgLogRegHypothesis`. Remember, this is just a function that calls `mySigmoid(linAlgRegHypothesis(data, thetas))` and returns the result. <br> <br> Then, make a new cost function `costFuncLogReg`. It should implement the formula: <br> <br>$$Cost(x) = -y \\cdot log(h_\\theta(x))- (1-y) \\cdot log(1-h_\\theta(x))$$ <br> with $$h_\\theta(x) = sigmoid(\\theta^T \\cdot x)$$\n",
    "<br>\n",
    "Note that it should work with arrays, i.e. y is a column vector of 0 or 1, which looks like this: $y = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\end{bmatrix}$, and x is an array with samples on the rows and their features in the columns (such as $ X = \\begin{bmatrix} 0.18 & 1.03 \\\\ \\vdots & \\vdots \\\\ -0.72 & 0.4 \\end{bmatrix}$). <br> Use `np.nansum` rather than `np.sum`: this ignores NaNs if for some reason they are present in the data, rather than throwing an error. \n",
    "\n",
    "**Hints:**\n",
    "* The output of your `linAlgLogRegHypothesis` using `testX` as data and `testThetas` as parameters should be: <br>\n",
    "`[[0.71300016]\n",
    " [0.95302385]]`\n",
    "* The output of your `costFuncLogReg(testX, testY, testThetas)` should be ~`0.64819`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.array([[0.2, -0.03], [-0.3, 0.52]])\n",
    "testY = np.array([[0], [1]])\n",
    "testThetas = np.array([[0.8], [1.3], [5]])\n",
    "\n",
    "def linAlgRegHypothesis(data, thetas):\n",
    "    data = np.array(data)\n",
    "    oneFeatToAdd = np.ones(len(data))\n",
    "    newFeatArray = np.c_[oneFeatToAdd, data]\n",
    "    #make sure thetas are always of the form np.array([[theta1], [theta2]]), i.e. column vector\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    predictions = newFeatArray @ thetas\n",
    "    return predictions\n",
    "\n",
    "#your answer \n",
    "# remember: the only thing that changes is that for logistic regression, we apply the sigmoid function\n",
    "# to all the outputs of a linear regression hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba7e8c",
   "metadata": {},
   "source": [
    "## Using gradient descent for your logistic regression\n",
    "\n",
    "Nearly there, we just need to change the gradient descent function from yesterday to use logistic regression for prediction rather than linear regression. We'll do that by adding a call to `globals()` so the same gradient descent function can handle linear and logistic regression. A copy of the gradient descent function from yesterday is provided below. Your job is to add an extra argument `hypothesis = 'linAlgLogRegHypothesis'` that can be set to the name of a hypothesis function you've defined, and make sure the function uses that using `globals()`\n",
    "\n",
    "* Your function definition should look like `def linAlgGradientDescent(x, y, thetas, alpha, hypothesis = \"linAlgLogRegHypothesis\"):`\n",
    "\n",
    "\n",
    "\n",
    "**Optional:** here's a link to a derivation showing that the gradient of the logistic regression cost function is the same as that of linear regression, and that it is convex: [LogRegGradientDerivation](WhyLogRegGradEqual.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linAlgGradientDescent(x, y, thetas, alpha) :\n",
    "    m = len(x)\n",
    "    # all these ndim checks are just to make sure that everything that should be a column vector\n",
    "    # indeed is a column vector\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    preds  = linAlgRegHypothesis(x, thetas)\n",
    "    if preds.shape != (m, 1):\n",
    "        preds  = preds[:, np.newaxis]\n",
    "    if y.ndim < 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation  = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    finalGradientSteps = alpha/m * gradientSummation\n",
    "    newThetas          = thetas - finalGradientSteps.T\n",
    "    return newThetas\n",
    "\n",
    "#your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080093d9",
   "metadata": {},
   "source": [
    "## Testing your gradient descent function\n",
    "\n",
    "The below puts your function to the test. We assume that the predictedClass you made earlier (where you used 100 random normally distributed values, put them into the sigmoid, and assigned the 30% highest values as 1 and the rest as 0) are the true labels. We start from random thetas, update them by gradient descent, and look whether the predicted class indeed moves towards the actual signal (decision threshold) that is present in the data.\n",
    "\n",
    "After looking at the images, answer the following question: \n",
    "* Why is there still a cost when all predictions are correct? How is that possible?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = predictedClass[:, np.newaxis]\n",
    "values = vals[:, np.newaxis]\n",
    "thetas = np.array([[-5.0], [5.0]])\n",
    "alpha = 0.8\n",
    "stepsToTake = 1000\n",
    "\n",
    "# running gradient descent\n",
    "thetasAlongDescent = []\n",
    "thetasAlongDescent.append(thetas)\n",
    "costAlongDescent   = []\n",
    "for gradientDescentStep in range(0, stepsToTake):\n",
    "    costNow   = costFuncLogReg(values, labels, thetasAlongDescent[-1])\n",
    "    costAlongDescent.append(costNow)\n",
    "    newThetas = linAlgGradientDescent(values, labels, thetasAlongDescent[-1], alpha)\n",
    "    thetasAlongDescent.append(newThetas)\n",
    "    \n",
    "#Plot the predictions for the untrained model and the trained model using a threshold of 0.5 (50%)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize = (10,10))\n",
    "\n",
    "initialPredictions = linAlgLogRegHypothesis(values, thetas)\n",
    "initialClassPred   = np.where(initialPredictions<=0.5,0,1)\n",
    "finalPredictions   = linAlgLogRegHypothesis(values, thetasAlongDescent[-1])\n",
    "finalClassPred     = np.where(finalPredictions<=0.5,0,1)\n",
    "sigmoidValues      = mySigmoid(values)\n",
    "\n",
    "ax[0].scatter(vals[np.where(np.ravel(initialClassPred) == 0)],\n",
    "              sigmoidValues[np.where(np.ravel(initialClassPred) == 0)],\n",
    "              color = \"darkred\", label = \"predicted negative class\")\n",
    "ax[0].scatter(vals[np.where(np.ravel(initialClassPred) == 1)],\n",
    "              sigmoidValues[np.where(np.ravel(initialClassPred) == 1)],\n",
    "              color = \"darkgreen\", label = \"predicted positive class\")\n",
    "ax[0].set_xlabel(\"Values\")\n",
    "ax[0].set_ylabel(\"Sigmoid output\")\n",
    "ax[0].vlines(thresholdXLocation, -0.2, 1.2, linestyle = \"dashed\", label = \"actual boundary class 0 and 1\")\n",
    "ax[0].set_ylim([0, 1])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[0].text(0.9, 0.2, 'Cost: ' + str(np.round(np.sum(costAlongDescent[0]), 2)), horizontalalignment='center', \n",
    "                                    verticalalignment='center', transform=ax[0].transAxes)\n",
    "ax[0].text(0.9, 0.1, 'Thetas: ' + str(np.ravel(thetas)), horizontalalignment='center', \n",
    "                                    verticalalignment='center', transform=ax[0].transAxes)\n",
    "\n",
    "ax[1].scatter(vals[np.where(np.ravel(finalClassPred) == 0)],\n",
    "              sigmoidValues[np.where(np.ravel(finalClassPred) == 0)],\n",
    "              color = \"darkred\", label = \"predicted negative class\")\n",
    "ax[1].scatter(vals[np.where(np.ravel(finalClassPred) == 1)],\n",
    "              sigmoidValues[np.where(np.ravel(finalClassPred) == 1)],\n",
    "              color = \"darkgreen\", label = \"predicted positive class\")\n",
    "ax[1].set_xlabel(\"Values\")\n",
    "ax[1].set_ylabel(\"Sigmoid output\")\n",
    "ax[1].vlines(thresholdXLocation, -0.2, 1.2, linestyle = \"dashed\", label = \"actual boundary class 0 and 1\")\n",
    "ax[1].set_ylim([0, 1])\n",
    "ax[1].legend()\n",
    "\n",
    "ax[1].text(0.9, 0.2, 'Cost: ' + str(np.round(np.sum(costAlongDescent[-1]), 2)), horizontalalignment='center',\n",
    "    verticalalignment='center', transform=ax[1].transAxes)\n",
    "ax[1].text(0.9, 0.1, 'Thetas: ' + str(np.ravel(np.round(thetasAlongDescent[-1], 2))), horizontalalignment='center', \n",
    "                                    verticalalignment='center', transform=ax[1].transAxes)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21bc24",
   "metadata": {},
   "source": [
    "## Plotting parameter changes along gradient descent\n",
    "Let's plot how the parameters of our linear regression change along gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70655446",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaTimeCourse = np.hstack(thetasAlongDescent)\n",
    "\n",
    "figTheta, axTheta = plt.subplots()\n",
    "axTheta.plot(range(0, stepsToTake), thetaTimeCourse[0,1:], label = \"Theta0\")\n",
    "axTheta.plot(range(0, stepsToTake), thetaTimeCourse[1,1:], label = \"Theta1\")\n",
    "\n",
    "print(\"Initial cost: \" + str(costAlongDescent[0]))\n",
    "print(\"Final cost: \" + str(costAlongDescent[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae460b4c",
   "metadata": {},
   "source": [
    "## Other optimisation algorithms\n",
    "\n",
    "Gradient descent is one way to get the minimum of a function, but it is certainly not the only one. You might have noticed that we ran it for a thousand iterations above and it moved slowly. This is because the gradients become very small when the predictions are almost correct (if your hypothesis predicts 0.001 but it should be 0 exactly, you can see how that would lead to exceedingly small steps to correct this). Therefore, we might wish to turn to other optimisation methods. <br> There are many other ones that incorporate all sorts of smart tricks. Those of you who've followed the BiBC Essentials Course should remember some of them! Here, you're going to optimise logistic regression not using simple gradient descent, but one of the more involved optimisers. We won't go into how they work (that's a numerical mathematics course in its own right), just know that they are faster and will require inputs in a slightly different form. You need to:\n",
    "\n",
    "* Skim [this](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html) and try to understand what the first 4 arguments mean.\n",
    "* You'll need to supply this advanced optimisation algorithm with 1: the function to optimise, and 2. (optionally, but we will do it) its derivative(s).\n",
    "* To do that:\n",
    "    1. Make a copy of the gradient descent function, with the following arguments: `linAlgGradient(thetas, x, y, hypothesis = 'linAlgLogRegHypothesis')`. Notice how the arguments have been reordered. This is because `fmin_bfgs` wants the first argument of each function to be its parameters (thetas).<br> This copy should **not return the new thetas** (as in our normal gradient descent function) **but return the array of gradients** (partial derivatives) of the cost function. Hence this function doesn't need an alpha parameter. <br> Make sure to wrap the returned values in `np.ravel()` so they are in the dimension fmin_bfgs wants (so something like  `return np.ravel(partialDerivativesThetas)`).\n",
    "    2. The function to optimise is the cost function. Make a copy of it that has the thetas as its first arguments `costFuncLogReg2(thetas, x, y)`. Other than that it stays exactly the same.\n",
    "* Once that's done, run the code cell below it to see the resulting thetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca78c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2dddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the values\n",
    "from scipy.optimize import fmin_bfgs\n",
    "labels = predictedClass[:, np.newaxis]\n",
    "values = vals[:, np.newaxis]\n",
    "thetas = np.array([[-5.], [5.]])\n",
    "\n",
    "#final function call\n",
    "\n",
    "optimisedOutput = fmin_bfgs(costFuncLogReg2, thetas, linAlgGradient, (values, labels), retall = True)\n",
    "finalThetas     = optimisedOutput[0]\n",
    "\n",
    "\n",
    "#plotting\n",
    "\n",
    "figThetaBFGS, axThetaBFGS = plt.subplots()\n",
    "axThetaBFGS.plot(range(0, len(optimisedOutput[1])), list(zip(*optimisedOutput[1]))[0], label = \"Theta0\")\n",
    "axThetaBFGS.plot(range(0, len(optimisedOutput[1])), list(zip(*optimisedOutput[1]))[1], label = \"Theta1\")\n",
    "axThetaBFGS.text(0.1, 0.4, 'Cost initial thetas: ' + str(np.round(costFuncLogReg(values, labels, thetas), 2)),\n",
    "    verticalalignment='center', transform=axThetaBFGS.transAxes)\n",
    "axThetaBFGS.text(0.1, 0.2, 'Cost final thetas: ' + str(np.round(costFuncLogReg(values, labels, finalThetas), 2)),\n",
    "    verticalalignment='center', transform=axThetaBFGS.transAxes)\n",
    "axThetaBFGS.legend()\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots()\n",
    "initialPredictions = np.ravel(linAlgLogRegHypothesis(values, thetas))\n",
    "finalPredictions   = np.ravel(linAlgLogRegHypothesis(values, finalThetas))\n",
    "initialClasses     = np.where(initialPredictions<=0.5,0,1)\n",
    "finalClasses       = np.where(finalPredictions<=0.5,0,1)\n",
    "\n",
    "ax.scatter(values[np.where(initialClasses == 0)], initialPredictions[np.where(initialClasses == 0)],\n",
    "                           color = \"red\", label = \"initial negative class predictions\")\n",
    "ax.scatter(values[np.where(initialClasses == 1)], initialPredictions[np.where(initialClasses == 1)],\n",
    "                           color = \"green\", label = \"initial positive class predictions\")\n",
    "\n",
    "ax.scatter(values[np.where(finalClasses == 0)], finalPredictions[np.where(finalClasses == 0)],\n",
    "                           color = \"darkred\", label = \"final negative class predictions\")\n",
    "ax.scatter(values[np.where(finalClasses == 1)], finalPredictions[np.where(finalClasses == 1)],\n",
    "                           color = \"darkgreen\", label = \"final positive class predictions\")\n",
    "ax.legend()\n",
    "\n",
    "classMemberShipFig, classMemberShipAx = plt.subplots()\n",
    "randomYPositions = rng.random(len(labels)) * 0.01\n",
    "classMemberShipAx.scatter(values[np.where(labels == 0)], randomYPositions[np.where(np.ravel(labels) == 0)],\n",
    "                          color = 'darkred', label = \"negative class in data\")\n",
    "classMemberShipAx.scatter(values[np.where(labels == 1)], randomYPositions[np.where(np.ravel(labels) == 1)],\n",
    "                          color = 'darkgreen', label = \"positive class in data\")\n",
    "classMemberShipAx.legend()\n",
    "classMemberShipAx.set_ylim((-0.1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d5e67",
   "metadata": {},
   "source": [
    "## Conclusion BFGS optimisation\n",
    "\n",
    "You see that the optimal situation (when we don't consider generalisation) is to simply parameterise the sigmoid function such that it become a so-called step function: <div>\n",
    "<img src=\"Dirac_distribution_CDF.svg\" width=\"500\"/>\n",
    "</div>\n",
    "at exactly the point where the class shifts from 0 to 1. You see that all the predictions before this threshold are (up to some numerical precision) 0, and all the ones after it are 1. To get this step function you need rather extreme values for $\\theta_0$ and $\\theta_1$. With gradient descent it would have taken ages to find them, with BFGS it's rather quick. Note that the warnings are because certain values, when exponentiated, become np.nan (too large or too small to hold in a floating point value) and because log(0) is -Inf. In the end, these values are ignored due to `np.nansum` and the -Inf values are also not tallied. Hence, nothing to worry about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e4fa9",
   "metadata": {},
   "source": [
    "## Working with more features\n",
    "\n",
    "Until now we've worked with just one feature, which is somewhat unfulfilling. Let's change to two features and see what happens then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomFeat1, randomFeat2 = rng.standard_normal(100), rng.standard_normal(100)\n",
    "labels = np.logical_and(randomFeat1 <= 0.25,randomFeat2 <= 0.25).astype(int)\n",
    "\n",
    "plt.scatter(randomFeat1[labels == 0], randomFeat2[labels == 0], color = \"red\", label = \"negative samples\")\n",
    "plt.scatter(randomFeat1[labels == 1], randomFeat2[labels == 1], color = \"darkgreen\",\n",
    "            label = \"positive samples\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c6266",
   "metadata": {},
   "source": [
    "## Calculating the decision boundary\n",
    "Up to you to:\n",
    "* Calculate the cost for the initial thetas\n",
    "* Optimise the thetas using BFGS\n",
    "* Plot the decision boundary and the total cost before and after optimisation (helpful code provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d860a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialThetas = np.array([[0.2], [0.3], [-0.1]])\n",
    "features = np.hstack([randomFeat1[:, np.newaxis], randomFeat2[:, np.newaxis]])\n",
    "twoDLabels = np.squeeze(labels)[:, np.newaxis]\n",
    "\n",
    "#your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#helpful for plotting\n",
    "def getDecisionBoundaryYValues(feat1, thetas):\n",
    "    \"\"\"Returns the y values for a decision boundary based on thetas for a 2D logistic regression problem\"\"\"\n",
    "    thetas = np.ravel(thetas)\n",
    "    b, w1, w2 = thetas\n",
    "    intercept = -b/w2\n",
    "    gradient  = -w1/w2\n",
    "    yValues   = gradient * feat1 + intercept\n",
    "    return yValues\n",
    "\n",
    "#plot set-up\n",
    "finalFig, finalAx = plt.subplots(figsize=(10,10))\n",
    "finalAx.scatter(randomFeat1[labels == 0], randomFeat2[labels == 0], color = \"red\", label = \"negative samples\")\n",
    "finalAx.scatter(randomFeat1[labels == 1], randomFeat2[labels == 1], color = \"darkgreen\",\n",
    "            label = \"positive samples\")\n",
    "\n",
    "#Up to you to add the decision boundary lines!\n",
    "\n",
    "finalAx.legend()\n",
    "finalAx.set_xlabel(\"Feature 1\")\n",
    "finalAx.set_ylabel(\"Feature 2\")\n",
    "finalAx.set_ylim(-3,3)\n",
    "finalAx.set_xlim(-3,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcc8cf",
   "metadata": {},
   "source": [
    "## What I'd like you to remember here:\n",
    "* What the sigmoid function is and does, and why it is useful for classification.\n",
    "* How to plot basic graphs using matplotlib\n",
    "* How the cost function for logistic regression works (another name for it is the cross-entropy), and why we don't use the mean-squared error (MSE).\n",
    "* That there's more under the Sun than simple gradient descent: more advanced optimisation algorithms exist. You don't need to know how they work, but do need to know that they exist and you might want to use them.\n",
    "\n",
    "\n",
    "## The end\n",
    "\n",
    "Congratulations, you've implemented logistic regression, both using gradient descent and using a more advanced optimisation algorithm. You might feel that you don't know what the heck BFGS is doing, and that's fine: the modern ML libraries you'll use when applying your knowledge to (biological) problems use appropriate optimisation methods behind the scenes, or if you need to select them manually you can simply test and see which works best. The thing you should remember is that if gradients get small, gradient descent can progress extremely slowly. \n",
    "\n",
    "## Survey\n",
    "No worries, I see you over there, pining for that sweet, sweet survey. I [got you fam](https://docs.google.com/forms/d/e/1FAIpQLScoQynzU2aQrduUsmz8eimbE85Cn4_ytWJfnRTtEtcoHlLCaw/viewform?usp=sf_link)!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
