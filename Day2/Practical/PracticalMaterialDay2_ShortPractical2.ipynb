{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fa6c2",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 2\n",
    "\n",
    "Welcome to the second practical of today. Here, you will work on implementing regularised logistic regression, as well as implementing cross-validation on some data and making an ROC curve yourself. First run the two cells below to set things up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from scipy.optimize import fmin_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#important functions\n",
    "def mySigmoid(data):\n",
    "    output = 1/(1+ np.exp(-data))\n",
    "    return output\n",
    "\n",
    "def linAlgRegHypothesis(data, thetas):\n",
    "    data = np.array(data)\n",
    "    oneFeatToAdd = np.ones(len(data))\n",
    "    newFeatArray = np.c_[oneFeatToAdd, data]\n",
    "    #make sure thetas are always of the form np.array([[theta1], [theta2]]), i.e. column vector\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    predictions = newFeatArray @ thetas\n",
    "    return predictions\n",
    "\n",
    "def linAlgLogRegHypothesis(data, thetas):\n",
    "    output = mySigmoid(linAlgRegHypothesis(data, thetas))\n",
    "    return output\n",
    "\n",
    "def costFuncLogReg(x, y, thetas):\n",
    "    predictions      = linAlgLogRegHypothesis(x, thetas)\n",
    "    costsPerSample   = -y * np.log(predictions) - (1-y) * np.log(1 - predictions)\n",
    "    totalCosts       = np.nansum(1/len(x) * costsPerSample)\n",
    "    return totalCosts\n",
    "\n",
    "def makeCrossValData(dataFrame, k=10):\n",
    "    '''function to make splits into training and validation sets.\n",
    "    Outputs two lists of length k, where each element is the indices of samples to train on for that fold, \n",
    "    and the indices of samples to test on for that fold, respectively.'''\n",
    "    #shuffle data\n",
    "    dataFrame = dataFrame.sample(frac=1)\n",
    "    m = len(dataFrame)\n",
    "    #see how many equal-sized sets you can make\n",
    "    dataPerSplit = int(np.floor(m/k))\n",
    "    dataPartitions = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(0,k):\n",
    "        #make a list of all the samples for each fold\n",
    "        dataPartitions.append(list(range(counter,counter+dataPerSplit)))\n",
    "        counter += dataPerSplit\n",
    "\n",
    "    samplesEquallySplit = k * dataPerSplit\n",
    "    if not samplesEquallySplit == m:\n",
    "        #after making equal splits there will be samples left, i.e. you cannot always make k exactly evenly sized subsets.\n",
    "        #randomly assign left over samples to folds after\n",
    "        toDivide = m-samplesEquallySplit\n",
    "        for extraSampleIndex in range(counter, counter+toDivide):\n",
    "            #only assign to lists of samples that have the current minimum amount of samples\n",
    "            currentSubsetSizes = np.array([len(subset) for subset in dataPartitions])\n",
    "            assignTo = np.random.choice(np.where(currentSubsetSizes == np.min(currentSubsetSizes))[0])\n",
    "            dataPartitions[assignTo].append(extraSampleIndex)\n",
    "    \n",
    "    #Now make the final cross-validation set: make k sets, each set has (k-1)/k folds to train on, and 1 fold to test on.\n",
    "    testSet = []\n",
    "    trainSet = []\n",
    "    for validationSetIndex in range(0,k):\n",
    "        #put 1 fold in the test set\n",
    "        testSet.append(dataPartitions[validationSetIndex])\n",
    "        #put all other folds in the train set\n",
    "        trainSet.append(dataPartitions.copy())\n",
    "        trainSet[validationSetIndex].pop(validationSetIndex)\n",
    "        #this line makes sure all training set indices are in one big list, rather than k-1 small lists. \n",
    "        trainSet[validationSetIndex] = [item for sublist in trainSet[validationSetIndex] for item in sublist]\n",
    "    \n",
    "    return dataFrame, trainSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8c72a",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "Regularisation is a method of automatically constraining how much your model can (over)fit on the training data. We add some factor (regularisation weight $\\lambda$) times the sum of squares of the parameter (excluding the intercept ($\\theta_0$) to the cost function. In this way, the model cannot pick extremely large values for the parameters, i.e. when you have 100 features, the model is forced to only have high $\\theta$ parameters for those features that matter a lot for correct classification, while having extremely low or even 0 values for features that don't. Hence, regularisation also automatically selects features that are of importance to your problem: feature selection! Note that once you have trained the model and want to know the cost on the validation/test set, you should not used regularised cost: you care about your performance in the end (which you hope is better because you constrain the parameters during fitting).\n",
    "\n",
    "* To get started, change your costFuncLogReg to have an extra argument `lambda_ = 0` ( _ because lambda is a keword for anonymous functions), that, if set to a value higher than 0, causes regularisation to be performed.\n",
    "* Make sure to exclude the bias/intercept term ($\\theta_0$) from this. By convention this is not regularised.\n",
    "* While you are at it, also reorder the arguments to `thetas, x, y, lambda_=0` so it is easier to use BFGS or another optimizer if we want to!\n",
    "\n",
    "Hint:\n",
    "* Remember that the regularised logistic regression cost function is:\n",
    "![APicture](RegLogRegEq.PNG) You already had the first part implemented, you only need to add the second part!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2aaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8ee45",
   "metadata": {},
   "source": [
    "## Changing gradient descent\n",
    "\n",
    "The gradients should also change. Luckily, since all that's added is a plus term, the change is extremely minor:\n",
    "![gradients_logreg](GradientsRegLogReg.PNG)\n",
    "\n",
    "* Up to you to implement the changes in the `linAlgGradientDescent` function. Add another `lambda_ = 0` argument and change the gradients as needed.\n",
    "* After that's done, let's refactor: make one function called `computeGradients()` that computes and returns the gradients. Make another function called `gradientDescentStep()` that takes a step using current thetas, those gradients, and an alpha value. In this way, we can use the first one if we want to use BFGS, and the second one if we want to use gradient descent proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91272388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old function\n",
    "def linAlgGradientDescent(x, y, thetas, alpha, hypothesis = \"linAlgLogRegHypothesis\") :\n",
    "    m = len(x)\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    preds  = globals()[hypothesis](x, thetas)\n",
    "    if preds.shape != (m, 1):\n",
    "        preds  = preds[:, np.newaxis]\n",
    "    if y.ndim < 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation  = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    finalGradientSteps = alpha/m * gradientSummation\n",
    "    newThetas          = thetas - finalGradientSteps.T\n",
    "    return newThetas\n",
    "\n",
    "\n",
    "# your answer\n",
    "\n",
    "\n",
    "# refactor into separate functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd32fd36",
   "metadata": {},
   "source": [
    "## Loading in some data for testing\n",
    "\n",
    "Let's train on the Pima Indians dataset, which contains information on multiple clinical variables and whether or not patients have diabetes. The below code loads in the data. Up to you to investigate this data somewhat:\n",
    "\n",
    "* Are there any NaNs in the data?\n",
    "* Are there other values that seem circumspect? Name 2 examples. How many of these circumspect values are there in these features?\n",
    "* How many cases and controls are there? Is this a balanced dataset?\n",
    "\n",
    "Hint(s):\n",
    "* Use the `.describe` method of the dataframe to help you answer these questions.\n",
    "* Remember that you can index a dataframe using `df.loc[df[\"colName\"] < 12, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d804a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetesData = pd.read_csv(\"PimaIndiansDiabetes.csv\")\n",
    "\n",
    "#your answers below \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8a8b1",
   "metadata": {},
   "source": [
    "## Cleaning up the dataset\n",
    "\n",
    "The dirty secret of ML is that you spend most of your time cleaning data. So you'll have to spend some time on that here. Do the following:\n",
    "\n",
    "* Replace the 0 values with `np.nan` (**Note**: be aware that you shouldn't do this for all columns. Think about it.)\n",
    "* Use [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) to impute values that are missing for those columns where you inserted NaNs. Those who have followed the BiBC Essentials Course might remember K-Nearest Neighbour clustering. This function determines the (by default) 5 most similar samples (based on data that is _not_ missing) and sets the bmi/glucose level, etc. to the mean of their values. Euclidean distance is used. We will discuss K-Nearest Neighbour clustering in two days. For now, you can just use it. To do so, use `a = KNNImputer(missing_values = np.nan)` followed by `imputedData = a.fit_transform(nonImputedData)`.\n",
    "* Note that this turns the DataFrame into a numpy array, which is not a problem but it's good to know. To make it into a dataframe again, use `pd.DataFrame(yourArray)`. Be sure to add back features that you might have removed because you didn't want to impute them.\n",
    "* Mean-normalise (i.e. subtract the mean and divide by the standard deviation) the features using the function from yesterday (provided below). This should be done on all the data except the labels. Note that this function expected a DataFrame as input (DataFrames automatically apply features like .mean() by column, numpy arrays don't do that).\n",
    "* Put the class into a `np.array` (a column vector) called `diabetesClassLabels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def createNormalisedFeatures(dataFrame, mode = \"range\"):\n",
    "    featureMeans = dataFrame.mean()\n",
    "    if mode == \"range\":\n",
    "        featureRanges = dataFrame.max() - dataFrame.min()\n",
    "        normalisedFeatures = (dataFrame - featureMeans)/featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = dataFrame.std()\n",
    "        normalisedFeatures = (dataFrame - featureMeans)/featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "    return None\n",
    "\n",
    "\n",
    "# your answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9f384",
   "metadata": {},
   "source": [
    "## Testing your new functions' mettle\n",
    "\n",
    "Okay, now we can train regularised logistic regression on this data. Let's **use lambda values of 0, 0.5, 1, 5, 10, 100, and 1000**. We'll downsample the data so we have equal amounts of the positive and negative class, and train the classifier on 80% of the training data while testing on 20% held-out data (normally we'd use cross-validation but let's not put that extra level of complication in here as well). \n",
    "\n",
    "The visualisation of a decision boundary/what has been learned is somewhat complex: we can't just draw some boundary in 2D as our data isn't 2D but 8D.\n",
    "We'll reduce the dimensionality to two dimensions using PCA, and then show in those two dimensions which points are positive or negative for diabetes, and what the classifier predicts everywhere in that plane. This is done for you. We'll talk about dimensionality reduction on the last day of this week. For now, know that, by its nature, dimensionality reduction will lose some of the true differences in your data, so visualisation of the decision boundary in this 2D space is bound to be an approximation, and cannot capture completely what your classifier is doing (as it's separating things in 8 dimensions rather than 2)! \n",
    "\n",
    "Your job:\n",
    "* Make a list of the lambda values to train on (`lambdaValues`), an empty list to store the test cost in (`testCostList`), and a list for the final thetas after gradient descent (`finalThetaList`).\n",
    "* Downsample the normalised diabetesData: remove random rows of the controls so you have equal # of non-diabetes and diabetes cases. You could use `np.random.choice(a=rowIndicesOfoRowsThatDon'tHaveDiabetes, size = howManySamplesNeedToBeRemoved, replace = False)`, where you then remove (`np.delete()` can be useful) those rows from the feature and class label array. You'll probably also need `np.ravel(diabetesClassLabels)` and `np.where()`. <br> Save the new data as `equalClassSizeDiabetesData` and `equalClassSizeClassLabels` for the labels.\n",
    "* Randomly sample 80% of that for training, and save the rest for testing. **_Code for this is given below!_**.\n",
    "* Now make a `for`-loop that loops over the different lambdaValues.\n",
    "* In that loop, make another loop that performs 300 gradient descent steps with an alpha of 0.2 on `trainDataDiabetes`.\n",
    "* After that's done, calculate the cost on `testDataDiabetes` **without regularisation (lambda of 0)**. Remember: you don't use the regularisation parameter in the final predictions, because you use it _during training_ to prevent overfitting, and then want to know how well you really do on the test data. \n",
    "* Append the result to the `testCostList`.\n",
    "* Finally, look at the DataFrame containing the theta parameters found for the different values of lambdas, and the cost calculated on the test set (code to make it is given below). What do you see? \n",
    "\n",
    "Hints:\n",
    "* There are many steps here. If you get stuck on one, ask a question or look at the answers to see how to do that step.\n",
    "* `np.where` returns a tuple, of which you need the first element.\n",
    "* Note that you can always insert a new cell above or below the current one for testing or debugging using `escape + a` or `escape + b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure everyone gets the same split\n",
    "np.random.seed(500)\n",
    "startThetas = np.array([0] * 9)[:,np.newaxis]\n",
    "nSteps      = 500\n",
    "alpha       = 0.2\n",
    "\n",
    "#make lists\n",
    "\n",
    "#downsample the data\n",
    "\n",
    "#     code for dividing into 80% and 20%\n",
    "#     uncomment when you have done the above using Ctrl + /\n",
    "\n",
    "\n",
    "# nrSamplesToTake        = int(np.ceil(0.8*np.sum(equalClassSizeClassLabels == 0)))\n",
    "# negativeSampleIdxTrain = np.random.choice(np.arange(0,np.sum(equalClassSizeClassLabels == 0)), \n",
    "#                                        size = nrSamplesToTake, replace = False)\n",
    "# positiveSampleIdxTrain = np.random.choice(np.arange(0,np.sum(equalClassSizeClassLabels == 1)), \n",
    "#                                        size = nrSamplesToTake, replace = False)\n",
    "# positiveSamplesTrain, positiveClassLabelsTrain = equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1,:][positiveSampleIdxTrain,:], equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1,:][positiveSampleIdxTrain,:]\n",
    "# negativeSamplesTrain, negativeClassLabelsTrain = equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0,:][negativeSampleIdxTrain,:], equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0,:][negativeSampleIdxTrain,:]\n",
    "# trainDataDiabetes        = np.vstack([positiveSamplesTrain, negativeSamplesTrain])\n",
    "# trainClassLabelsDiabetes = np.vstack([positiveClassLabelsTrain, negativeClassLabelsTrain])\n",
    "\n",
    "\n",
    "# negativeSampleIdxTest = np.array([i for i in np.arange(0,np.sum(equalClassSizeClassLabels == 0)) if i not in negativeSampleIdxTrain])\n",
    "# positiveSampleIdxTest = np.array([i for i in np.arange(0,np.sum(equalClassSizeClassLabels == 1)) if i not in positiveSampleIdxTrain])\n",
    "# positiveSamplesTest, positiveClassLabelsTest = equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 1,:][positiveSampleIdxTest,:], equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 1,:][positiveSampleIdxTest,:]\n",
    "# negativeSamplesTest, negativeClassLabelsTest = equalClassSizeDiabetesData[np.ravel(equalClassSizeClassLabels) == 0,:][negativeSampleIdxTest,:], equalClassSizeClassLabels[np.ravel(equalClassSizeClassLabels) == 0,:][negativeSampleIdxTest,:]\n",
    "# testDataDiabetes        = np.vstack([positiveSamplesTest, negativeSamplesTest])\n",
    "# testClassLabelsDiabetes = np.vstack([positiveClassLabelsTest, negativeClassLabelsTest])\n",
    "\n",
    "# your looping code, performing gradient descent for each lambda and\n",
    "# calculating the cost on the test set after it's done, should go here:\n",
    "\n",
    "\n",
    "\n",
    "#     code to make a final DataFrame to show what happens:\n",
    "#     Uncomment all this code at once by selecting it and pressing Ctrl + /\n",
    "\n",
    "# finalThetas = [np.ravel(elem) for elem in finalThetaList]\n",
    "# dataFrame = pd.DataFrame(np.c_[np.vstack(finalThetas), np.vstack(testCostList)])\n",
    "# columnNames = [\"theta_\" + str(elem) for elem in [0, 1, 2, 3, 4, 5, 6, 7, 8]]\n",
    "# columnNames.append(\"testSetCost\")\n",
    "# dataFrame.columns = columnNames\n",
    "# dataFrame.set_index(lambdaValues, inplace = True)\n",
    "# display(dataFrame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bbb54",
   "metadata": {},
   "source": [
    "## Regularised logistic regression results + visualisation\n",
    "\n",
    "If all goes well, you will see that the cost on the test set is lowest when using $\\lambda = 10$. In other words: unregularised logistic regression would have overfit, tuning the parameters _too specifically_ to the values in the training set, which increases the cost on the unseen test set. Regularisation guards against this by penalising too large parameters. Note that if you change the seed, sometimes you actually get lower cost values without regularisation: in that case there was apparently a lucky split of the data such that overfitting was very difficult anyway. However, you don't want to be dependent on lucky splits, so you can use regularisation to make sure that there's no overfitting to the training data.\n",
    "\n",
    "We can visualise what logistic regression is doing in a dimension-reduced space. Below, the first plot shows how the data points look when reduced to 2 dimensions using PCA. The following plots show how the decision boundary changes for different $\\lambda$ values. Since we evaluate it for the test data only, I have kept only those points in the plots that show what the classifier has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbe9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCAForTwoD = PCA(n_components = 2)\n",
    "PCAForTwoD.fit(normDiabFeats)\n",
    "coordsXYOriginalData  = list(zip(*PCAForTwoD.transform(normDiabFeats)))\n",
    "coordsXYTrainData     = list(zip(*PCAForTwoD.transform(trainDataDiabetes)))\n",
    "coordsXYTestData      = list(zip(*PCAForTwoD.transform(testDataDiabetes)))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(np.array(coordsXYTrainData[0])[np.ravel(trainClassLabelsDiabetes) == 1],\n",
    "           np.array(coordsXYTrainData[1])[np.ravel(trainClassLabelsDiabetes) == 1],\n",
    "           facecolor = \"green\", edgecolor = 'black', label = \"Positive samples training set\")\n",
    "ax.scatter(np.array(coordsXYTrainData[0])[np.ravel(trainClassLabelsDiabetes) == 0],\n",
    "           np.array(coordsXYTrainData[1])[np.ravel(trainClassLabelsDiabetes) == 0],\n",
    "           facecolor = \"red\", edgecolor = 'black', label = \"Negative samples training set\")\n",
    "ax.scatter(np.array(coordsXYTestData[0])[np.ravel(testClassLabelsDiabetes) == 0],\n",
    "           np.array(coordsXYTestData[1])[np.ravel(testClassLabelsDiabetes) == 0],\n",
    "           facecolor = \"blue\", edgecolor = 'black', label = \"Negative samples test set\")\n",
    "ax.scatter(np.array(coordsXYTestData[0])[np.ravel(testClassLabelsDiabetes) == 1],\n",
    "           np.array(coordsXYTestData[1])[np.ravel(testClassLabelsDiabetes) == 1],\n",
    "           facecolor = \"yellow\", edgecolor = 'black', label = \"Positive samples test set\")\n",
    "ax.legend()\n",
    "\n",
    "for index, lambda_ in enumerate(lambdaValues):\n",
    "    #predict on the test set given learned thetas\n",
    "    predictionsOnTestSet         = linAlgLogRegHypothesis(testDataDiabetes, finalThetaList[index])\n",
    "    predictedClassLabels         = np.ravel(np.where(predictionsOnTestSet <= 0.5, 0, 1))\n",
    "    #make a series of values in the PCA space to transform back, and predict using the learned classifier\n",
    "    #in that way, we can colour the background with the predicted probabilities.\n",
    "    PCAX, PCAY                   = np.meshgrid(np.linspace(-6, 6, 1000), np.linspace(-6, 6, 1000))\n",
    "    backTransformedFeatures      = PCAForTwoD.inverse_transform(np.c_[np.ravel(PCAX), np.ravel(PCAY)])\n",
    "    classifierContourPredictions = linAlgLogRegHypothesis(backTransformedFeatures, finalThetaList[index])\n",
    "    classifierContourPredictions = classifierContourPredictions.reshape(PCAX.shape)\n",
    "    #make the figure\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    contour = ax.contourf(PCAX, PCAY, classifierContourPredictions, cmap=plt.cm.coolwarm, alpha=0.8, levels = 10)\n",
    "    fig.colorbar(contour, label = \"probability of being class 1\", ticks = list(np.arange(0,1.1,0.1)))\n",
    "    ax.scatter(np.array(coordsXYTrainData[0]),\n",
    "               np.array(coordsXYTrainData[1]),\n",
    "               facecolor = \"none\", edgecolor = 'black', alpha = 0.3, label = \"Training data\")\n",
    "    ax.scatter(np.array(coordsXYTestData[0])[np.logical_and(predictedClassLabels == 1, np.ravel(testClassLabelsDiabetes) == 1)],\n",
    "               np.array(coordsXYTestData[1])[np.logical_and(predictedClassLabels == 1, np.ravel(testClassLabelsDiabetes) == 1)],\n",
    "               facecolor = \"#1b9e77\", edgecolor = 'black', label = \"Predicted positive and really positive\")\n",
    "    ax.scatter(np.array(coordsXYTestData[0])[np.logical_and(predictedClassLabels == 1, np.ravel(testClassLabelsDiabetes) == 0)],\n",
    "               np.array(coordsXYTestData[1])[np.logical_and(predictedClassLabels == 1, np.ravel(testClassLabelsDiabetes) == 0)],\n",
    "               facecolor = \"#d95f02\", edgecolor = 'black', label = \"Predicted positive but really negative\")\n",
    "    ax.scatter(np.array(coordsXYTestData[0])[np.logical_and(predictedClassLabels == 0, np.ravel(testClassLabelsDiabetes) == 0)],\n",
    "               np.array(coordsXYTestData[1])[np.logical_and(predictedClassLabels == 0, np.ravel(testClassLabelsDiabetes) == 0)],\n",
    "               facecolor = \"#7570b3\", edgecolor = 'black', label = \"Predicted negative and really negative\")\n",
    "    ax.scatter(np.array(coordsXYTestData[0])[np.logical_and(predictedClassLabels == 0, np.ravel(testClassLabelsDiabetes) == 1)],\n",
    "               np.array(coordsXYTestData[1])[np.logical_and(predictedClassLabels == 0, np.ravel(testClassLabelsDiabetes) == 1)],\n",
    "               facecolor = \"#e7298a\", edgecolor = 'black', label = \"Predicted negative but really positive\")\n",
    "    ax.set_title(\"Dimension reduced classifier visualisation for lambda = \" + str(lambda_) + \"\\n mean cost: \" + str(testCostList[index]))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd07cb2",
   "metadata": {},
   "source": [
    "## Conclusion visualisation\n",
    "\n",
    "The main thing to note is, of course, that such a dimension-reduced visualisation is imperfect. You do clearly see that as the lambda values increase, so too does the size of the banded regions become more uniform: the classifier has learned less because with lambdas of 100 and 1000 we are hugely penalising it for any fit to the training data.\n",
    "\n",
    "## Classifier performance\n",
    "\n",
    "We've talked in the lectures about the performance of a classification algorithm. We want to know the true positive rate and false positive rates for a given threshold, but also the classifier's performance over a range of thresholds. It is not too difficult to make a ROC curve yourself. Let's do that now for the best classifier (with the lowest mean cost on the test set).\n",
    "\n",
    "Up to you to:\n",
    "* Make a range of 200 thresholds (from 1 to 0) for saying something is the positive set (use `np.linspace` for this).\n",
    "* Make two empty lists: `truePositiveRates` and `trueNegativeRates`.\n",
    "* Make predictions on the `testDataDiabetes` using the best set of learned thetas (which you can manually select).\n",
    "* Make a for loop over the different thresholds you defined. Within that loop:\n",
    "    * Turn the predictions into class labels using `np.where` and the current threshold value.\n",
    "    * Calculate the true positive rate (sensitivity/recall) and append it to the list.\n",
    "    * Calculate the true negative rate and append it to the list.\n",
    "* Finally make a plot of the sensitivity (true positive rate) on the y-axis and 1-specificity (1-TNR) on the x-axis. (use `fig, ax = plt.subplots()` and `ax.plot()`). Don't forget to set the axis labels and a title!\n",
    "\n",
    "See the relevant excerpt from the slide below, and look [here](https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/) for more explanation if you want it! <br> ![SensitivityAndSpecificity](SensitivityAndSpecificity.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477afa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78ed82",
   "metadata": {},
   "source": [
    "## What I'd like you to remember here:\n",
    "* What regularisation is, and how it works (penalising large weights for parameters, thereby forcing the algorithm to focus on those that really give it a lot of _bang for its buck_ and decreasing overfitting)\n",
    "* How to implement regularisation, and how the parameter $\\lambda$ affects it\n",
    "* How to do some basic cleaning on a dataset, and what _the idea_ of imputation is (specifically of a KNNImputer)\n",
    "* How to make a ROC plot, and what exactly is depicted on it, as well as why we might want to compare something like ROC AUC between classifiers, rather than accuracy.\n",
    "\n",
    "## Final words\n",
    "\n",
    "Congratulations. You've implemented regularised logistic regression on a real dataset (that you cleaned up yourself) and made your own ROC curve. We'll now move on to multiclass logistic regression and then to neural networks!\n",
    "\n",
    "## Survey\n",
    "\"I want a Survey, hey! Giving feedback for the very first time. I want a su-u-u-u-rvey, got some feedback, on my mi-i-i-n-d\". Thanks Weird Al, [very cool](https://www.youtube.com/watch?v=notKtAgfwDA). Here you go: [clickety-click](https://docs.google.com/forms/d/e/1FAIpQLSfaeqtRTz5KMqcmxQuOI5GYWHMejjh5_yuiCNSnNblpdKb0hQ/viewform?usp=sf_link)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
