{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 2 day 6\n",
    "\n",
    "Here you'll be burying the traumatic experience of training your own neural networks by doing it _much_ more easily using Keras. We'll work on the same MNIST dataset as before. First run the two code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#I don't know why, but for some people these 2 extra imports are necessary for Keras to work\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation, Flatten, Conv2D, MaxPooling2D\n",
    "# make it so you can use exactly the code from the tutorial\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11f405",
   "metadata": {},
   "source": [
    "## Getting a grip on the Keras functional API\n",
    "\n",
    "We want to remake our fully-connected neural network that we made last week in Keras as a first step. So, let's make sure you understand the Keras functional API. To do so, follow along with [this link](https://keras.io/guides/functional_api/) up to and until Training, evaluation, and inference.\n",
    "\n",
    "**NOTE: TO BE ABLE TO SHOW THE DIAGRAM, YOU NEED TO RUN conda install pydot IN CONDA (IN YOUR COURSE ENVIRONMENT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code from following along here. Or if you want it spread over cells, add new ones with esc+b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7201868",
   "metadata": {},
   "source": [
    "## Remaking our model from last week\n",
    "\n",
    "You now basically have all the ingredients to remake your model. In fact, you already made a larger model above. Now, try to do the same thing for the neural network architecture that we used last week, which looked like this:\n",
    "![NeuralNetworkImage](NeuralNetwork.PNG)\n",
    "We will use one Hidden Layer with 25 units, and one output layer with 10 units. Use the sigmoid activation that we've used before. See [here](https://keras.io/api/layers/activations/) if you are lost on how to use the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58852652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8a0dd",
   "metadata": {},
   "source": [
    "## What you should see\n",
    "\n",
    "For one layer and about 80 neurons less you lose 5 percentage points of accuracy. Our simple network is still not too bad. \n",
    "\n",
    "## Using the softmax function and ReLUs\n",
    "\n",
    "Until now, we've been allowing the outputs of our network to sum up to more than one. That's fine for predictions, since we just pick the node with the highest activation and take that as our predicted class. Still, it might be nice, at least for interpretability, if we compress the sum of all these outputs to 1. Then we can interpret each output as a probability of being that class.\n",
    "\n",
    "What's more, sigmoid neurons have some problems, and Rectified Linear Units (linear, except for negative values which become 0) have been shown to work very well in deep neural networks. Note that they are nonlinear for data < 0, and otherwise linear. That's why they still count as nonlinear activation functions. See [this](https://stats.stackexchange.com/a/510818) or [this](https://www.youtube.com/watch?v=68BZ5f7P94E) for more info (**optional**).\n",
    "\n",
    "Over to you: use the same network you just trained, only switch to ReLUs and use a softmax output layer! Also, train for 4 instead of 2 epochs (4 full passes over the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0780b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f8178",
   "metadata": {},
   "source": [
    "## What you see\n",
    "\n",
    "If all goes well, you should see you get a slightly better outcome in this way. This is only due to the ReLUs and/or a different initialisation. I ran it 3 times, and got a worse performance once and a better performance twice. Remember, neural networks get random initialisations to break the symmetry that otherwise occurs in the errors of neurons (causing everything to update the exact same way).\n",
    "\n",
    "## Training a convolutional neural network\n",
    "Using Keras we are not beholden to simple dense networks. We can quite easily train a convolutional neural network. Let's do that now and see its performance. Go [here](https://keras.io/examples/vision/mnist_convnet/) and follow the instructions. **Make sure you understand what each layer does, so quickly look up Dropout and Flatten to see what they might do!**. Note that this tutorial doesn't use the functional API but the `Sequential()` syntax. It leads to the same thing, and it is good for you to see that these two different ways of defining Keras models exist!\n",
    "\n",
    "**Beware: training this network for 15 epochs takes quite some time. Reduce it to 8 epochs instead of the 15 in the example for speed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fa1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a40c3",
   "metadata": {},
   "source": [
    "## What you should see\n",
    "\n",
    "You should see that there is indeed an accuracy of ~99% on the test set. For fewer epochs it might be more ~98%, but still. _Pretty nifty_ for a few lines of code. \n",
    "\n",
    "Do note that, if you were to run this code on real handwritten digits, you still need to solve the problem of segmenting out the digits in an image. Also note that in this dataset all the ones look like | and not so much like 1. So a 1 might register as a 7. The point is that this classifier works fine for the narrow task you gave it, so it will perform nigh-on perfectly on classifying MNIST digits, but add a little noise or a small difference, and there's still a problem. It generalises well to the _specific_ problem of MNIST data, but certainly not _as well_ to the actual problem of digit recognition. It is still just a toy classifier. See [here](https://matteo-a-barbieri.medium.com/why-mnist-is-the-worst-thing-that-has-ever-happened-to-humanity-49fd053f0f66) for more info.\n",
    "\n",
    "## Finally: integrating neural networks with scikit-learn for (nested) cross-validation\n",
    "\n",
    "What we _want_ to do is to be able to take a neural network and use the tools you have learned for cross-validation to improve it. Ideally, you'd use a scikit-learn Pipeline with a StandardScaler, and then perhaps a dimension reduction step if necessary, and then a neural network, all with tunable hyperparameters, which could all be tuned at once with the pipeline and GridSearchCV or RandomSearchCV. \n",
    "\n",
    "Luckily, this is possible. We just need to do two things:\n",
    "1. Make a _function_ that constructs a Keras neural network model, that takes arguments which are parameters we might want to change. Say, the learning rate, size of Hidden Layer 1, and size of Hidden Layer 2. \n",
    "2. Instantiate a scikit-learn model object built from our neural network, using `KerasClassifier`\n",
    "\n",
    "This is not too difficult. An example of building such a function and instantiating a model is shown below. Make sure you understand the steps: making a model-building function in Keras, wrapping that function in a call that lets it behave as a scikit-learn classifier, and then hyperparameter tuning as normal (here without nested cross-val for speed).\n",
    "\n",
    "**Note: RandomisedSearchCV depends on random numbers, so your outcome will be slightly different.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ec8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV #rather than trying all combinations of hyperparams, sample some options from a range\n",
    "\n",
    "randomNumberMaker = np.random.default_rng(seed=42)\n",
    "\n",
    "#I am just reloading the train data so it's all unaltered.\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "#subsample\n",
    "indicesTrain = randomNumberMaker.integers(0, len(x_train), 3000)\n",
    "indicesTest = randomNumberMaker.integers(0, len(x_test), 200)\n",
    "x_train = x_train[indicesTrain]\n",
    "y_train = y_train[indicesTrain]\n",
    "x_test  = x_test[indicesTest]\n",
    "y_test  = y_test[indicesTest]\n",
    "print(x_train.shape[0], \"train samples after downsampling for faster runtime\")\n",
    "print(x_test.shape[0], \"test samples after downsampling for faster runtime\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#function to build our model\n",
    "def build_neural_net(hiddenLayerOne=784, hiddenLayerTwo=256, learnRate=0.01):\n",
    "    # initialize a sequential model and add layer to flatten the input data\n",
    "    model = Sequential()\n",
    "    #Flatten turns 28*28 image into 1 by 784 (just 784 pixel values)\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(hiddenLayerOne, activation=\"relu\",\n",
    "        input_shape=(784,)))\n",
    "    #note that the dropout rate here could also be a hyperparameter!\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(hiddenLayerTwo, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # add a softmax layer on top\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learnRate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    # return compiled model\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=build_neural_net, verbose=0)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#let's define some parameters to search over\n",
    "hiddenLayerOne = [5, 10, 50]\n",
    "hiddenLayerTwo = [5, 10, 50]\n",
    "learnRate = [1e-2, 1e-3]\n",
    "batchSize = [4, 8, 16]\n",
    "epochs = [4, 8, 16, 32]\n",
    "\n",
    "# create a dictionary, this is what sklearn wants\n",
    "\n",
    "# note that batch_size and epochs are not arguments of\n",
    "# our model construction function, but they are\n",
    "# normally arguments of model.fit() in Keras,\n",
    "# see what you did at the top of the notebook.\n",
    "\n",
    "# The names on the left here NEED to match the argument names\n",
    "# of your build_neural_net function.\n",
    "grid = dict(\n",
    "    hiddenLayerOne=hiddenLayerOne,\n",
    "    hiddenLayerTwo=hiddenLayerTwo,\n",
    "    learnRate=learnRate,\n",
    "    batch_size=batchSize,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "#-1 = use all CPUs, cv--> 3-fold cross-val.\n",
    "search = RandomizedSearchCV(estimator=model, n_jobs=-1, cv=3,\n",
    "    param_distributions=grid, scoring=\"accuracy\", n_iter = 5)\n",
    "searchResults = search.fit(x_train, y_train)\n",
    "bestScore = searchResults.best_score_\n",
    "bestParams = searchResults.best_params_\n",
    "print(\"Best score is {:.2f} using {}\".format(bestScore,\n",
    "    bestParams))\n",
    "\n",
    "#finally, use best model on train data on test data (normally would use nested cross-validation)\n",
    "\n",
    "bestModel = searchResults.best_estimator_\n",
    "accuracy = bestModel.score(x_test, y_test)\n",
    "print(\"accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7531a12",
   "metadata": {},
   "source": [
    "## What I want you to remember here\n",
    "\n",
    "There is of course a lot more nuance and power in Keras, this is just the tip of the iceberg. But the practical skills you need to take from here:\n",
    "* You know how to build and train a neural network model using Keras (both Dense and Convolutional models).\n",
    "* You know how to wrap a Keras model in a constructor function, and instantiate it as a sklearn model, so that you can use it with sklearn for hyperparameter optimalisation\n",
    "\n",
    "## The end\n",
    "\n",
    "With that, the course content is done. All that's left is to apply ML to an example dataset using your understanding of the fundamentals of what's _really_ going on, and the high-level high-powered libraries that you can now command. \n",
    "\n",
    "## Survey\n",
    "[Click me, click me. Oh, oh, oh click me!](https://docs.google.com/forms/d/e/1FAIpQLScURtN2iRNJrLGHqYLxOOZ8Fz8xjoqsIcC_lGzfwuukRvZ0Ew/viewform?usp=sf_link) - after Donkey (on the Shrek DVD main menu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
