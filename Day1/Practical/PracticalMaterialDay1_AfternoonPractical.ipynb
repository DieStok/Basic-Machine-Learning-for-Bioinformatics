{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Afternoon practical day 1** <br>\n",
    "Welcome to the afternoon practical. Here, you're going to write a function to use cross-validation rather than training on all data. After that, you'll implement multivariate linear regression with linear algebra using numpy. Finally, you'll run it on a small dataset of SNPs, thereby performing an elementary GWAS analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import random as rand\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary functions you defined before:\n",
    "def univariateHypothesis(x, thetas):\n",
    "    predict = thetas[0] + thetas[1] * x\n",
    "    return predict\n",
    "\n",
    "def multiHypothesis(x, thetas):\n",
    "    #add a 0 to x as the first 'feature'\n",
    "    x = pd.Series(1).append(x)\n",
    "    if not len(x) == len(thetas):\n",
    "        print(\"Error, x and theta should have equal length!\")\n",
    "        return\n",
    "    prediction = sum([x.iloc[index] * thetas[index] for index, _ in enumerate(x)])\n",
    "    return prediction\n",
    "\n",
    "def MyMSE(dataframe, thetas, hypothesis = \"multiHypothesis\"):\n",
    "    totalSumSquares = 0\n",
    "    for index, rowData in dataframe.iterrows():\n",
    "        prediction = globals()[hypothesis](rowData.drop(\"y\"), thetas)\n",
    "        squareError = (prediction-rowData[\"y\"])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(dataframe) \n",
    "    return meanSquaredError\n",
    "\n",
    "def gradientDescent(dataframe, thetas, alpha, hypothesis = \"multiHypothesis\"):\n",
    "    m = len(dataframe)\n",
    "    #print(\"m: \"); print(m)\n",
    "    totalErrorThetaZero = 0\n",
    "    totalErrorOtherThetas = [0] * (len(thetas)-1)\n",
    "    for index, row in dataframe.iterrows():\n",
    "        #needed for all thetas, calculate the prediction only once.\n",
    "        hypothesisOutcome = globals()[hypothesis](row.drop(\"y\"), thetas)\n",
    "        #calculate partial derivative theta zero.\n",
    "        errorThetaZero = hypothesisOutcome - row[\"y\"]\n",
    "        totalErrorThetaZero += errorThetaZero\n",
    "        #theta zero partial derivative calculated. Now loop over all remaining thetas:\n",
    "        for otherThetasIndex in range(1,len(thetas)):\n",
    "            errorThisTheta  = (hypothesisOutcome - row[\"y\"]) * row[otherThetasIndex]\n",
    "            totalErrorOtherThetas[otherThetasIndex-1] += errorThisTheta\n",
    "    \n",
    "    \n",
    "    #now take a step for every theta\n",
    "    finalThetas = []\n",
    "    partialDerivativesAllThetas = totalErrorOtherThetas\n",
    "    partialDerivativesAllThetas.insert(0, totalErrorThetaZero)\n",
    "    #loop over all thetas, subtracting alpha/m * its partial derivative from what was put into the function.\n",
    "    for index, theta in enumerate(partialDerivativesAllThetas):\n",
    "        finalThetas.append(thetas[index] - alpha/m * partialDerivativesAllThetas[index])\n",
    "    \n",
    "    return np.array(finalThetas)\n",
    "\n",
    "def makePolynomialFeatures(x = pd.DataFrame(None), power = 2):\n",
    "    \n",
    "    columns = []\n",
    "    nameList = []\n",
    "    for i in range(1, power+1):\n",
    "        columns.append(x**i)\n",
    "    finalFeaturesDataFrame = pd.concat(columns, axis = 1)\n",
    "    columnNames = [name + \"toPower\" + str(power) for name, power in zip(finalFeaturesDataFrame.columns, list(range(1,power+1)))]\n",
    "    finalFeaturesDataFrame.columns = columnNames\n",
    "    return finalFeaturesDataFrame\n",
    "\n",
    "def createNormalisedFeatures(dataFrame, mode = \"range\"):\n",
    "    featureMeans = dataFrame.mean()\n",
    "    if mode == \"range\":\n",
    "        featureRanges = dataFrame.max() - dataFrame.min()\n",
    "        normalisedFeatures = (dataFrame - featureMeans)/featureRanges\n",
    "        return [normalisedFeatures, featureMeans, featureRanges]\n",
    "    elif mode == \"SD\":\n",
    "        featureSDs = dataFrame.std()\n",
    "        normalisedFeatures = (dataFrame - featureMeans)/featureSDs\n",
    "        return [normalisedFeatures, featureMeans, featureSDs]\n",
    "\n",
    "\n",
    "#sample data for use\n",
    "data = pd.read_csv(\"sampleDataLinearRegression.csv\", header= None)\n",
    "data.columns = [\"x\", \"y\"]\n",
    "\n",
    "featuresHP = makePolynomialFeatures(data[\"x\"], power = 2)\n",
    "\n",
    "normalisedFeaturesHP, featureMeansHP, featureRangesHP = createNormalisedFeatures(featuresHP, mode = \"range\")\n",
    "normalisedFeaturesHP[\"y\"] = data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a function that splits your training data into k folds for cross-validation\n",
    "\n",
    "Before, we left off at a point where we could run multivariate  linear regression, in this case on polynomial features. However, we were still fitting to all the data, and are thereby not guarding ourselves against overfitting. What we need is cross-validation: splitting the dataset in k folds, training on k-1 parts of the dataset, then testing on the k-th part, for as many times as you have k. This is called k-fold cross-validation. A number of 10 is often used (10-fold cross-validation), although that's not because it is universally optimal. See here: https://stats.stackexchange.com/a/357749 and here: https://stats.stackexchange.com/a/264721 for more info. It comes down to the bias-variance trade-off that you have heard about. <br> For now, let's set that discussion aside, and focus on making a function `makeCrossValData` that can take our data and create k folds from it for cross-validation.\n",
    "\n",
    "So, what we want it to use as input and give as output: \n",
    "* It takes in a dataFrame with samples on the rows and features in the columns, and a parameter k\n",
    "* It should return two lists of length k. One list (trainSet) contains the row indices for the training data for a certain fold, the other contains the row indices of the test data for a certain fold. This means that these lists are lists of lists: `trainSet[0]` should return something like `[3, 9, 14, 15, 20, 34]`, which are all the indices of the training samples to train on for the first cross-validation fold. It should also return the shuffled dataFrame. <br> <br>\n",
    "\n",
    "What we want it to do:\n",
    "* It first shuffles the data (use `DataFrame.sample(frac=1)` for this).\n",
    "* It then makes k splits of the data. Note: you probably can't split the data exactly equally. Hence, use something like `int(np.floor(m/k))` , where m = # of samples, to determine the size of the equal splits you can make. And/Or use the modulo operator (`%`) to see the remainder after division by k (i.e. `9 % 4` = 1, because 2\\*4 = 8, and then 1 is left).\n",
    "* Make sure to identify the samples that remain because they can't be divided equally among the folds, and assign them to random folds.\n",
    "\n",
    "Hints:\n",
    "* You can make a function return three objects by simply separating them with comma's: `return dataFrame, trainSet, testSet`. Then, when using the function, you should do `shuffledDataFrame, trainSetOutput, testSetOutput = makeCrossValData()`.\n",
    "* This is somewhat difficult, requiring a bit of looping and checking to make sure that you first make proper-sized folds, then adding the remaining samples to random folds, and then outputting a correct list. If you have difficulties, first use DuckDuckGo (or Google), ask a question, or refer to the answers.\n",
    "* If you need it: `[item for sublist in listWithSublists for item in sublist]` will take a list that looks like this: `[[25, 12, 3], [18, 33, 21], [1, 13, 5]]` and turn it into `[25, 12, 3, 18, 33, 21, 1, 13, 5]`. This is called a [list comprehension](https://www.datacamp.com/community/tutorials/python-list-comprehension) and is very powerful (but can also be somewhat unclear, and remember that one of the main purposes of code should be to be intelligible to humans that have to read and/or maintain it!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the cross-validation function\n",
    "Now, let's use your cross-validation function to train a multivariate linear regression on the polynomial training data we have been using all along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run your function\n",
    "shuffledDataFrame, trainSetIndices, testSetIndices = makeCrossValData(normalisedFeaturesHP, 9)\n",
    "\n",
    "print(\"List of training set indices to use: \\n\")\n",
    "print(trainSetIndices)\n",
    "print(\"--------\")\n",
    "print(\"List of test set indices to use: \\n\")\n",
    "print(testSetIndices)\n",
    "\n",
    "#make a figure to show this\n",
    "stepsGradDescent = 40\n",
    "#note: this alpha actually results in rising MSE values after a number of gradient descent steps. In a real scenario,\n",
    "#I would recommend an alpha of 0.01 to start\n",
    "alpha = 0.2 \n",
    "startThetas = [0,0,0]\n",
    "\n",
    "#lists to store information for all cross-validations:\n",
    "thetaValuesDuringDescent = []\n",
    "MSEDuringDescent = []\n",
    "testMSE = []\n",
    "\n",
    "figCrossVal, axsCrossVal = plt.subplots(nrows = 3, ncols = 3, figsize = (10,10))\n",
    "\n",
    "\n",
    "for i in range (0,9):\n",
    "    trainSubset = shuffledDataFrame.iloc[trainSetIndices[i],:]\n",
    "    testSubset  = shuffledDataFrame.iloc[testSetIndices[i],:]\n",
    "    thetaValuesDuringDescent.append([])\n",
    "    MSEDuringDescent.append([])\n",
    "    thetasNow = startThetas.copy()\n",
    "    \n",
    "    for steps in range(0,stepsGradDescent):\n",
    "        oneStep = gradientDescent(trainSubset, thetasNow, alpha)\n",
    "        thetaValuesDuringDescent[-1].append(oneStep)\n",
    "        thetasNow = oneStep\n",
    "        MSEDuringDescent[-1].append(MyMSE(trainSubset, thetasNow, hypothesis = 'multiHypothesis'))\n",
    "    \n",
    "    #with final thetas, test on the validation set and check MSE\n",
    "    testMSE.append(MyMSE(testSubset, thetasNow, hypothesis = 'multiHypothesis'))\n",
    "    \n",
    "\n",
    "#now plot this data\n",
    "counterPlotting = 0\n",
    "for plotY in range(0,3):\n",
    "    for plotX in range(0,3):\n",
    "\n",
    "        #print(counterPlotting)\n",
    "        axsCrossVal[plotX, plotY].scatter(shuffledDataFrame.iloc[trainSetIndices[counterPlotting],0],\n",
    "                           shuffledDataFrame.iloc[trainSetIndices[counterPlotting],-1],\n",
    "                           color = \"blue\", label = \"training data\")\n",
    "        axsCrossVal[plotX, plotY].scatter(shuffledDataFrame.iloc[testSetIndices[counterPlotting],0],\n",
    "                           shuffledDataFrame.iloc[testSetIndices[counterPlotting],-1],\n",
    "                           color = \"red\", label = \"test data\")\n",
    "        \n",
    "        #plot predicted test data\n",
    "        axsCrossVal[plotX, plotY].scatter(shuffledDataFrame.iloc[testSetIndices[counterPlotting],0],\n",
    "                           [multiHypothesis(row, thetaValuesDuringDescent[counterPlotting][-1]) for index, row in shuffledDataFrame.iloc[testSetIndices[counterPlotting],:].drop(\"y\", axis = 1).iterrows()],\n",
    "                           color = \"green\", label = \"test data predictions\")\n",
    "        \n",
    "        #plot all data as predicted by these thetas\n",
    "        xValuesToPlotHere      = shuffledDataFrame.iloc[trainSetIndices[counterPlotting],0]\n",
    "        fittedValuesToPlotHere = [multiHypothesis(row, thetaValuesDuringDescent[counterPlotting][-1]) for index, row in shuffledDataFrame.iloc[trainSetIndices[counterPlotting],:].drop(\"y\", axis = 1).iterrows()]\n",
    "        zippedTogether         = list(zip(xValuesToPlotHere, fittedValuesToPlotHere))\n",
    "        sortedZipped           = sorted(zippedTogether, key = lambda x : x[0])\n",
    "        unzippedXAndFitted     = list(zip(*sortedZipped))\n",
    "        xValuesToPlotHere      = unzippedXAndFitted[0]\n",
    "        fittedValuesToPlotHere = unzippedXAndFitted[1]\n",
    "        axsCrossVal[plotX, plotY].plot(xValuesToPlotHere, fittedValuesToPlotHere,\n",
    "                    color = \"black\", label = \"fitted line\", linestyle = 'dashed')\n",
    "        \n",
    "        axsCrossVal[plotX, plotY].legend(fontsize = \"small\")\n",
    "        \n",
    "        #add theta values for this cross-validation\n",
    "        stringToAdd = 'thetas: ' + str(np.round(thetaValuesDuringDescent[counterPlotting][-1], 2))\n",
    "        axsCrossVal[plotX, plotY].text(0.95, 0.01, stringToAdd ,\n",
    "        verticalalignment='bottom', horizontalalignment='right',\n",
    "        transform=axsCrossVal[plotX, plotY].transAxes,\n",
    "        color='black', fontsize=7)\n",
    "        \n",
    "        #add mean-squared error on the test set\n",
    "        stringToAdd = 'MSE on test data: ' + str(np.round(testMSE[counterPlotting], 2))\n",
    "        axsCrossVal[plotX, plotY].text(0.95, 0.10, stringToAdd ,\n",
    "        verticalalignment='bottom', horizontalalignment='right',\n",
    "        transform=axsCrossVal[plotX, plotY].transAxes,\n",
    "        color='black', fontsize=7)\n",
    "        counterPlotting += 1\n",
    "        \n",
    "        \n",
    "print(\"Average test set error: \" + str(np.round(np.mean(testMSE), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validated polynomial fitting outcome\n",
    "\n",
    "Unsurprisingly, you'll see that the prediction performance is abysmal, and also varies a lot per cross-validation. This is because the polynomial function actually doesn't fit the data as it is linear, and there is extremely little data. Hence, it matters a lot which data points are in the test set and which aren't. <br>\n",
    "Still, it's a good illustration of the power of cross-validation: by seperating our data and training on subsets, we can get a better idea of the generalisation performance, i.e. how well our model will perform on real-world data. If we'd have had a good model rather than a toy one, we could now decide to train the final model on **all** training data, and then publish it and put it to use. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra exercises\n",
    "\n",
    "Until now, you've been working with for loops and lists, looping over the theta values and corresponding features, multiplying them, then summing these and thereby obtaining the final predictions. For gradient descent, too, we have been working with iteration. As you've been told in the lectures, the language of machine learning is linear algebra. We're now going to focus on implementing what you've done so far in linear algebra. Before we move on, if you're still a little bit unsure about some of what we've done until now, please watch some of the videos given in the course reader, for example on linear regression, the bias-variance trade-off, or cross-validation.\n",
    "\n",
    "Let's get a bit of a feel for working with matrices. To do that, we'll do some pen-and-paper exercises:\n",
    "* Go [here](https://www.algebrapracticeproblems.com/matrix-multiplication-product-multiplying-matrices/), read the text and do the first 3 practice problems. You can check your solutions on the site. Be sure to read through the procedures and properties of matrix multiplication so you're sure about your understanding.\n",
    "* After that's done, do the 3 problems below: <br> <br> <br>\n",
    "\n",
    "1. $$\\begin{bmatrix} 6 & -2 & 5 \\\\ 1 & 6 & 2 \\\\ -3 & 4 & 7 \\end{bmatrix} \\cdot \\begin{bmatrix} -4 \\\\ 8 \\\\ 3 \\end{bmatrix}$$ <br>\n",
    "2. $$\\begin{bmatrix} 2 & -6 & 11 \\end{bmatrix} \\cdot \\begin{bmatrix} -16 \\\\ -5 \\\\ 2 \\end{bmatrix}$$ <br>\n",
    "3. $$\\begin{bmatrix} 2 & -6 & 11 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ -6 \\\\ 11 \\end{bmatrix}$$\n",
    "\n",
    "<br> <br> <br> <br> \n",
    "\n",
    "\n",
    "Note that problem 3 is the same as summing the element-wise squares of the vector entries. In other words: squaring vector elements and summing them is equal to $V^t*V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking your answers with numpy\n",
    "\n",
    "Instead of supplying answers for the above, how's about you check it yourself using numpy? Use the following commands:\n",
    "\n",
    "* `np.array([[6, 12, 3], [4, 5, 6]])` will result in $\\begin{bmatrix} 6 & 12 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n",
    "* `A = np.array([[6, 12, 3], [4, 5, 6]])` <br>`B = np.array([[3, 4], [-2, 8], [8, 9]])` <br> `A @ B` <br> \n",
    "will result in $\\begin{bmatrix} 6 & 12 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix} 3 & 4 \\\\ -2 & 8 \\\\ 8 & 9 \\end{bmatrix} = \\begin{bmatrix} 18 & 147 \\\\ 50 & 110 \\end{bmatrix}$ <br>\n",
    "* `A.transpose()` or `A.T` will yield $\\begin{bmatrix} 6 & 4 \\\\ 12 & 5 \\\\ 3 & 6 \\end{bmatrix}$ (useful for the 3rd problem so you don't repeat yourself)\n",
    "\n",
    "Check your answers for the three final questions above using these commands in the code cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answers below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewriting the univariate and multivariate regression function with linear algebra\n",
    "\n",
    "In the code cell below, you've been given working examples of the regression hypothesis functions you've been working with. Your job is to make just one function `linAlgRegHypothesis(data, thetas)` that can perform linear regression for any number of features, using linear algebra . To do this, remember the following:\n",
    "* You can just prepend 1 to a feature vector to make the multiplication of a theta-vector with those features incredibly straightforward.\n",
    "* You don't need to loop: you can just calculate all predicted values at once using matrix-vector multiplication.\n",
    "* Use `np.array(normalisedFeaturesHP.loc[:, [\"xtoPower1\", \"xtoPower2\"]])` for testing. You'll probably have to use the transpose function.\n",
    "* To add a column of ones to a feature matrix, use `np.c_[arrayOfOnes, featureArrayOrMatrix]` They should of course have the same length (number of rows).\n",
    "\n",
    "If you have trouble grappling with numpy, read [this](https://numpy.org/doc/stable/user/absolute_beginners.html) and use [the numpy documentation](https://numpy.org/doc/stable/reference/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariateHypothesis(x, thetas):\n",
    "    predict = thetas[0] + thetas[1] * x\n",
    "    return predict\n",
    "\n",
    "def multiHypothesis(x, thetas):\n",
    "    #add a 1 to x as the first 'feature'\n",
    "    x = pd.Series(1).append(x)\n",
    "    if not len(x) == len(thetas):\n",
    "        print(\"Error, x and theta should have equal length!\")\n",
    "        return\n",
    "    prediction = sum([x.iloc[index] * thetas[index] for index, _ in enumerate(x)])\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two functions for a given theta vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas        = np.array([12, 3, 5])\n",
    "data          = np.array(normalisedFeaturesHP.loc[:, [\"xtoPower1\", \"xtoPower2\"]])\n",
    "dataFrame     = normalisedFeaturesHP.loc[:, [\"xtoPower1\", \"xtoPower2\"]]\n",
    "oldFuncOutput = np.array([multiHypothesis(row, thetas) for index, row in dataFrame.iterrows()])\n",
    "newFuncOutput = linAlgRegHypothesis(data, thetas)\n",
    "\n",
    "print(\"Old output: \\n\")\n",
    "print(str(oldFuncOutput) + \"\\n\")\n",
    "print(\"New output: \\n\")\n",
    "print(str(newFuncOutput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a mean-squared error function and gradient descent with linear algebra\n",
    "\n",
    "Now that you've performed this feat for the hypothesis function, let's do the same for the myMSE function and gradient descent. Start with the Mean-Squared error function. There's one additional change: until now we've kept our true values in the data as a column. You might have noticed that we've been working around this. Let's change that, and make sure the true values are vector `y` with n entries, and the data is a matrix with all the features in it.\n",
    "\n",
    "So:\n",
    "\n",
    "* rewrite the `myMSE` function (call it `linAlgMSE`) to use linear algebra to immediately calculate the MSE for all samples given the current thetas (rather than looping over them). Give it an extra argument `y` which accepts the true values. Also make it use the new `linAlgRegHypothesis` function you defined above!\n",
    "* Note: \n",
    "* Test that it works using the data provided in the cell below. \n",
    "\n",
    "Hints:\n",
    "\n",
    "* You don't need the call to globals anymore: the `linAlgRegHypothesis` function works for any regression (univariate or multivariate) so you can just use that without giving it as an argument.\n",
    "* Make sure that the shape of the predictions made with `linAlgRegHypothesis` and of `y` are the same when subtracting. i.e. make them both row-vectors or column-vectors. To make the predictions a column vector, use `np.reshape(predictions, (len(predictions), 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data to use for testing\n",
    "dataMatrix = data\n",
    "trueValues = np.array(normalisedFeaturesHP.loc[:, [\"y\"]])\n",
    "thetas = np.array([50, -48, 155])\n",
    "print(dataMatrix[0:3, :])\n",
    "print(trueValues[0:3])\n",
    "\n",
    "#old function\n",
    "def MyMSE(dataframe, thetas, hypothesis = \"multiHypothesis\"):\n",
    "    totalSumSquares = 0\n",
    "    for index, rowData in dataframe.iterrows():\n",
    "        prediction = globals()[hypothesis](rowData.drop(\"y\"), thetas)\n",
    "        squareError = (prediction-rowData[\"y\"])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(dataframe) \n",
    "    return meanSquaredError\n",
    "\n",
    "#your answer here\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with old myMSE\n",
    "Let's compare your outcomes with the old way of doing things. Run the cell below to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldMSEResults = MyMSE(normalisedFeaturesHP, thetas)\n",
    "newMSEResults = linAlgMSE(dataMatrix, trueValues, thetas)\n",
    "print(\"Old function result: \")\n",
    "print(str(oldMSEResults) + \"\\n\")\n",
    "print(\"New function result: \")\n",
    "print(newMSEResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to gradient descent\n",
    "\n",
    "Now do the same thing for gradient descent. Call the function `linAlgGradientDescent`. You can use [this link](https://medium.com/analytics-vidhya/vectorized-implementation-of-gradient-descent-in-linear-regression-12a10ea37210) if you want to get a bit more insight. The below cell sets some things up and reiterates the old function. Write your now function in the cell below that (so 2 cells down). The basic things to realise are:\n",
    "\n",
    "* If you add 1 as the first feature (as in calculation of the predictions), then the partial derivative for the intercept will automatically just be multiplied by 1.\n",
    "* You can calculate the predictions with the new `linAlgRegHypothesis` you made.\n",
    "* Watch out that your arrays are the same shape and in the correct column form. To get a column array from a normal (1D) numpy array, use `oneDArray[:, np.newaxis]`. To check array shapes, use `.shape`. If used on a 1D array like `np.array([15, 20, 30]).shape`, it will return `(3,)`. If used on a 2D column vector array like `np.array([[1],[5],[18]]).shape` it will return `(3,1)`. See the example below. You can also use the `.ndim` attribute. If it is 1, it is a 1D array, if 2 a 2D array, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(dataframe, thetas, alpha, hypothesis = \"multiHypothesis\"):\n",
    "    m = len(dataframe)\n",
    "    #print(\"m: \"); print(m)\n",
    "    totalErrorThetaZero = 0\n",
    "    totalErrorOtherThetas = [0] * (len(thetas)-1)\n",
    "    for index, row in dataframe.iterrows():\n",
    "        #needed for all thetas, calculate the prediction only once.\n",
    "        hypothesisOutcome = globals()[hypothesis](row.drop(\"y\"), thetas)\n",
    "        #calculate partial derivative theta zero.\n",
    "        errorThetaZero = hypothesisOutcome - row[\"y\"]\n",
    "        totalErrorThetaZero += errorThetaZero\n",
    "        #theta zero partial derivative calculated. Now loop over all remaining thetas:\n",
    "        for otherThetasIndex in range(1,len(thetas)):\n",
    "            errorThisTheta  = (hypothesisOutcome - row[\"y\"]) * row[otherThetasIndex]\n",
    "            totalErrorOtherThetas[otherThetasIndex-1] += errorThisTheta\n",
    "            \n",
    "    #now take a step for every theta\n",
    "    finalThetas = []\n",
    "    partialDerivativesAllThetas = totalErrorOtherThetas\n",
    "    partialDerivativesAllThetas.insert(0, totalErrorThetaZero)\n",
    "    #loop over all thetas, subtracting alpha/m * its partial derivative from what was put into the function.\n",
    "    for index, theta in enumerate(partialDerivativesAllThetas):\n",
    "        finalThetas.append(thetas[index] - alpha/m * partialDerivativesAllThetas[index])\n",
    "    \n",
    "    return np.array(finalThetas)\n",
    "\n",
    "#numpy example shape\n",
    "aOneDArray = np.array([15, 20, 30])\n",
    "print(\"1D Array: \\n\")\n",
    "print(aOneDArray)\n",
    "print(\"Its shape attribute: \\n\")\n",
    "print(aOneDArray.shape)\n",
    "print(\"---------\")\n",
    "print(\"2D Array (column vector) \\n\")\n",
    "aTwoDArray = np.array([[1],[5],[18]])\n",
    "print(aTwoDArray)\n",
    "print(\"Its shape attribute: \\n\")\n",
    "print(aTwoDArray.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "#subtracting these leads to a matrix where the first row contains [15,20,30] - 1, the second one [15,20,30] - 5, etc.:\n",
    "print(\"Subtracting 2D array from 1D array results in a matrix: \\n\")\n",
    "print(aOneDArray - aTwoDArray)\n",
    "print(\"\\n\")\n",
    "#make a 1D array into a 2D array with 1 column and len(1DArray) rows:\n",
    "print(\"You can transform the 1D array into a 2D one using reshape: \\n\")\n",
    "columnVectorOneDArray = np.reshape(aOneDArray, (len(aOneDArray), 1))\n",
    "print(columnVectorOneDArray)\n",
    "print(columnVectorOneDArray.shape)\n",
    "\n",
    "print(\"You can also transform a 1D array into a 2D one using np.newaxis: \\n\")\n",
    "print(aOneDArray[:, np.newaxis])\n",
    "\n",
    "#now you can subtract the two column vectors\n",
    "print(\"Now you get a simple subtraction to get a new vector: \\n\")\n",
    "print(columnVectorOneDArray - aTwoDArray)\n",
    "print(\"Number of dimensions: \" + str((columnVectorOneDArray - aTwoDArray).ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make your new function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your new gradient descent function\n",
    "Now let's test your new function. Let's use just the x1 feature (i.e. search for optimal parameters for univariate linear regression on the test data set). Note the use of `np.newaxis` to make sure that selecting just one column of the feature array doesn't result in a 1D array. Just run the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleFeatData = data[:,0, np.newaxis]\n",
    "trueValues     = np.array(normalisedFeaturesHP.loc[:, [\"y\"]])\n",
    "trueValues     = (trueValues - np.mean(trueValues))/np.std(trueValues)\n",
    "stepsToTake    = 11\n",
    "alpha          = 0.9\n",
    "startThetas    = np.array([[1],[0]])\n",
    "\n",
    "\n",
    "#Performing gradient descent\n",
    "thetasDuringDescent       = [startThetas.copy()]\n",
    "JsGradientDescent         = [linAlgMSE(singleFeatData, trueValues, np.ravel(thetasDuringDescent[-1].T))]\n",
    "for step in range(0, stepsToTake):\n",
    "    currentThetas = thetasDuringDescent[-1]\n",
    "    newThetas     = linAlgGradientDescent(singleFeatData, trueValues, currentThetas, alpha)\n",
    "    newJ          = linAlgMSE(singleFeatData, trueValues, newThetas)\n",
    "    \n",
    "    thetasDuringDescent.append(newThetas)\n",
    "    JsGradientDescent.append(newJ)\n",
    "\n",
    "\n",
    "##Plotting shenanigans##\n",
    "\n",
    "#set up 2 plots\n",
    "figGradDescent, axGradDescent = plt.subplots(nrows = 1, ncols = 2, figsize=(9,5.5))\n",
    "#figGradDescent.tight_layout()\n",
    "#colors for plotting gradient descent steps\n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "\n",
    "#scatterplot without normalised x values\n",
    "axGradDescent[0].scatter(singleFeatData, trueValues, marker='x', s=40, color='k')\n",
    "axGradDescent[0].set_ylabel(\"y\")\n",
    "axGradDescent[0].set_xlabel(\"x\")\n",
    "\n",
    "#calculate cost values for many different theta0, theta1 combinations\n",
    "theta0Vals =  np.linspace(-5, 5, 200)\n",
    "theta1Vals =  np.linspace(-5, 5, 200)\n",
    "X, Y = np.meshgrid(theta0Vals, theta1Vals)\n",
    "JVals = np.zeros(np.shape(X))\n",
    "for i in range(0,len(theta0Vals)):\n",
    "    for j in range(0,len(theta1Vals)):\n",
    "        JVals[i,j] = linAlgMSE(singleFeatData, trueValues, np.array([X[i,j], Y[i,j]]))\n",
    "\n",
    "#we can reuse the contour plot values we calculated before\n",
    "contours = axGradDescent[1].contour(X, Y, np.round(JVals,1),\n",
    "                         levels = 10)\n",
    "axGradDescent[1].clabel(contours)\n",
    "axGradDescent[1].set_xlabel(r\"$\\theta_0$\")\n",
    "axGradDescent[1].set_ylabel(r\"$\\theta_1$\")\n",
    "minimum = axGradDescent[1].scatter(X[JVals == np.min(JVals)], Y[JVals == np.min(JVals)],\n",
    "                      color = \"blue\", label = \"minimum\", marker = \"x\",\n",
    "                      s = 30)\n",
    "\n",
    "#make the plot of gradient descent steps:\n",
    "\n",
    "#if stepsToTake > colors, repeat the colors\n",
    "colors = colors * int(np.ceil(stepsToTake/len(colors)))\n",
    "\n",
    "#plot initial regression line\n",
    "axGradDescent[0].plot(singleFeatData, linAlgRegHypothesis(singleFeatData, startThetas),\n",
    "           color=colors[0], lw=2, linestyle = \"dashed\",\n",
    "                        label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(startThetas[0,0], startThetas[1,0])) \n",
    "\n",
    "#plot regression lines for each update\n",
    "for j in range(1,stepsToTake):\n",
    "    axGradDescent[1].annotate('', xy=np.ravel(thetasDuringDescent[j]), xytext=np.ravel(thetasDuringDescent[j-1]),\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    axGradDescent[0].plot(singleFeatData, linAlgRegHypothesis(singleFeatData, thetasDuringDescent[j]),\n",
    "               color=colors[j], lw=1.5, alpha = 0.6, linestyle = \"dashed\",\n",
    "               label=list(map(r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format,*thetasDuringDescent[j])))\n",
    "\n",
    "    \n",
    "#add points in contour plot for different updated thetas\n",
    "pointColors = colors.copy()\n",
    "pointColors.insert(0, \"black\")\n",
    "axGradDescent[1].scatter(*zip(*thetasDuringDescent), c=pointColors[0:len(thetasDuringDescent)], s=40, lw=0)\n",
    "\n",
    "# Labels, titles and a legend.\n",
    "axGradDescent[1].set_xlabel(r'$\\theta_0$')\n",
    "axGradDescent[1].set_ylabel(r'$\\theta_1$')\n",
    "axGradDescent[1].set_title('Cost function')\n",
    "\n",
    "axGradDescent[0].set_title('Data and fit')\n",
    "axbox = axGradDescent[0].get_position()\n",
    "# Position the legend by hand so that it doesn't cover up any of the lines.\n",
    "#if not stepsToTake > 10:\n",
    "#    axGradDescent[0].legend(fontsize='small')\n",
    "\n",
    "figGradDescent.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up and moving on to a GWAS(-like) analysis\n",
    "\n",
    "Hopefully, it's a success! If not, be sure to check the shape of your arrays with `.shape` and `.ndim`, and try to figure out what inputs are going awry. <br> <br> <br> Now let's move on to some (very limited) GWAS analysis, based on data from [here](https://github.com/MareesAT/GWA_tutorial/). Below, we load in some example data and explore it. Before this can be done, do the following:\n",
    "\n",
    "* Open an Anaconda prompt, activate the environment you use for the course (if using), and type `conda install -c conda-forge pandas-plink`\n",
    "* You should have done this already in the pre-course setup but it's understandable if you missed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_plink import read_plink1_bin\n",
    "euroGWASData = read_plink1_bin(\"HapMap_3_r3_1.bed\", \"HapMap_3_r3_1.bim\", \"HapMap_3_r3_1.fam\", verbose=True)\n",
    "print(euroGWASData)\n",
    "print(euroGWASData.sel(sample = \"NA06989\"))\n",
    "print(euroGWASData.sel(sample = \"NA06989\").values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about the GWAS dataset\n",
    "The print-out of this data is rather complicated. That's because it is a read-in of binary datafiles in PLINK format, a command-line programme that is often used for GWAS analysis. The data is saved in an xarray, which is a specialised datatype that we won't go into here. Note that the rows have samples, and the columns contain features (here: 0, 1 or 2 for a certain SNP). Do the following:\n",
    "\n",
    "* Go [here](https://pandas-plink.readthedocs.io/en/latest/usage.html) and read the entry under Genotype (ignore the Kinship matrix part). After that, read [this](https://pandas-plink.readthedocs.io/en/latest/api/pandas_plink.read_plink1_bin.html)\n",
    "\n",
    "Answer these questions:\n",
    "\n",
    "1. How many SNPs are in this data?\n",
    "2. How many SNPs are on chromosome 11?\n",
    "3. How many male genomes are in this dataset? (see [this](https://www.cog-genomics.org/plink/1.9/formats#fam))\n",
    "4. How many genomes lack gender annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answers here (and code to get them)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP analysis on chromosome eleven subset.\n",
    "Let's continue with only SNPs on chromosome eleven, and 'only' 200 SNPs there. That's still plenty of data. As you've been told, a GWAS simply trains a linear model on the #of alles you have for a certain SNP: if A is normal at a location and you have 2* A, you have 0. If you have a T in 1 copy of that gene, you have 1. If you are homozygously different from A (2 Ts), you have a 2. In contrast to more complex algorithms, SNPs are supposed to have additive effects: we simply tally the effect of one SNP on having the outcome measure with all others, and the total would be the expected effect of your genetic make-up on some outcome like BMI. Within SNPs, too, we generally assume an additive model: that having 2 alternative alleles at a SNP location has twice the effect of having 1. In reality, of course, there can be epistasis, or one allele could do nothing but having 2 mutations has a very large effect (All this is simplifying the field of GWAS, of course, there's a _lot_ of nuance we're leaving out for the purpose of brevity).\n",
    "\n",
    "The cell immediately below makes sure that there is a simulated quantitative outcome measure (BMI), and that a few SNPs on chromosome eleven influence this trait. You can just run it and move on to the next cell to run linear regressions on those SNPs using your own functions. Below are the well-known Manhattan plots of GWAS, which require p-values for the associations in the linear regressions. We haven't talked about how to get those.\n",
    "![Manhattan plots](ManhattanPlot.jpg)\n",
    "\n",
    "It's outside of the scope here to go all-in on explaining this, but you can calculate the standard error of a regression slope. The formula for it is: $$\\sqrt{\\frac{1}{m-2} \\cdot \\frac{\\sum_{i=1}^m(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^m(x_i - x_{mean})^2}}$$\n",
    "Here, $m$ is the number of samples, the top calculates sum of square errors, and the bottom calculates the sum of squares of the features (since we mean-center our features the mean should be 0). The upshot is that this formula calculates how much, on average, the real values differ from the predicted values. For further info, see [this StatQuest video](https://www.youtube.com/watch?v=XNgt7F6FqDU) and [this web page](https://www.statology.org/standard-error-regression/).\n",
    "\n",
    "With this formula in hand, we can calculate a t-statistic, and with a t-statistic, we can perform significance testing, i.e. we can say what the chance is that a result this strong or stronger could have occured just by chance (Although of course significance testing and/or hypothesis testing has received a lot of flak, and rightly so, see: [1](https://arbital.com/p/likelihoods_not_pvalues/?l=4xx), [2](https://arbital.com/p/likelihoods_not_pvalues/), and [3](https://sci-hub.se/https://doi.org/10.17763/haer.48.3.t490261645281841). For a zealous account, see [4](https://sci-hub.se/https://doi.org/10.5153%2Fsro.3857)). Still, that discussion is not the focus here. Calculating the t-statistic is extremely simple: it's just $t = \\frac{\\theta_1}{SE(\\theta_1)}$\n",
    "\n",
    "Intuitively, you can understand that the right plot below has lower standard error, so we are more sure of our estimated slope of the regression line than we are on the left, where it feels like the slope might be very different if we had more samples. We can codify this with a p-value, as is done in GWAS (although see the above _strong condemnations_ of p-values).\n",
    "![plotSERegression](ExampleSERegression.PNG)\n",
    "\n",
    "I supply functions to calculate the p-value from the linear regressions you fit, then we'll look at the 20 lowest p-values.\n",
    "\n",
    "That was a lot of text. Summary:\n",
    "* Run the cell below. It results in data that has 200 SNP positions on chromosome 11, of which 20 have an effect on BMI.\n",
    "* Move on to the cell below that to get to work doing the linear regressions you need to do\n",
    "* Use the supplied functions to calculate the p-value for the slope of each regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsizedData = euroGWASData.where(euroGWASData.chrom == \"11\", drop = True)\n",
    "downsizedData = downsizedData[:, rand.sample(range(0, downsizedData.shape[1]), 200)]\n",
    "#remove columns that have all 0, 1 or 2 --> can't infer anything there\n",
    "columnsWithoutVariation = np.all(downsizedData.values == downsizedData.values[0,:], axis = 0)\n",
    "downsizedData = downsizedData[:, np.invert(columnsWithoutVariation)]\n",
    "\n",
    "\n",
    "#sample some SNPs to assign a signal\n",
    "SNPsForSignal = np.array(rand.sample(range(0, downsizedData.shape[1]), 20))\n",
    "\n",
    "#add normally distributed BMI value with mean that is somewhat healthy \n",
    "newArray = np.zeros(len(downsizedData.trait))\n",
    "downsizedDataNew = downsizedData.drop_vars(\"trait\")\n",
    "newArray = np.random.default_rng().normal(22.5, 1, len(newArray))\n",
    "downsizedDataNew[\"BMI\"] = (\"sample\", newArray)\n",
    "\n",
    "\n",
    "#now, give certain SNPs a protective effect for BMI (help in being leaner) and a few the effect of predisposing to high BMI\n",
    "#assume additivity within the SNP, which is to say: one protective allele has half the protective effect of 2.\n",
    "protectiveSNPs = np.array(rand.sample(list(SNPsForSignal), 5))\n",
    "diseaseSNPs    = np.array([i for i in SNPsForSignal if i not in protectiveSNPs])\n",
    "\n",
    "protectiveEffect = 3\n",
    "for protSNPCoord in protectiveSNPs:\n",
    "    twoAlleles  = np.where(downsizedDataNew[:, protSNPCoord].values == 2.)\n",
    "    oneAllele   = np.where(downsizedDataNew[:, protSNPCoord].values == 1.)\n",
    "    noMuts      = np.where(downsizedDataNew[:, protSNPCoord].values == 0.)\n",
    "    newArray[twoAlleles] -= np.random.default_rng().normal(protectiveEffect, 0.1, len(twoAlleles))\n",
    "    newArray[oneAllele]  -= np.random.default_rng().normal(protectiveEffect/2, 0.1, len(oneAllele))\n",
    "\n",
    "obesityEffect = 3.87\n",
    "for disSNPCoord in diseaseSNPs:\n",
    "    twoAlleles  = np.where(downsizedDataNew[:, disSNPCoord].values == 2.)\n",
    "    oneAllele   = np.where(downsizedDataNew[:, disSNPCoord].values == 1.)\n",
    "    noMuts      = np.where(downsizedDataNew[:, disSNPCoord].values == 0.)\n",
    "    newArray[twoAlleles] += np.random.default_rng().normal(obesityEffect, 0.1, len(twoAlleles))\n",
    "    newArray[oneAllele]  += np.random.default_rng().normal(obesityEffect/2, 0.1, len(oneAllele))\n",
    "\n",
    "\n",
    "downsizedData[\"BMI\"] = (\"sample\", newArray)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing your analysis\n",
    "Okay, the data is all set up. There is just one thing: there are missing data in this GWAS dataset: SNPs that were not measured for certain people. For now, we'll just replace these NaNs with 0, assuming they are the most common genotype. In fact, there's two things: the second one is that there are SNPs where every person in the dataset has either 0, 1, or 2: we can't say anything there. I've prefiltered those columns out for you in the code cell above. Now to run the linear regressions as used in GWAS:\n",
    "* Loop over the columns of the `downsizedData`. Each column contains data for a single SNP. To get at this data, use `downsizedData[:, columnNr].values`\n",
    "* This SNP data is your feature. Run linear regression on this feature, using the BMI as the outcome. Don't forget to add the intercept feature which is just an array of ones (or alternatively, this might be incorporated into your linAlgGradientDescent function already).\n",
    "* To run linear regression:\n",
    "    * Make sure you normalise the feature and the BMI.\n",
    "    * Initialise thetas (done for you).\n",
    "    * Run gradient descent for a number of steps (nSteps = 20 below)\n",
    "    * Save the resulting theta values in a list or array, so that you get, per SNP in the data, the two thetas.\n",
    "\n",
    "After this is done, we'll worry about calculating the p-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up. Set mean BMI as start value for intercept.\n",
    "startThetas = np.array([[np.mean(newArray)], [0]])\n",
    "nSteps      = 20\n",
    "outcome     = downsizedData[\"BMI\"].values\n",
    "#Normalise the outcome measure!\n",
    "\n",
    "alpha       = 0.3 \n",
    "listThetasForEachSNP = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#up to you: use gradient descent to find the thetas, save these thetas in a list after all nSteps are done.\n",
    "#don't forget to normalise BMI and the features!\n",
    "for SNPIndex in range(0, downsizedData.shape[1]):\n",
    "    #replace np.nan with 0\n",
    "    values = downsizedData[:, SNPIndex].values\n",
    "    values[np.isnan(values)] = 0\n",
    "    \n",
    "    for step in range(0, nSteps):\n",
    "        break\n",
    "    break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get the p-values for each regression\n",
    "\n",
    "Run the cell below to get the function you need. Note: you separately fit a regression for each SNP, and this function works on the theta parameters (here, only $\\theta_1$) for each of these regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPValuesRegSlope(X, y, thetas):\n",
    "    \"\"\"Skips calculation for the significance of the intercept (i.e. it being sig. different from 0,\n",
    "    as it's the relation between BMI and the SNP we are interested in. Assumes theta as a vector (2D numpy array) of\n",
    "    n_thetas by 1 (so 2 rows, 1 column for univariate linear regression)\"\"\"\n",
    "    m = len(X)\n",
    "    thetasToCalc = thetas[1:,:]\n",
    "    preds = linAlgRegHypothesis(X, thetas)\n",
    "    #print(preds)\n",
    "    #print(y)\n",
    "    if not y.ndim > 1:\n",
    "        y = y[:, np.newaxis]\n",
    "    #print(X)\n",
    "\n",
    "    sumSquareErrorsPred = np.sum(np.square(np.array(preds) - y))\n",
    "    stdErrors = []; tStats = []; pVals = []\n",
    "    for index, theta in enumerate(thetasToCalc):\n",
    "        #print(theta) ;print(index)\n",
    "        sumSquaresFeature = np.sum(np.square(X - np.mean(X)))\n",
    "        stdError = np.sqrt((1/(m-2)) * (sumSquareErrorsPred/sumSquaresFeature))\n",
    "        tStat    = theta/stdError\n",
    "        pVal     = scipy.stats.t.sf(abs(tStat), m-1)\n",
    "        stdErrors.append(stdError); tStats.append(tStat); pVals.append(pVal)\n",
    "    \n",
    "    df = pd.DataFrame(np.vstack([stdErrors, tStats, pVals]))\n",
    "    df.set_index(pd.Index([\"standard errors\", \"t statistics\", \"p-values\"]), inplace = True)\n",
    "    return[pVals, df]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the p-values\n",
    "\n",
    "The below code will get you p-values for every SNP. All you need to do is give it a list of thetas to work on. That list of thetas should have n_SNPs entries, and each entry is, in this case, a 2 by 1 2D array, containing $\\theta_0$ and $\\theta_1$ for the linear regression of BMI against # of alternative alleles for that SNP. Up to you to:\n",
    "\n",
    "* **Replace listThetasForEachSNP in `enumerate(listThetasForEachSNP)` with your name for a list with the 2 thetas for each of the 200 linear regressions you trained**\n",
    "* Inspect the p-values. How many are less than 0.05? How many would you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pValsPerSNP = []\n",
    "listDFs = []\n",
    "outcomeNorm, outcomeMean, outcomeSD = createNormalisedFeatures(outcome, mode = \"SD\")\n",
    "dataWithoutNA = downsizedData.values; dataWithoutNA[np.isnan(dataWithoutNA)] = 0\n",
    "dataNorm, dataMeans, dataSDs        = createNormalisedFeatures(pd.DataFrame(dataWithoutNA), mode = \"SD\")\n",
    "for SNPindex, thetas in enumerate(listThetasForEachSNP):\n",
    "    pVal, dfInfo = calcPValuesRegSlope(np.array(dataNorm.iloc[:, SNPindex]), outcomeNorm, thetas)\n",
    "    pValsPerSNP.append(pVal[0])\n",
    "    listDFs.append(dfInfo)\n",
    "\n",
    "pValuesPerSNPFinal = np.ravel(np.hstack(pValsPerSNP))\n",
    "print(pValuesPerSNPFinal)\n",
    "\n",
    "# answer\n",
    "print(\"# of locations where p <= 0.05: \" + str(len(np.where(pValuesPerSNPFinal <= 0.05)[0])))\n",
    "# given a cut-off of 0.05, you'd expect 200* 1/20 (random chance) + 20 (signal I added) = 30. So there's quite a bit more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the regression values you obtained\n",
    "\n",
    "Now, let's take the 20 lowest p-values values and see whether these correspond to the signal we put into the BMI values. Remember, I added signal to 20 SNPs so you would find it easily in this _very small_ practice dataset. Of course, in normal GWAS procedure they use very stringent genome-wide p-value thresholds, much lower than 0.05, or they do some other sort of multiple testing correction, and often the SNPs are validated later. For our simple purposes here, though, let's just take the 20 lowest p-values, i.e. the SNPs where the chance that this regression slope or a more extreme one was found by simple chance/noise is lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowestPValues = np.argsort(pValuesPerSNPFinal)[0:20]\n",
    "# check whether this is the signal I put in\n",
    "matches  = np.in1d(SNPsForSignal, lowestPValues)\n",
    "nMatches = sum(matches)\n",
    "print(\"SNPs with top 20 lowest p-values which are indeed of the 20 to which I added signal: \" + str(nMatches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "If things are similar on your side, you'll get that about half (I had 12/20) were indeed the SNPs where I put signal in. This shows that correction for spurious hits is a big deal in GWAS: even though I _added_ a lot of over-the-top signal, we still didn't retrieve all of them. Of course, our sample size is very low, which makes spurious results easier. Below we plot the regressions you fitted for the SNPs that had actual business being found as significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the linear regressions for SNPs with large effect sizes\n",
    "\n",
    "The below plots the linear regression plots for those SNPs that have actual signal. **Again, replace listThetasForEachSNP with the correct name for your list of theta parameters for each linear regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "%matplotlib inline \n",
    "\n",
    "SNPsWithLowPValues = SNPsForSignal[matches]\n",
    "\n",
    "outcomeNorm, outcomeMean, outcomeSD = createNormalisedFeatures(outcome, mode = \"SD\")\n",
    "\n",
    "neededData = downsizedData[:, SNPsWithLowPValues]\n",
    "\n",
    "for SNPindex, SNPlocation in enumerate(SNPsWithLowPValues):\n",
    "    thetasHere = listThetasForEachSNP[SNPlocation]\n",
    "    #twentyLargestThetaOnes.iloc[0:2, np.where(twentyLargestThetaOnes.columns == SNPlocation)[0][0]]\n",
    "    #maybe return to normal coordinates\n",
    "    features = neededData.values[:, SNPindex]\n",
    "    features[np.isnan(features)] = 0\n",
    "    featuresNorm, mean, SD = createNormalisedFeatures(features, mode= \"SD\")\n",
    "    SNPsPresentInData = np.in1d(np.array([0., 1.,2.]), features)\n",
    "    #skip SNPs where not all 3 genotypes are present, complicates plotting considerably\n",
    "    if not np.all(SNPsPresentInData):\n",
    "        continue\n",
    "    normalAllele = neededData[\"a0\"].values[SNPindex]\n",
    "    alternativeAllele = neededData[\"a1\"].values[SNPindex]\n",
    "    xLabels = [normalAllele*2, normalAllele+alternativeAllele, alternativeAllele*2]\n",
    "    xLabels = np.array(xLabels)[np.where(np.in1d(np.array([0., 1.,2.]), features))]\n",
    "    SNPName = neededData[\"snp\"].values[SNPindex]\n",
    "    #only predict once for each possible feature value\n",
    "    uniqueAlleleNr = np.sort(np.unique(featuresNorm))\n",
    "    predictions = linAlgRegHypothesis(uniqueAlleleNr, thetasHere)\n",
    "    #print(predictions)\n",
    "    #print(predictions * SD + mean)\n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    ax.scatter(featuresNorm, outcomeNorm *outcomeSD + outcomeMean, edgecolors = \"black\")\n",
    "    ax.set_xticks(np.unique(featuresNorm))\n",
    "    ax.set_xlabel(\"SNP genotype\")\n",
    "    ax.set_xticklabels(xLabels.tolist())\n",
    "    ax.set_ylabel (\"BMI\")\n",
    "    ax.set_title(SNPName + \"; p-value (uncorrected for multiple testing): \" + str(np.round(pValuesPerSNPFinal[SNPlocation], 4)) + \"; $\\\\theta_1$: \" + str(np.round(np.ravel(thetasHere)[1], 3)))\n",
    "    ax.plot(uniqueAlleleNr, predictions * outcomeSD + outcomeMean, color = \"red\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I want you to remember here\n",
    "* Though implementing a function that makes folds for cross-validation might have been somewhat difficult, what you mostly need to understand is the logic and why we do it: by subdividing our data into training folds and test folds, we have a number of estimates for how well our classifier will perform on unseen data. In other words: how well it will _generalise_. You don't, in the end, _use_ any of the classifiers you train during cross-validation: your final classifier would be trained on all your training data, and then published or shared with people. This means that your cross-validation estimates are even slightly pessimistic: more training data usually increases performance, and with cross-validation you always train on slightly less data then you have!\n",
    "* How matrix-vector and matrix-matrix multiplication works, i.e. how to do it and how to code it with numpy.\n",
    "* How we use these types of multiplication to streamline and (especially for large datasets) speed up our implementations of ML algorithms, and specifically How you can implement linear regression and gradient descent using it. (Note: I don't expect you to know the exact implementations by heart, but I do want you to know the gist of the multiplications you're doing).\n",
    "* That with these humble beginnings, you can already implement (a facsimile of) a GWAS procedure: relating a continuous variable to the nr. of alternative alleles you have at certain SNP locations. Pretty neat!\n",
    "\n",
    "## The end\n",
    "\n",
    "Congratulations, you've mastered linear regression using gradient descent and linear algebra, as well as implemented cross-validation! <br> I'd like to stress once more that the 'GWAS' we've done here is a parody of the real thing. There's familial relations to take into account (you share many SNPs with parents and grandparent), sex assignment to check, missing data to impute or otherwise correct for, etc. Nevertheless, the principle of running a linear regression for each SNP is the same. At the end of the week, we'll be able to correct a GWAS for population covariance in SNPs which will already bring us closer to the real deal.\n",
    "\n",
    "## Survey\n",
    "Did you like this practical? Did you hate it with a burning passion? Fill out [this survey](https://docs.google.com/forms/d/e/1FAIpQLSeJXmTlxMXUyCAndmuWVG4FhdFzNm4DEsy1nwswP7SAhOQwoA/viewform?usp=sf_link) and let us know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
