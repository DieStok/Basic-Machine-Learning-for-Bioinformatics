{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Morning Practical 1**\n",
    "Welcome to the practical exercises. Here, you're going to define your own hypothesis, mean-squared error, and gradient descent functions. First, run the cell below to set things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two multiple choice questions\n",
    "Let's start things off with 2 multiple choice questions. Run each code cell below in turn to get the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i InteractiveQuestion1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lectures, we have shown that gradient descent can minimise the cost function. Of course, that assumes that the minimum is reachable. We won't have to worry about this today, because linear functions always have a global minimum. What do you think would happen if you tried gradient descent in the picture on the right below? <br>\n",
    "![gradientDescentImage.jpg](gradientDescentImage.jpg) [source](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i InteractiveQuestion2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sample data\n",
    "Let's load some sample data and visualise it. We use matplotlib, numpy, and pandas to do this. Pandas is used to make so-called DataFrames, which are tables that usually have observations in the rows and features or properties in the columns. Unlike matrices, they can contain many data types (i.e. one column is a descriptive string, the other is numeric, etc.). Numpy is a library for working with numbers in Python. It comes with a data type called arrays, which can also have rows and columns. Numpy will be used to perform calculations for fitting ML models when necessary. Numpy and Pandas both have great documentation ([Numpy](https://numpy.org/doc/stable/user/absolute_beginners.html), [Pandas](https://pandas.pydata.org/docs/user_guide/index.html)), so be sure to search online when you need to use functions from these packages during the exercises and it isn't working out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sampleDataLinearRegression.csv\", header= None)\n",
    "data.columns = [\"x\", \"y\"]\n",
    "startTheta = np.array([2,2])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(num = \"linearRegressionPlot\")\n",
    "scatter = ax.scatter(data[\"x\"], data[\"y\"])\n",
    "regressionLine, = ax.plot(data[\"x\"], startTheta[0] + startTheta[1] * data[\"x\"], color = 'red', linestyle = \"dashed\")\n",
    "plt.title(\"Univariate Linear Regression Example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing your own MSE function\n",
    "During the lectures you just saw, we discussed a widely-used cost function, the mean-squared error or MSE. Write your own MSE function. It should take a DataFrame with observations in the rows and features/outcomes in the columns, and an array of thetas. Assume univariate linear regression, that is, there is only $\\theta_0$ and $\\theta_1$ to worry about. Call the function `MyMSE()`.\n",
    "* Hint: you can use `def FunctionName(arg1, arg2, arg3):` to define a function. In this case, use `def MyMSE(dataframe, thetas):`.\n",
    "* Hint: loop over all x values, calculating the prediction for each using the thetas that you also supply as an argument. Then simply take the square difference with the corresponding y value. Use the DataFrame method `.iterrows()` for this (although it can also be done differently, up to you!).\n",
    "* Hint: Don't forget to take the mean (for example by dividing by `len(DataFrame)`)\n",
    "* Hint: DataFrame giving you grief? Pandas has excellent documentation, and StackOverflow is always at the ready. Duckduckgo (or Google, if you must) is your friend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying your MSE function\n",
    "Now let's use your MSE function to calculate the MSE for different $\\theta_0$ and $\\theta_1$ values and see the corresponding regression line and contour plot of the cost function. Complete the code in the cell below by replacing all instances of `MyMSE()` with your MSE function name if you decided to be cheeky and <i> not </i> name it `MyMSE()`. Drag the sliders to see what values lead to the lowest mean-squared error. <br> <br>\n",
    "Note: since the cost function is calculated many times for many different thetas to make a contour plot, this will take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer: \n",
    "currentMSE = MyMSE(data, startTheta)\n",
    "\n",
    "#this sets up the regression plot\n",
    "fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=(10,6.15))\n",
    "scatter = ax1.scatter(data[\"x\"], data[\"y\"])\n",
    "regressionLine, = ax1.plot(data[\"x\"], startTheta[0] + startTheta[1] * data[\"x\"]\n",
    "                          , color = 'red', linestyle = \"dashed\")\n",
    "ax1.text(0, 100, \"Mean-Squared Error: \" + str(currentMSE))\n",
    "ax1.set_xlim([-10,110])\n",
    "ax1.set_ylim([-10,110])\n",
    "ax1.set_xlabel(\"$x$\")\n",
    "ax1.set_ylabel(\"$y$\")\n",
    "ax1.set_title(\"Scatterplot with regression line\")\n",
    "\n",
    "#this sets up a contour plot of the cost function.\n",
    "ax2.set_ylabel(\"$θ_1$\")\n",
    "ax2.set_xlabel(\"$θ_0$\")\n",
    "ax2.set_title(\"Contour plot of cost function\")\n",
    "\n",
    "#calculate cost values for many different theta0, theta1 combinations\n",
    "theta0Vals =  np.linspace(-20, 20, 120)\n",
    "theta1Vals =  np.linspace(0, 2, 120)\n",
    "X, Y = np.meshgrid(theta0Vals, theta1Vals)\n",
    "JVals = np.zeros(np.shape(X))\n",
    "for i in range(0,len(theta0Vals)):\n",
    "    for j in range(0,len(theta1Vals)):\n",
    "        JVals[i,j] = MyMSE(data, np.array([X[i,j], Y[i,j]]))\n",
    "\n",
    "#make a contour plot based on these\n",
    "ax2.set_ylim([np.min(theta1Vals),np.max(theta1Vals)])\n",
    "ax2.set_xlim([np.min(theta0Vals),np.max(theta0Vals)])\n",
    "minimum = ax2.scatter(X[JVals == np.min(JVals)], Y[JVals == np.min(JVals)],\n",
    "                      color = \"blue\", label = \"minimum\", marker = \"x\",\n",
    "                      s = 30)\n",
    "thetaCoordinate = ax2.plot(2, 2, color = \"black\", label = \"current cost\", linestyle = \"\", marker = \"o\")\n",
    "contour = ax2.contour(theta0Vals,theta1Vals,JVals, levels = [300, 500, 1000, 1500, 2000, 2500, 5000])\n",
    "ax2.clabel(contour)\n",
    "ax2.legend()\n",
    "\n",
    "\n",
    "#this defines a function that updates the plot based on the theta values in the sliders        \n",
    "def update(thetaZero = startTheta[0], thetaOne = startTheta[1]):\n",
    "    MSE = MyMSE(data, np.array([thetaZero, thetaOne]))\n",
    "    ax1.texts[-1].set_text(\"Mean-Squared Error: \" + str(round(MSE, 4)))\n",
    "    regressionLine.set_ydata(thetaZero + thetaOne *  regressionLine.get_xdata())\n",
    "    thetaCoordinate[0].set_xdata(thetaZero)\n",
    "    thetaCoordinate[0].set_ydata(thetaOne)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "#this displays the sliders  \n",
    "thetaZero = widgets.FloatSlider(min=-5, max=15, step=0.25, value=startTheta[0], description = \"$θ_0$\")\n",
    "thetaOne  = widgets.FloatSlider(min=-5, max=5, step=0.1, value=startTheta[1], description = \"$θ_1$\")\n",
    "widgets.interact(update, thetaZero = thetaZero,\n",
    "                thetaOne = thetaOne)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3D surface plot of the cost function\n",
    "Besides the contour plot, you can also look at a surface plot of the cost function. This will show the surface along which we use gradient descent, with its calculation of partial derivatives, to find the direction of steepest descent at any point along our path. Have a look at the surface below. You can rotate it by dragging with the mouse, and change the current cost by changing the theta parameters as above!\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSurface(thetaZeroSurf, thetaOneSurf):\n",
    "    fig3, ax3 = plt.subplots()\n",
    "\n",
    "    ax3 = plt.axes(projection='3d')\n",
    "    ax3.plot_surface(X, Y, JVals ,cmap='viridis', edgecolor='none', alpha = 0.5)#, vmax= 35000)\n",
    "    #scatterPoint = ax3.scatter(thetaZero.value,thetaOne.value,MyMSE(xData, yData,\n",
    "    #                                                 np.array([thetaZero.value, thetaOne.value])))\n",
    "    ax3.set_title('Surface plot')\n",
    "    ax3.set_xlabel(\"theta0\")\n",
    "    ax3.set_ylabel(\"theta1\")\n",
    "    ax3.set_zlabel(\"Mean-squared error \\n (J($θ_0,θ_1$))\")\n",
    "    ax3.set_zlim([0,5000])\n",
    "    ax3.set_xlim([-15,15])\n",
    "    ax3.set_ylim([0,3.5])\n",
    "    ax3.scatter(thetaZeroSurf,thetaOneSurf,MyMSE(data,\n",
    "                                                 np.array([thetaZeroSurf, thetaOneSurf])))\n",
    "    fig3.canvas.draw()\n",
    "\n",
    " \n",
    "\n",
    "thetaZero1 = widgets.FloatSlider(min=-5, max=15, step=1, value=5,\n",
    "                                 description = \"$θ_0$\", continuous_update = False)\n",
    "thetaOne1  = widgets.FloatSlider(min=0.5, max=3.5, step=0.5, value=2.0,\n",
    "                                 description = \"$θ_1$\", continuous_update = False)\n",
    "\n",
    "widgets.interactive(updateSurface,\n",
    "                   thetaZeroSurf = thetaZero1, thetaOneSurf = thetaOne1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Defining a separate univariate hypothesis function\n",
    "Now that you've seen how the cost function works, let's get to programming it in a more modular way. First, we'll update the MSE function to call a specific hypothesis function. This provides extendability: later on we'll make sure that the MSE function works with any hypothesis function, and then you can simply supply the hypothesis function you'll be using as an argument! For now, though, just do the below:\n",
    "* Write the univariate regression hypothesis as a function that takes in a data point x  and an array or list of two thetas and returns the predicted value. Call this function `univariateHypothesis`.\n",
    "* Update the MSE function to use this hypothesis.\n",
    "\n",
    "<br>\n",
    "<b> Hints </b>\n",
    "\n",
    "* Updating `MyMSE` requires only a small change: the prediction step, which should not hardcode the hypothesis function but just call the pre-defined `univariateHypothesis` function. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, on to a simple implementation of gradient descent so that we can automatically determine the theta values:\n",
    "\n",
    "* Write a gradient descent function called `gradientDescent` that takes in a DataFrame (that contains x and y values, i.e. features and the corresponding values of some variable to predict),  thetas, and alpha, and calculates the new thetas for one gradient descent step: `def gradientDescent(dataframe, thetas, alpha): `. Assume univariate linear regression, i.e. you only need to calculate 2 partial derivatives. Use your `univariateHypothesis` function to calculate the predictions. Have this `gradientDescent` function return the thetas as a numpy array, i.e. `np.array([thetaZeroStep, thetaOneStep])` or similar. <br> <br>\n",
    "\n",
    "For your convenience, here are the relevant formulae from the lecture:\n",
    "![PartialDerivativeUnivariateRegressionImage.JPG](PartialDerivativeUnivariateRegressionImage.JPG)\n",
    "<br>\n",
    "![GradientDescentStepUnivariateRegressionImage.JPG](GradientDescentStepUnivariateRegressionImage.JPG)\n",
    "\n",
    "<b> Hint </b>\n",
    "* The gradient descent function contains a sum term, so loop over every row, calculate the partial derivative with respect to $\\theta_0$ and $\\theta_1$ and add it to a running total. After that is done, you can take the step by doing something like `newThetaZero = currentThetaZero - alpha/m * runningTotalPartDerivThetaZero`. <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Putting your gradient descent function to the test\n",
    "Now, let's use your gradient descent function to calculate better parameters from some initial values. We do this here for both normalised/transformed data and non-transformed data. You will learn more about this normalisation in the coming lecture. We run 5 iterations of gradient descent, and show the updated regression lines and position on the contour plot of the cost function. Simply execute the cell and see the result below!  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we use your gradient descent function to calculate optimal parameters\n",
    "#for the regression you toyed with before.\n",
    "#Based on: https://scipython.com/blog/visualizing-the-gradient-descent-method/ \n",
    "\n",
    "\n",
    "#set steps to take for original and normalised data. Feel free to change this to see more steps after you've run it once.\n",
    "alphaXNotNormalised = 0.0005\n",
    "stepsToTakeXNotNormalised = 5\n",
    "\n",
    "alphaNormalised = 0.6\n",
    "stepsToTakeNormalised = 5\n",
    "\n",
    "startThetas = [[0,0]]\n",
    "\n",
    "#set up 4 plots\n",
    "figGradDescent, axGradDescent = plt.subplots(nrows = 2, ncols = 2, figsize=(10,12.3))\n",
    "#colors for plotting gradient descent steps\n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "\n",
    "#scatterplot without normalised x values\n",
    "axGradDescent[0,0].scatter(data[\"x\"], data[\"y\"], marker='x', s=40, color='k')\n",
    "axGradDescent[0,0].set_ylabel(\"y\")\n",
    "axGradDescent[0,0].set_xlabel(\"x\")\n",
    "\n",
    "#we can reuse the contour plot values we calculated before\n",
    "contours = axGradDescent[0,1].contour(X, Y, JVals,\n",
    "                         levels = 15)\n",
    "axGradDescent[0,1].clabel(contours)\n",
    "axGradDescent[0,1].set_xlabel(r\"$\\theta_0$\")\n",
    "axGradDescent[0,1].set_ylabel(r\"$\\theta_1$\")\n",
    "minimum = axGradDescent[0,1].scatter(X[JVals == np.min(JVals)], Y[JVals == np.min(JVals)],\n",
    "                      color = \"blue\", label = \"minimum\", marker = \"x\",\n",
    "                      s = 30)\n",
    "\n",
    "\n",
    "thetasXNotNorm = startThetas.copy()\n",
    "JsGradientDescentXNotNorm = [MyMSE(data, thetasXNotNorm[-1])]\n",
    "for step in range(0, stepsToTakeXNotNormalised):\n",
    "    currentThetas = thetasXNotNorm[-1]\n",
    "    newThetas     = np.array(gradientDescent(data, currentThetas, alphaXNotNormalised))\n",
    "    newJ          = MyMSE(data, newThetas)\n",
    "    \n",
    "    thetasXNotNorm.append(newThetas)\n",
    "    JsGradientDescentXNotNorm.append(newJ)\n",
    "\n",
    "\n",
    "#make the plot of gradient descent steps:\n",
    "\n",
    "#if stepsToTake > colors, repeat the colors\n",
    "colors = colors * int(np.ceil(stepsToTakeXNotNormalised/len(colors)))\n",
    "\n",
    "#plot initial regression line\n",
    "axGradDescent[0,0].plot(data[\"x\"], [univariateHypothesis(row[\"x\"], thetasXNotNorm[0]) for index, row in data.iterrows()],\n",
    "           color=colors[0], lw=2, linestyle = \"dashed\",\n",
    "                        label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(thetasXNotNorm[0][0], thetasXNotNorm[0][1])) \n",
    "\n",
    "#plot regression lines for each update\n",
    "for j in range(1,stepsToTakeXNotNormalised):\n",
    "    axGradDescent[0,1].annotate('', xy=thetasXNotNorm[j], xytext=thetasXNotNorm[j-1],\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    axGradDescent[0,0].plot(data[\"x\"], [univariateHypothesis(row[\"x\"], thetasXNotNorm[j]) for index, row in data.iterrows()],\n",
    "               color=colors[j], lw=1.5, alpha = 0.6, linestyle = \"dashed\",\n",
    "               label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*thetasXNotNorm[j]))\n",
    "\n",
    "#add points in contour plot for different updated thetas\n",
    "pointColors = colors.copy()\n",
    "pointColors.insert(0, \"black\")\n",
    "axGradDescent[0,1].scatter(*zip(*thetasXNotNorm), c=pointColors, s=40, lw=0)\n",
    "\n",
    "# Labels, titles and a legend.\n",
    "axGradDescent[0,1].set_xlabel(r'$\\theta_0$')\n",
    "axGradDescent[0,1].set_ylabel(r'$\\theta_1$')\n",
    "axGradDescent[0,1].set_title('Cost function')\n",
    "\n",
    "axGradDescent[0,0].set_title('Data and fit')\n",
    "axbox = axGradDescent[0,0].get_position()\n",
    "# Position the legend by hand so that it doesn't cover up any of the lines.\n",
    "if not stepsToTakeXNotNormalised > 10:\n",
    "    axGradDescent[0,0].legend(fontsize='small')\n",
    "\n",
    "##############################\n",
    "##\n",
    "##     with normalisation of x and y\n",
    "##\n",
    "##############################\n",
    "\n",
    "#normalise and 0-center by subtracting the mean from every observation and dividing by the standard deviation.\n",
    "mean        = data.mean() #you need to save these so you can use them to transform new samples you want to predict!\n",
    "std         = data.std()\n",
    "def transformationFunction(column):\n",
    "    mean = np.mean(column)\n",
    "    std  = np.std(column)\n",
    "    newValues = (column - mean)/std\n",
    "    return newValues\n",
    "\n",
    "transformedDataFrame = data.transform(func = transformationFunction, axis = 0)\n",
    "\n",
    "#calculate the contours of the cost function for this transformed data\n",
    "theta0Vals1 = np.linspace(-5, 5, 55)\n",
    "theta1Vals1 = np.linspace(-2, 2, 55)\n",
    "X1, Y1 = np.meshgrid(theta0Vals1, theta1Vals1)\n",
    "\n",
    "JVals1 = np.zeros((len(theta0Vals1), len(theta1Vals1)))\n",
    "for i in range(0,len(theta0Vals1)):\n",
    "    for j in range(0,len(theta1Vals1)):\n",
    "        JVals1[i,j] = MyMSE(transformedDataFrame, np.array([X1[i,j], Y1[i,j]]))\n",
    "\n",
    "\n",
    "#plotting, same as above\n",
    "axGradDescent[1,0].scatter(transformedDataFrame[\"x\"], transformedDataFrame[\"y\"], marker='x', s=40, color='k')\n",
    "axGradDescent[1,0].set_ylabel(\"y\")\n",
    "axGradDescent[1,0].set_xlabel(\"x\")\n",
    "\n",
    "#set up contour plot for normalised X data\n",
    "contours = axGradDescent[1,1].contour(X1, Y1, JVals1,\n",
    "                         levels = 15)\n",
    "axGradDescent[1,1].clabel(contours)\n",
    "axGradDescent[1,1].set_xlabel(r\"$\\theta_0$\")\n",
    "axGradDescent[1,1].set_ylabel(r\"$\\theta_1$\")\n",
    "minimum = axGradDescent[1,1].scatter(X1[JVals1 == np.min(JVals1)], Y1[JVals1 == np.min(JVals1)],\n",
    "                      color = \"blue\", label = \"minimum\", marker = \"x\",\n",
    "                      s = 30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "thetasNorm = [[-2, 0]]\n",
    "\n",
    "JsGradientDescentNorm = [MyMSE(transformedDataFrame, thetasNorm[-1])]\n",
    "for step in range(0, stepsToTakeNormalised):\n",
    "    currentThetas = thetasNorm[-1]\n",
    "    #even kijken niet op transformed data\n",
    "    newThetas     = np.array(gradientDescent(transformedDataFrame, currentThetas, alphaNormalised))\n",
    "    newJ          = MyMSE(transformedDataFrame, newThetas)\n",
    "    thetasNorm.append(newThetas)\n",
    "    JsGradientDescentNorm.append(newJ)\n",
    "\n",
    "\n",
    "#make the plot of gradient descent steps:\n",
    "#if stepsToTake > colors, repeat the colors\n",
    "colors = colors * int(np.ceil(stepsToTakeNormalised/len(colors)))\n",
    "\n",
    "#plot initial regression line\n",
    "axGradDescent[1,0].plot(transformedDataFrame[\"x\"],\n",
    "                        [univariateHypothesis(row[\"x\"], thetasNorm[0]) for index, row in transformedDataFrame.iterrows()],\n",
    "                        color=colors[0], lw=2, linestyle = \"dashed\",\n",
    "                        label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(thetasNorm[0][0], thetasNorm[0][1])) \n",
    "\n",
    "#plot regression lines for each update\n",
    "for j in range(1,stepsToTakeNormalised):\n",
    "    axGradDescent[1,1].annotate('', xy=thetasNorm[j], xytext=thetasNorm[j-1],\n",
    "                   arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                   va='center', ha='center')\n",
    "    axGradDescent[1,0].plot(transformedDataFrame[\"x\"],\n",
    "                            [univariateHypothesis(row[\"x\"], thetasNorm[j]) for index, row in transformedDataFrame.iterrows()],\n",
    "                            color=colors[j], lw=1.5, alpha = 0.6, linestyle = \"dashed\",\n",
    "                            label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*thetasNorm[j]))\n",
    "\n",
    "#add points in contour plot for different updated thetas\n",
    "pointColors = colors.copy()\n",
    "pointColors.insert(0, \"black\")\n",
    "axGradDescent[1,1].scatter(*zip(*thetasNorm), c=pointColors, s=40, lw=0)\n",
    "\n",
    "# Labels, titles and a legend.\n",
    "axGradDescent[1,1].set_xlabel(r'$\\theta_0$')\n",
    "axGradDescent[1,1].set_ylabel(r'$\\theta_1$')\n",
    "axGradDescent[1,1].set_title('Cost function')\n",
    "\n",
    "axGradDescent[1,0].set_title('Normalised data and fit')\n",
    "axbox = axGradDescent[1,0].get_position()\n",
    "if not stepsToTakeNormalised  > 10:\n",
    "    axGradDescent[1,0].legend(\n",
    "             fontsize='small')\n",
    "\n",
    "figGradDescent.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see above is that for non-normalised data (top row), gradient descent is having a hard time quickly finding a good estimate, and is jumping back and forth along the path to a good theta1. The new value for $\\theta_1$ found by a gradient descent step is dependent on x, which is on a scale from 0-100, while $\\theta_0$ is just dependent on the errors, which are on a much smaller scale. Because of this, $\\theta_0$ almost doesn't change whereas $\\theta_1$ quickly jumps all over the place if you don't normalise. Therefore, we are also forced to use a very low alpha (0.0005), and even then it's not perfect, as you still see jumps back and forth. <br> <br>\n",
    "\n",
    "In the normalised case, this problem is much reduced, and with an alpha of 0.6 we quickly step towards the minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I'd like you to remember here:\n",
    "* That you can implement ML requirements in Python yourself, and how to do it: this practical was chiefly focussed on implementing linear regression and gradient descent yourself. I hope it's given you an appreciation that this is not some mystical thing libraries do for you but something you can just _do yourself_.\n",
    "* The partial derivatives we use in linear regression (and how we get them).\n",
    "* That you're trying to minimise a cost function by evaluating it using training data, and then using its partial derivatives to take small steps in the direction of steepest descent of the cost function, and doing this until converging.\n",
    "* How a contour plot or surface plot visualises the cost.\n",
    "\n",
    "## The end\n",
    "One practical down, many to go! Use your exuberant joy at having finished this practical to fill out the survey below!\n",
    "\n",
    "## Survey\n",
    "Did you like this practical? Did you hate it with a burning passion? Fill out [this survey](https://docs.google.com/forms/d/e/1FAIpQLSdTuTS3hZbQLprNO0DStD5v4xjP6JogbJPglMQ5xfNfZwMpkQ/viewform?usp=sf_link) and let us know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
