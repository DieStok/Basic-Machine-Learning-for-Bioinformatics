{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morning practical 2:\n",
    "\n",
    "In the first practical, you worked on getting the basics of hypothesis functions and gradient descent down. This was probably somewhat difficult, but extending this towards polynomial regression will be relatively easier now that you have the basis. Let's move towards polynomial regression. First, run the two code cells below, then move on. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary functions you defined before:\n",
    "def univariateHypothesis(x, thetas):\n",
    "    predict = thetas[0] + thetas[1] * x\n",
    "    return predict\n",
    "\n",
    "def MyMSE(dataframe, thetas):\n",
    "    totalSumSquares = 0\n",
    "    for index, rowData in dataframe.iterrows():\n",
    "        prediction = univariateHypothesis(rowData['x'], thetas)\n",
    "        squareError = (prediction-rowData[\"y\"])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(dataframe) \n",
    "    return meanSquaredError\n",
    "\n",
    "def gradientDescent(dataframe, thetas, alpha):\n",
    "    m = len(dataframe)\n",
    "    #print(\"m: \"); print(m)\n",
    "    totalErrorThetaZero = 0\n",
    "    totalErrorThetaOne = 0\n",
    "    for index, row in dataframe.iterrows():\n",
    "    #print ([row[\"x\"], row[\"y\"]])\n",
    "        errorThetaZero = univariateHypothesis(row[\"x\"], thetas) - row[\"y\"]\n",
    "        errorThetaOne  = (univariateHypothesis(row[\"x\"], thetas) - row[\"y\"]) * row[\"x\"]\n",
    "        totalErrorThetaZero += errorThetaZero\n",
    "        totalErrorThetaOne += errorThetaOne\n",
    "    \n",
    "    thetaZeroStep = thetas[0] - alpha * (1/m) * totalErrorThetaZero\n",
    "    thetaOneStep  = thetas[1] - alpha * (1/m) * totalErrorThetaOne\n",
    "    return np.array([thetaZeroStep, thetaOneStep])\n",
    "    \n",
    "#sample data\n",
    "data = pd.read_csv(\"sampleDataLinearRegression.csv\", header= None)\n",
    "data.columns = [\"x\", \"y\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some dummy polynomial features\n",
    "For polynomial regression, we first need some polynomial data. To get that, we give you this function called `makePolynomialFeatures` that takes in a feature column and an argument `power`, that returns a DataFrame with a number of columns equal to power, where each column contains the original feature column raised to a power in `range(1, power+1)`. So `makePolynomialFeatures(np.array(featureColumn = [3,4,5], power = 3)` returns: <br> \n",
    "\\[3; 9 ; 27\\] <br>\n",
    "\\[4; 16; 64\\] <br>\n",
    "\\[5; 25; 125\\] <br>\n",
    "<br>\n",
    "\n",
    "Run the below cell to see it in action and move on to normalising the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makePolynomialFeatures(x = pd.DataFrame(None), power = 2):\n",
    "    \n",
    "    columns = []\n",
    "    nameList = []\n",
    "    for i in range(1, power+1):\n",
    "        columns.append(x**i)\n",
    "    finalFeaturesDataFrame = pd.concat(columns, axis = 1)\n",
    "    columnNames = [name + \"toPower\" + str(power) for name, power in zip(finalFeaturesDataFrame.columns, list(range(1,power+1)))]\n",
    "    finalFeaturesDataFrame.columns = columnNames\n",
    "    return finalFeaturesDataFrame\n",
    "\n",
    "        \n",
    "testPolynomial = makePolynomialFeatures(x = data[\"x\"], power = 2)\n",
    "display(testPolynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Making a function for data normalisation\n",
    "Good. You've seen in **Exercise 1** and in the lectures that the features should be normalised to make gradient descent work well. High time to write a function to normalise these features. Call it `createNormalisedFeatures`. The function should have an argument `mode` that accepts two inputs, 'SD' and 'range'. For 'SD', the normalisation should return:\n",
    "* the mean of each feature.\n",
    "* the standard deviation of each feature.\n",
    "* the normalised feature itself, using the formula: <br> $$\\frac{(feature\\ values - feature\\ mean)}{feature\\ standard\\  deviation}$$ <br>\n",
    "\n",
    "For 'range' as input the normalisation should return:\n",
    "* the mean of each feature.\n",
    "* the range of each feature (max - min).\n",
    "* the normalised feature itself, using the formula:\n",
    "$$\\frac{(feature\\ values - feature\\ mean)}{(feature\\ max - feature\\ min)}$$\n",
    "\n",
    "After you are done, test your function on a DataFrame with a linear and a quadratic feature (i.e. made with `power = 2` in `makePolynomialFeatures`) using `mode = 'range'`. Code to plot the features in this space has been provided below.\n",
    "\n",
    "##### Why do we need the means and std. dev./range saved?\n",
    "\n",
    "We need these saved so if we want to predict for an unseen data point, we can transform its features in the same way we do with our training data: by subtracting the means of the training data and dividing by the std.dev./range as we do in our normalisations.\n",
    "\n",
    "##### Why these different normalisations? <br>\n",
    "The SD normalisation brings features to *approximately* the \\[-1,1\\] range, however outliers still go slightly above or below this: we normalise based on the standard deviation or average difference of the data from the mean, so points that are far from this mean will have larger values. In other words: this assumes a normal distribution of data, and shrinks that distribution to have mean 0 and *most* of its observations in the \\[-1,1\\] range. <br>\n",
    "The range normalisation brings everything to a 0-1 range outright, no exceptions. Do note that if you have huge outliers, most data will be compressed to a tiny range, with the outliers being close to 1 or 0. <br>\n",
    "<br>\n",
    "<b> Hints </b> \n",
    "* To return multiple values in one function, simply put them in a list: `return [thing1,thing2,ChickenLittle]`. In this case, order the return values like so: `[normalisedFeatures, featureMeans, featureSD/featureRanges]` <br>\n",
    "* You can simply use `someDataFrame.mean()` and `someDataFrame.std()` to get means and standard deviations for each column, respectively. Range can be found in a similar way, using `someDataFrameRange = someDataFrame.max() - someDataFrame.min()`. If you then do `transformedDataFrame = (someDataFrame - someDataFrame.mean())/someDataFrameRange` you're done for range transformation! <br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here\n",
    "\n",
    "\n",
    "\n",
    "#plotting code\n",
    "#don't run before you made the function! You can comment it out with ctrl + /\n",
    "runOnPowerTwoData = createNormalisedFeatures(makePolynomialFeatures(data[\"x\"], power = 2),\n",
    "                                          mode = \"range\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([0,100])\n",
    "ax.scatter(runOnPowerTwoData[0].iloc[:,0], data[\"y\"], label = \"linear feature\")\n",
    "ax.plot([np.mean(runOnPowerTwoData[0].iloc[:,0]), np.mean(runOnPowerTwoData[0].iloc[:,0])],\n",
    "       [0,100], linestyle = 'dashed', linewidth = 1, color = \"red\", label = 'mean linear feature')\n",
    "ax.scatter(runOnPowerTwoData[0].iloc[:,1], data[\"y\"], label = \"quadratic feature\")\n",
    "ax.plot([np.mean(runOnPowerTwoData[0].iloc[:,0]), np.mean(runOnPowerTwoData[0].iloc[:,0])],\n",
    "       [0,100], linestyle = 'dotted', linewidth = 1, color = \"blue\", label = 'mean quadratic feature')\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Plotting different data normalisations\n",
    "Run the below cell to see its effects on the sample linear regression data with only the normal feature `data['x']`. Note how every normalisation has slightly different characteristics (note the axis scales!).\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a figure\n",
    "figNormPlot, ((ax1NormPlot, ax2NormPlot, ax3NormPlot), (ax4NormPlot, ax5NormPlot, ax6NormPlot)) = plt.subplots(2,3, figsize=(15,6.15))\n",
    "figNormPlot.tight_layout()\n",
    "\n",
    "#create normal scatterplot\n",
    "ax1NormPlot.set_title(\"Untransformed data\")\n",
    "scatter = ax1NormPlot.scatter(data[\"x\"], data[\"y\"])\n",
    "regressionLine, = ax1NormPlot.plot(data[\"x\"], 0.31 + 1.05 * data[\"x\"],\n",
    "                                   color = 'red', linestyle = \"dashed\")\n",
    "\n",
    "#range-normalised plot\n",
    "ax2NormPlot.set_title(\"Range transformed (mean-normalised) data\")\n",
    "featsRange, featsRangeMean, featsRangeRange =  createNormalisedFeatures(data[\"x\"], mode = \"range\")\n",
    "scatter = ax2NormPlot.scatter(featsRange, data[\"y\"])\n",
    "ax2NormPlot.set_xlim([-1,1])\n",
    "\n",
    "#st.dev.-normalised plot\n",
    "ax3NormPlot.set_title(\"St.Dev. transformed data\")\n",
    "featsSD, featsSDMean, featsSDSD =  createNormalisedFeatures(data[\"x\"], mode = \"SD\")\n",
    "scatter = ax3NormPlot.scatter(featsSD, data[\"y\"])\n",
    "ax3NormPlot.set_xlim([-2.5,2.5])\n",
    "\n",
    "#distribution plots. They all look the same, but note that the axis limits vary!\n",
    "sns.kdeplot(data = np.array(data[\"x\"]), ax = ax4NormPlot)\n",
    "sns.kdeplot(data = np.array(featsRange), ax = ax5NormPlot)\n",
    "sns.kdeplot(data = np.array(featsSD), ax = ax6NormPlot)\n",
    "ax4NormPlot.hist(data[\"x\"], bins = 10, edgecolor='black', linewidth=1.2, density = True)\n",
    "ax5NormPlot.hist(featsRange, bins = 10, edgecolor='black', linewidth=1.2, density = True)\n",
    "ax6NormPlot.hist(featsSD, bins = 10, edgecolor='black', linewidth=1.2, density = True)\n",
    "\n",
    "#show all plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Towards multivariate linear regression: updating the hypothesis function\n",
    "\n",
    "Okay, now let's put it all together. First, update the hypothesis function to accomodate any number of features and parameters. It should take in the features for one sample, so a row of a DataFrame with features of length *n*, and an array/list of thetas with length *n*, calculate $\\theta_0, \\theta_1 * x_1, ..., \\theta_n * x_n$ and return the predicted value for the data point. Call it `multiHypothesis`. <br> <br>\n",
    "\n",
    "<b> Hints </b>\n",
    "* Loop over the thetas, and multiply each with the corresponding element in x. <br>\n",
    "* You can prepend 1 to the data that the function takes in. By adding this feature that is 1, $\\theta_0$ is multiplied by 1 when you use a for loop. To do this, when you are working with a row of a Pandas DataFrame, which is a `pd.Series()` object, use: `pd.Series(1).append(currentRowOfFeatures)`<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Towards multivariate regression: running gradient descent with `multiHypothesis`\n",
    "\n",
    "Armed with this glorious function, we'll take the following steps:\n",
    "* Use the `makePolynomialFeatures` function with `power = 2` to make features.\n",
    "* Normalise the features using `createNormalisedFeatures`, with `mode = 'range'`.\n",
    "* Use this normalised data in a linear regression with 2 variables by using gradient descent with this new hypothesis function. To do this, we need to change the `MyMSE` and `gradientDescent` functions to use the new `multiHypothesis` function to calculate cost. The new `MyMSE` function has been supplied as an example below. <br>\n",
    "* <b> You need to change the `gradientDescent` function to use `multiHypothesis`! </b> <br> <br>\n",
    "\n",
    "<b> Hints </b>\n",
    "* It is wasteful to keep redefining the MSE function for different hypotheses. Instead, we added an argument `hypothesis` to the MSE function. Then, in the code, we use `globals()[hypothesis](arg1, arg2)`. This causes Python to search the functions that have been defined in the session for the one called `hypothesis` and execute it if found. <br>\n",
    "* The `multiHypothesis` function does not need y values. When you are iterating over the samples using `index, rowData = DataFrame.iterrows()`, you can use `withoutYRowData = rowData.drop(\"y\")` to remove it. <br>\n",
    "* You can assume that there are only two features, 'xToPower1' and 'xToPower2'. <br>\n",
    "* `gradientDescent` should now return 3 values, for $\\theta_0$ , $\\theta_1$, and $\\theta_2$. Do this in a list, as before.\n",
    "* If you want to access a Pandas DataFrame column or Series object by a number, use `DataFrame.iloc[0,2]` (first row, second column) or `Series.iloc[1]` (2nd item in the series).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start things off\n",
    "startThetasHP = np.array([0,0,0])\n",
    "\n",
    "featuresHP = makePolynomialFeatures(data[\"x\"], power = 2)\n",
    "\n",
    "normalisedFeaturesHP, featureMeansHP, featureRangesHP = createNormalisedFeatures(featuresHP, mode = \"range\")\n",
    "\n",
    "#add 'y' variable to the normalised data frame\n",
    "normalisedFeaturesHP[\"y\"] = data[\"y\"]\n",
    "\n",
    "display(normalisedFeaturesHP)\n",
    "\n",
    "#example use of globals()\n",
    "def printMyMan(smiley = False):\n",
    "    if smiley == True:\n",
    "        print(\"My Man! (:\")\n",
    "    else:\n",
    "        print(\"My Man!\")\n",
    "    return None\n",
    "\n",
    "globals()[\"printMyMan\"]()\n",
    "globals()[\"printMyMan\"](smiley = True)\n",
    "\n",
    "#example change in MyMse() to use any hypothesis function you define\n",
    "def MyMSE(dataframe, thetas, hypothesis = \"multiHypothesis\"):\n",
    "    totalSumSquares = 0\n",
    "    for index, rowData in dataframe.iterrows():\n",
    "        prediction = globals()[hypothesis](rowData.drop(\"y\"), thetas)\n",
    "        squareError = (prediction-rowData[\"y\"])**2\n",
    "        totalSumSquares += squareError\n",
    "    meanSquaredError = totalSumSquares/len(dataframe) \n",
    "    return meanSquaredError\n",
    "\n",
    "\n",
    "\n",
    "#sample usage:\n",
    "exampleMSE = MyMSE(normalisedFeaturesHP, startThetasHP, \"multiHypothesis\")\n",
    "print(\"MSE for thetas \" + np.array2string(startThetasHP) + \": \" + str(exampleMSE))\n",
    "\n",
    "#now it's up to you to change gradientDescent in a similar way in the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running your very own multivariate linear regression algorithm\n",
    "All done? Great! You've made it through Mean-Squared Error functions, gradient descent implementations, polynomial feature generation and gradient descent that works for >1 feature. Let's see this brand spankin' new set-up in action. Of course, the underlying data is actually linear, so the quadratic equation we're fitting here will not be the best fit. But it will improve thanks to your gradient descent function-writing prowess. Run the cell below and marvel (and/or feel slightly underwhelmed) at the results!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaValuesDuringDescent = []\n",
    "MSEDuringDescent = []\n",
    "thetasNow = startThetasHP.copy()\n",
    "stepsGradDecent = 10\n",
    "alpha = 0.2\n",
    "for i in range(0,stepsGradDecent):\n",
    "    oneStep = gradientDescent(normalisedFeaturesHP, thetasNow, 0.2)\n",
    "    thetaValuesDuringDescent.append(oneStep)\n",
    "    thetasNow = oneStep\n",
    "    MSEDuringDescent.append(MyMSE(normalisedFeaturesHP, thetasNow, hypothesis = 'multiHypothesis'))\n",
    "#print(thetaValuesDuringDescent)\n",
    "\n",
    "fig10, ax10 = plt.subplots()\n",
    "scatter10One = ax10.scatter(normalisedFeaturesHP.iloc[:,0], normalisedFeaturesHP[\"y\"])\n",
    "regressionLine10, = ax10.plot(normalisedFeaturesHP.iloc[:,0],\n",
    "                              [multiHypothesis(row, startThetasHP) for index, row in normalisedFeaturesHP.drop(\"y\", axis = 1).iterrows()],\n",
    "                              color = 'red', linestyle = \"dashed\",\n",
    "                              label = \"start thetas: \" + str(startThetasHP) + \" ; MSE: \" + str(np.round(exampleMSE,1)))\n",
    "\n",
    "colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "#repeat colours if more steps\n",
    "colors = colors * int(np.ceil(len(thetaValuesDuringDescent)/5))\n",
    "#show for all gradient steps\n",
    "for i in range(0, len(thetaValuesDuringDescent)):\n",
    "    \n",
    "    ax10.plot(normalisedFeaturesHP.iloc[:,0],\n",
    "                              [multiHypothesis(row, thetaValuesDuringDescent[i]) for index, row in normalisedFeaturesHP.drop(\"y\", axis = 1).iterrows()],\n",
    "                              color = colors[i], linestyle = \"dashed\", alpha = 0.8,\n",
    "                              label = \"thetas: \" + str(np.round(thetaValuesDuringDescent[i], 1)) + \" ; MSE: \" + str(np.round(MSEDuringDescent[i])))\n",
    "\n",
    "ax10.legend(fontsize = \"small\")    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final words\n",
    "\n",
    "\"Aaarghh\". But also: well done! Time for one more lecture, in which we'll dive into using linear algebra to make our calculations (and hopefully also our function definitions) easier, and/or further explore the bias-variance trade-off.\n",
    "\n",
    "Note: linear regression actually has an algebraically defined minimum of the cost function, which can also be approached via something called the Normal Equation (given tractable data size). Hence, linear regression libraries will probably make use of this method if feasible, and many other speed ups and other features. Still, doing it yourself is an achievement!\n",
    "\n",
    "## What I'd like you to remember here:\n",
    "* Extending linear regression to multinomial (and polynomial, i.e. with exponents) regression is relatively straightforward once you have the basic routine down.\n",
    "* How to normalise data. Especially the mean-centering and scaling to unit variance (data-mean(data))/std(data) is used very often. \n",
    "* Why normalising data is important: that otherwise you get gradients operating at different scales, messing up your gradient steps, and costs which are much higher for the polynomial variables than for the others. To avoid this, we normalise. You'll be using normalisation prodigiously throughout the rest of the course.\n",
    "* That running polynomial regression on a linear relationship is a bit bonkers (but illustrative, nonetheless)\n",
    "\n",
    "\n",
    "## Survey\n",
    "Hi it's me again, the incessant reminder that you fill out the survey. Boy, surveys huh, who doesn't love 'em!? Great minds think alike, [here you go](https://docs.google.com/forms/d/e/1FAIpQLScoqJtzOclzOl8DrXnoukfySI3HAdfJNeGw_Gxplas09KdEDw/viewform?usp=sf_link)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
